<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <link rel="shortcut icon" href="../img/favicon.ico" />
    <title>The HDH Database - HDH</title>
    <link rel="stylesheet" href="../css/theme.css" />
    <link rel="stylesheet" href="../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "The HDH Database";
        var mkdocs_page_input_path = "database.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href=".." class="icon icon-home"> HDH
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="..">Home</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../intro/">An introduction to DQC</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../hdh/">A brief introduction to HDHs</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../models/">Quantum Computational Model mappings to HDHs</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../vis/">Visualizing HDHs</a>
                </li>
              </ul>
              <ul class="current">
                <li class="toctree-l1 current"><a class="reference internal current" href="#">The HDH Database</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#database-location-and-structure">Database Location and Structure</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#database-directory-structure">Database Directory Structure</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#database_generator-directory">Database_generator Directory</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#file-formats">File Formats</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#workloads-directory">Workloads Directory</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#hdhs-directory">HDHs Directory</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#partitions-directory">Partitions Directory</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#hdh-metadata">HDH Metadata</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#partitioning-performance-metrics">Partitioning Performance Metrics</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#available-metrics-from-hdhpasses">Available Metrics (from hdh.passes)</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#1-costhdh_graph-partitions-tuplefloat-float">1. cost(hdh_graph, partitions) → Tuple[float, float]</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#2-partition_sizepartitions-listint">2. partition_size(partitions) → List[int]</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#3-participationhdh_graph-partitions-dictstr-float">3. participation(hdh_graph, partitions) → Dict[str, float]</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#4-parallelismhdh_graph-partitions-dictstr-float">4. parallelism(hdh_graph, partitions) → Dict[str, float]</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#5-fair_parallelismhdh_graph-partitions-capacities-dictstr-float">5. fair_parallelism(hdh_graph, partitions, capacities) → Dict[str, float]</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#usage-example">Usage Example</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#extending-the-dataset">Extending the Dataset</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#how-to-add-to-this-database">How to Add to This Database</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#1-add-new-workloads-hdhs">1) Add New Workloads + HDHs</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#step-1-place-workloads">Step 1: Place Workloads</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#step-2-run-the-converter">Step 2: Run the Converter</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#converter-script-qasm-hdh-pklcsv">Converter Script (QASM → HDH → {pkl,csv})</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#step-3-verify-inspect">Step 3: Verify &amp; Inspect</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#step-4-submit-a-pr">Step 4: Submit a PR</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#2-add-partitioning-method-results">2) Add Partitioning Method Results</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#csv-format-for-partitioning-results">CSV Format for Partitioning Results</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#mandatory-columns">Mandatory Columns</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#optional-columns-method-specific">Optional Columns (Method-Specific)</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#example-csv">Example CSV</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#standard-partitioning-methods">Standard Partitioning Methods</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#greedy-hdh">Greedy (HDH)</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#metis-telegate-graph">METIS (Telegate graph)</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#greedy-tg-telegate-graph">Greedy-TG (Telegate graph)</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#adding-your-results">Adding Your Results</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#step-1-run-your-partitioning-method">Step 1: Run Your Partitioning Method</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#step-2-update-partitions_allcsv">Step 2: Update partitions_all.csv</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#step-3-document-your-method">Step 3: Document Your Method</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#step-4-capacity-constraints-and-failure-states">Step 4: Capacity Constraints and Failure States</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#step-5-recalculate-best">Step 5: Recalculate Best</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#step-6-submit-pr">Step 6: Submit PR</a>
    </li>
        </ul>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#database-usage-guidelines">Database Usage Guidelines</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#for-benchmarking">For Benchmarking</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#citation-and-acknowledgment">Citation and Acknowledgment</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#data-license">Data License</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#contributing">Contributing</a>
    </li>
    </ul>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../passes/">Partitioning HDHs for distribution</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../literature/">Related publications and information</a>
                </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="..">HDH</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href=".." class="icon icon-home" aria-label="Docs"></a></li>
      <li class="breadcrumb-item active">The HDH Database</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="the-hdh-database">The HDH Database</h1>
<p>To support reproducible evaluation and training of partitioning strategies, this library includes a database of pre-generated HDHs.<br />
We aim for this resource to facilitate benchmarking across diverse workloads and enables the development of learning-based distribution agents.</p>
<p><strong><em>Our goal is to extend the database with performance metrics of partitioning techniques for each workload.
This will allow the community to build a data-driven understanding of which hypergraph partitioning methods perform best under different conditions.
We encourage users to contribute results from their own partitioning methods when working with this database.
Instructions for how to upload results can be found below.</em></strong></p>
<h2 id="database-location-and-structure">Database Location and Structure</h2>
<p>The database is available in the <code>main</code> branch <a href="https://github.com/grageragarces/HDH">of the repository</a>. </p>
<p><strong>Important</strong>: The database exists only in the repository and is <strong>not included in the pip package or wheels</strong>. Users who want to use the database for benchmarking should clone the repository from GitHub. The database files are excluded from the PyPI distribution to keep the package size small.</p>
<p>The database is organized into two main directories:</p>
<ul>
<li><strong><code>Database/</code></strong>: Contains the actual database files (HDHs and workloads)</li>
<li><strong><code>Database_generator/</code></strong>: Contains scripts, converters, and utilities for generating and extending the database</li>
</ul>
<h3 id="database-directory-structure">Database Directory Structure</h3>
<pre><code>Database/
├── Workloads/          # Raw workload commands
│   └── &lt;Model&gt;/
│       └── &lt;Origin&gt;/
└── HDHs/               # Corresponding Hybrid Dependency Hypergraphs
    └── &lt;Model&gt;/
        └── &lt;Origin&gt;/
            ├── pkl/            # Pickled HDH objects
            ├── text/           # Human-readable CSV files
            ├── images/         # (reserved for future visualizations)
            └── Partitions/     # Partitioning method results
                ├── partitions_all.csv
                └── README.md
</code></pre>
<p>where:
* <strong>Model</strong> = computational model (e.g., Circuit, MBQC, QW, QCA)
* <strong>Origin</strong> = source of the workload (e.g., benchmark suite, custom circuit)</p>
<h3 id="database_generator-directory">Database_generator Directory</h3>
<p>The <code>Database_generator/</code> folder contains:
* Conversion scripts (QASM → HDH)
* Partitioning evaluation scripts
* Utilities for batch processing
* Configuration templates</p>
<p>The database currently contains:</p>
<ul>
<li>HDHs derived from the <a href="https://www.cda.cit.tum.de/mqtbench/">Munich Quantum Benchmarking Dataset</a></li>
</ul>
<h2 id="file-formats">File Formats</h2>
<h3 id="workloads-directory">Workloads Directory</h3>
<ul>
<li>QASM files representing the quantum workloads</li>
</ul>
<h3 id="hdhs-directory">HDHs Directory</h3>
<ul>
<li><strong><code>.pkl</code></strong>: Python-pickled <code>HDH</code> objects for programmatic use</li>
<li><strong><code>.txt</code></strong> / <strong><code>.csv</code></strong>: Human-readable text files with annotated metadata</li>
<li><code>__nodes.csv</code>: Node information (node_id, type, time, realisation)</li>
<li><code>__edges.csv</code>: Edge information (edge_index, type, realisation, gate_name, role, edge_args, edge_metadata)</li>
<li><code>__edge_members.csv</code>: Edge-node relationships (edge_index, node_id)</li>
</ul>
<h3 id="partitions-directory">Partitions Directory</h3>
<ul>
<li><strong><code>partitions_all.csv</code></strong>: Partitioning results from various methods</li>
<li><strong><code>README.md</code></strong>: Documentation of partitioning methods used</li>
</ul>
<h2 id="hdh-metadata">HDH Metadata</h2>
<p>Each HDH includes metadata describing:</p>
<ul>
<li><strong>Model type</strong>: Which computational model the HDH was generated from</li>
<li><strong>Workload origin</strong>: Reference to the source workload</li>
<li><strong>Hybrid status</strong>: Whether the HDH contains both quantum and classical nodes</li>
<li><strong>Node count</strong>: Total number of nodes in the hypergraph</li>
<li><strong>Connectivity degree</strong>: Average connectivity of the hypergraph</li>
<li><strong>Disconnected subgraphs</strong>: Number of disconnected components</li>
</ul>
<h2 id="partitioning-performance-metrics">Partitioning Performance Metrics</h2>
<p>Thanks to the recent additions in PR #24, the library now provides comprehensive metrics for evaluating partitioning quality. These metrics can be computed and added to the database to build a performance baseline.</p>
<h3 id="available-metrics-from-hdhpasses">Available Metrics (from <code>hdh.passes</code>)</h3>
<h4 id="1-costhdh_graph-partitions-tuplefloat-float">1. <strong><code>cost(hdh_graph, partitions)</code></strong> → <code>Tuple[float, float]</code></h4>
<p>Returns <code>(cost_q, cost_c)</code> - the quantum and classical cut costs:
* <code>cost_q</code>: Number of quantum hyperedges that span multiple partitions
* <code>cost_c</code>: Number of classical hyperedges that span multiple partitions</p>
<p>This is the <strong>primary metric</strong> for comparing partitioning methods.</p>
<h4 id="2-partition_sizepartitions-listint">2. <strong><code>partition_size(partitions)</code></strong> → <code>List[int]</code></h4>
<p>Returns the size (number of nodes) of each partition.
Useful for checking balance constraints.</p>
<h4 id="3-participationhdh_graph-partitions-dictstr-float">3. <strong><code>participation(hdh_graph, partitions)</code></strong> → <code>Dict[str, float]</code></h4>
<p>Measures temporal participation (which partitions have activity at each timestep).
<strong>Note</strong>: This measures presence, not true computational parallelism.</p>
<p>Returns:
* <code>max_participation</code>: Peak number of active partitions
* <code>average_participation</code>: Mean active partitions per timestep
* <code>temporal_efficiency</code>: How well time is utilized
* <code>partition_utilization</code>: Average fraction of partitions active
* <code>timesteps</code>: Total timesteps
* <code>num_partitions</code>: Number of partitions</p>
<h4 id="4-parallelismhdh_graph-partitions-dictstr-float">4. <strong><code>parallelism(hdh_graph, partitions)</code></strong> → <code>Dict[str, float]</code></h4>
<p>Measures <strong>true parallelism</strong> by counting concurrent τ-edges (operations) per timestep.
This represents actual computational work that can execute simultaneously.</p>
<p>Returns:
* <code>max_parallelism</code>: Peak concurrent operations
* <code>average_parallelism</code>: Mean operations per timestep
* <code>total_operations</code>: Total operation count
* <code>timesteps</code>: Total timesteps
* <code>num_partitions</code>: Number of partitions</p>
<h4 id="5-fair_parallelismhdh_graph-partitions-capacities-dictstr-float">5. <strong><code>fair_parallelism(hdh_graph, partitions, capacities)</code></strong> → <code>Dict[str, float]</code></h4>
<p>Implements <strong>Jean's fairness principle</strong> - normalizes parallelism by partition capacity to detect workload imbalances.</p>
<p>Returns:
* <code>max_fair_parallelism</code>: Peak fair parallelism
* <code>average_fair_parallelism</code>: Mean fair parallelism
* <code>fairness_ratio</code>: Distribution fairness (1.0 = perfectly fair)
* <code>total_operations</code>: Total operation count
* <code>timesteps</code>: Total timesteps
* <code>num_partitions</code>: Number of partitions</p>
<h3 id="usage-example">Usage Example</h3>
<pre><code class="language-python">from hdh.passes import (
    cost, partition_size, 
    participation, parallelism, fair_parallelism
)

# After running your partitioning method
bins, _, _, _ = your_partitioning_method(hdh_graph, k=3)

# Evaluate the partition
cost_q, cost_c = cost(hdh_graph, bins)
sizes = partition_size(bins)
participation_metrics = participation(hdh_graph, bins)
parallelism_metrics = parallelism(hdh_graph, bins)
fair_metrics = fair_parallelism(hdh_graph, bins, capacities=[10, 10, 10])

print(f&quot;Quantum cut cost: {cost_q}&quot;)
print(f&quot;Classical cut cost: {cost_c}&quot;)
print(f&quot;Partition sizes: {sizes}&quot;)
print(f&quot;Average parallelism: {parallelism_metrics['average_parallelism']}&quot;)
print(f&quot;Fairness ratio: {fair_metrics['fairness_ratio']}&quot;)
</code></pre>
<h2 id="extending-the-dataset">Extending the Dataset</h2>
<p>We encourage users to:</p>
<ul>
<li>Add new workloads (QASM or <a href="../models/">other supported formats</a>)</li>
<li>Generate corresponding HDHs</li>
<li>Run partitioning methods and contribute results</li>
<li>Propose and document new metrics</li>
</ul>
<p>Pull requests that expand the benchmark set or enrich metadata are very welcome!</p>
<h3 id="how-to-add-to-this-database">How to Add to This Database</h3>
<p>There are two ways to contribute:</p>
<hr />
<h2 id="1-add-new-workloads-hdhs">1) Add New Workloads + HDHs</h2>
<h4 id="step-1-place-workloads">Step 1: Place Workloads</h4>
<p>Put your workload origin files under:  </p>
<pre><code>Database/Workloads/&lt;Model&gt;/&lt;Origin&gt;/
</code></pre>
<p>This could be anything from a QASM file to circuit generation code.</p>
<p>If the HDH is not generated from functions within the library, we request you add a <code>README.md</code> to your origin folder explaining how the HDHs were generated.</p>
<p>Example:  </p>
<pre><code>Database/Workloads/Circuits/MQTBench/qft_8.qasm
</code></pre>
<h4 id="step-2-run-the-converter">Step 2: Run the Converter</h4>
<p>Convert the files (QASM strings, Qiskit circuits, etc.) to HDHs.</p>
<p>The converter will create:</p>
<pre><code>Database/HDHs/&lt;Model&gt;/&lt;Origin&gt;/pkl/&lt;filename&gt;.pkl
Database/HDHs/&lt;Model&gt;/&lt;Origin&gt;/text/&lt;filename&gt;__nodes.csv
Database/HDHs/&lt;Model&gt;/&lt;Origin&gt;/text/&lt;filename&gt;__edges.csv
Database/HDHs/&lt;Model&gt;/&lt;Origin&gt;/text/&lt;filename&gt;__edge_members.csv
</code></pre>
<h5 id="converter-script-qasm-hdh-pklcsv">Converter Script (QASM → HDH → {pkl,csv})</h5>
<p>The converter script is available in <code>Database_generator/</code> folder. Requirements: tqdm, the HDH library available on PYTHONPATH, and your QASM converter (<code>hdh.converters.from_qasm</code>).</p>
<pre><code class="language-python">#!/usr/bin/env python3
import sys
import os
import csv
import json
import pickle
from pathlib import Path
from tqdm import tqdm
import argparse

# Repo import path (adjust as needed)
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
from hdh.converters import from_qasm

BASE_DIR = Path(__file__).resolve().parent

def ensure_dir(p: Path):
    p.mkdir(parents=True, exist_ok=True)

def save_pkl(hdh_graph, out_base: Path):
    p = out_base.with_suffix(&quot;.pkl&quot;)
    with open(p, &quot;wb&quot;) as f:
        pickle.dump(hdh_graph, f)
    return p

def save_nodes_csv(hdh_graph, out_path: Path):
    with open(out_path, &quot;w&quot;, newline=&quot;&quot;, encoding=&quot;utf-8&quot;) as f:
        w = csv.writer(f)
        w.writerow([&quot;node_id&quot;, &quot;type&quot;, &quot;time&quot;, &quot;realisation&quot;])
        for nid in sorted(hdh_graph.S):
            w.writerow([
                nid,
                hdh_graph.sigma.get(nid, &quot;&quot;),
                getattr(hdh_graph, &quot;time_map&quot;, {}).get(nid, &quot;&quot;),
                hdh_graph.upsilon.get(nid, &quot;&quot;)
            ])

def save_edges_csvs(hdh_graph, edges_path: Path, members_path: Path):
    edges_sorted = sorted(hdh_graph.C, key=lambda e: tuple(sorted(e)))

    # edges table
    with open(edges_path, &quot;w&quot;, newline=&quot;&quot;, encoding=&quot;utf-8&quot;) as f:
        w = csv.writer(f)
        w.writerow([
            &quot;edge_index&quot;, &quot;type&quot;, &quot;realisation&quot;, &quot;gate_name&quot;,
            &quot;role&quot;, &quot;edge_args&quot;, &quot;edge_metadata&quot;
        ])
        for idx, e in enumerate(edges_sorted):
            w.writerow([
                idx,
                hdh_graph.tau.get(e, &quot;&quot;),
                hdh_graph.phi.get(e, &quot;&quot;),
                getattr(hdh_graph, &quot;gate_name&quot;, {}).get(e, &quot;&quot;),
                getattr(hdh_graph, &quot;edge_role&quot;, {}).get(e, &quot;&quot;),
                json.dumps(getattr(hdh_graph, &quot;edge_args&quot;, {}).get(e, None)) if e in getattr(hdh_graph, &quot;edge_args&quot;, {}) else &quot;&quot;,
                json.dumps(getattr(hdh_graph, &quot;edge_metadata&quot;, {}).get(e, None)) if e in getattr(hdh_graph, &quot;edge_metadata&quot;, {}) else &quot;&quot;
            ])

    # edge_members table
    with open(members_path, &quot;w&quot;, newline=&quot;&quot;, encoding=&quot;utf-8&quot;) as f:
        w = csv.writer(f)
        w.writerow([&quot;edge_index&quot;, &quot;node_id&quot;])
        for idx, e in enumerate(edges_sorted):
            for nid in sorted(e):
                w.writerow([idx, nid])

def main():
    ap = argparse.ArgumentParser(description=&quot;Convert QASM workloads to HDH artifacts&quot;)
    ap.add_argument(&quot;--model&quot;, default=&quot;Circuits&quot;, help=&quot;Model folder&quot;)
    ap.add_argument(&quot;--origin&quot;, default=&quot;MQTBench&quot;, help=&quot;Origin folder&quot;)
    ap.add_argument(&quot;--limit&quot;, type=int, default=None, help=&quot;Max files to convert&quot;)
    ap.add_argument(&quot;--src-root&quot;, default=None, help=&quot;Override source root&quot;)
    args = ap.parse_args()

    SRC_DIR = Path(args.src_root) if args.src_root else BASE_DIR / &quot;Database&quot; / &quot;Workloads&quot; / args.model / args.origin
    DST_ROOT = BASE_DIR / &quot;Database&quot; / &quot;HDHs&quot; / args.model / args.origin
    PKL_ROOT = DST_ROOT / &quot;pkl&quot;
    TXT_ROOT = DST_ROOT / &quot;text&quot;
    IMG_ROOT = DST_ROOT / &quot;images&quot;

    if not SRC_DIR.exists():
        print(f&quot;[error] Source directory not found: {SRC_DIR}&quot;)
        sys.exit(1)

    for d in (PKL_ROOT, TXT_ROOT, IMG_ROOT):
        ensure_dir(d)

    qasm_files = sorted(SRC_DIR.rglob(&quot;*.qasm&quot;))
    if not qasm_files:
        print(f&quot;[info] No .qasm files found under {SRC_DIR}&quot;)
        return

    if args.limit is not None:
        qasm_files = qasm_files[:args.limit]

    ok = fail = 0
    with tqdm(total=len(qasm_files), desc=&quot;Converting QASM → HDH&quot;, unit=&quot;file&quot;) as pbar:
        for qf in qasm_files:
            rel = qf.relative_to(SRC_DIR)
            stem = rel.stem
            pkl_dir = PKL_ROOT / rel.parent
            txt_dir = TXT_ROOT / rel.parent
            for d in (pkl_dir, txt_dir):
                ensure_dir(d)
            pbar.set_postfix_str(str(rel))
            try:
                hdh_graph = from_qasm(&quot;file&quot;, str(qf))
                save_pkl(hdh_graph, pkl_dir / stem)
                save_nodes_csv(hdh_graph, txt_dir / f&quot;{stem}__nodes.csv&quot;)
                save_edges_csvs(hdh_graph, txt_dir / f&quot;{stem}__edges.csv&quot;, txt_dir / f&quot;{stem}__edge_members.csv&quot;)
                ok += 1
            except Exception as e:
                tqdm.write(f&quot;[fail] {qf}: {e}&quot;)
                fail += 1
            finally:
                pbar.update(1)

    print(f&quot;[done] Converted: {ok} | Failed: {fail}&quot;)

if __name__ == &quot;__main__&quot;:
    main()
</code></pre>
<h4 id="step-3-verify-inspect">Step 3: Verify &amp; Inspect</h4>
<p>Please open at least one of the <code>text/*.csv</code> files and load at least one of the <code>pkl/*.pkl</code> objects in Python to verify everything works.</p>
<pre><code class="language-python">import pickle
import pandas as pd

# Load pickled HDH
with open(&quot;Database/HDHs/Circuits/MQTBench/pkl/qft_8.pkl&quot;, &quot;rb&quot;) as f:
    hdh = pickle.load(f)

# Load CSV files
nodes_df = pd.read_csv(&quot;Database/HDHs/Circuits/MQTBench/text/qft_8__nodes.csv&quot;)
edges_df = pd.read_csv(&quot;Database/HDHs/Circuits/MQTBench/text/qft_8__edges.csv&quot;)
members_df = pd.read_csv(&quot;Database/HDHs/Circuits/MQTBench/text/qft_8__edge_members.csv&quot;)
</code></pre>
<h4 id="step-4-submit-a-pr">Step 4: Submit a PR</h4>
<p>If all went smoothly, submit a PR with your workloads and HDHs back to the <code>main</code> branch.</p>
<p><strong>Note</strong>: Database files might be too large to directly upload. Use <a href="https://docs.github.com/en/repositories/working-with-files/managing-large-files/configuring-git-large-file-storage">Git LFS</a>:</p>
<p><strong>macOS:</strong></p>
<pre><code class="language-bash">brew install git-lfs
git lfs install
</code></pre>
<p><strong>Debian/Ubuntu:</strong></p>
<pre><code class="language-bash">sudo apt-get update
sudo apt-get install git-lfs
git lfs install
</code></pre>
<p><strong>Windows:</strong></p>
<pre><code class="language-bash">winget install Git.GitLFS
git lfs install
</code></pre>
<p>From repo root:</p>
<pre><code class="language-bash">git lfs track &quot;*.csv&quot;
git lfs track &quot;*.pkl&quot;
git add .gitattributes
git commit -m &quot;Adding &lt;Origin&gt; HDHs to database&quot;
git push -u origin main
</code></pre>
<hr />
<h2 id="2-add-partitioning-method-results">2) Add Partitioning Method Results</h2>
<p>If you want to share <strong>partitioning method results</strong>, add them to:</p>
<pre><code>Database/HDHs/&lt;Model&gt;/&lt;Origin&gt;/Partitions/partitions_all.csv
</code></pre>
<h3 id="csv-format-for-partitioning-results">CSV Format for Partitioning Results</h3>
<p>The <code>partitions_all.csv</code> file tracks performance metrics across different partitioning methods.</p>
<h4 id="mandatory-columns">Mandatory Columns</h4>
<ul>
<li><strong><code>file</code></strong>: Name of the origin file</li>
<li><strong><code>n_qubits</code></strong>: Number of qubits in workload</li>
<li><strong><code>k_partitions</code></strong>: Number of partitions (e.g., 2 if cut once)</li>
<li><strong><code>&lt;method&gt;_bins</code></strong>: Sets of qubits per partition (JSON format)</li>
<li><strong><code>&lt;method&gt;_cost</code></strong>: Quantum communication cost (number of quantum hyperedges cut)</li>
<li><strong><code>best</code></strong>: Name of the method with the lowest cost</li>
</ul>
<h4 id="optional-columns-method-specific">Optional Columns (Method-Specific)</h4>
<ul>
<li><strong><code>&lt;method&gt;_cost_q</code></strong>: Quantum cut cost (if separating q/c)</li>
<li><strong><code>&lt;method&gt;_cost_c</code></strong>: Classical cut cost</li>
<li><strong><code>&lt;method&gt;_partition_sizes</code></strong>: List of partition sizes</li>
<li><strong><code>&lt;method&gt;_avg_parallelism</code></strong>: Average parallelism metric</li>
<li><strong><code>&lt;method&gt;_fairness_ratio</code></strong>: Fairness ratio from fair_parallelism</li>
<li><strong><code>&lt;method&gt;_fails</code></strong>: Boolean indicating if method failed capacity constraints</li>
<li><strong><code>&lt;method&gt;_method</code></strong>: Sub-method used (e.g., for METIS: 'kl', 'recursive')</li>
<li><strong><code>contributor</code></strong>: GitHub username of the person who added this result</li>
</ul>
<h3 id="example-csv">Example CSV</h3>
<pre><code class="language-csv">file,n_qubits,k_partitions,greedy_bins,greedy_cost,metis_bins,metis_cost,metis_fails,metis_method,greedytg_bins,greedytg_cost,best,contributor
ae_indep_qiskit_10.qasm,10,2,&quot;[[&quot;&quot;q0&quot;&quot;,&quot;&quot;q1&quot;&quot;,&quot;&quot;q2&quot;&quot;,&quot;&quot;q3&quot;&quot;,&quot;&quot;q8&quot;&quot;],[&quot;&quot;q4&quot;&quot;,&quot;&quot;q5&quot;&quot;,&quot;&quot;q6&quot;&quot;,&quot;&quot;q7&quot;&quot;,&quot;&quot;q9&quot;&quot;]]&quot;,30,&quot;[[&quot;&quot;q1&quot;&quot;,&quot;&quot;q3&quot;&quot;,&quot;&quot;q5&quot;&quot;,&quot;&quot;q6&quot;&quot;,&quot;&quot;q7&quot;&quot;],[&quot;&quot;q0&quot;&quot;,&quot;&quot;q2&quot;&quot;,&quot;&quot;q4&quot;&quot;,&quot;&quot;q8&quot;&quot;,&quot;&quot;q9&quot;&quot;]]&quot;,25,False,kl,&quot;[[&quot;&quot;q0&quot;&quot;,&quot;&quot;q1&quot;&quot;,&quot;&quot;q2&quot;&quot;,&quot;&quot;q3&quot;&quot;,&quot;&quot;q9&quot;&quot;],[&quot;&quot;q4&quot;&quot;,&quot;&quot;q5&quot;&quot;,&quot;&quot;q6&quot;&quot;,&quot;&quot;q7&quot;&quot;,&quot;&quot;q8&quot;&quot;]]&quot;,30,metis,alice_researcher
ae_indep_qiskit_10.qasm,10,3,&quot;[[&quot;&quot;q0&quot;&quot;,&quot;&quot;q1&quot;&quot;,&quot;&quot;q2&quot;&quot;,&quot;&quot;q8&quot;&quot;],[&quot;&quot;q3&quot;&quot;,&quot;&quot;q4&quot;&quot;,&quot;&quot;q6&quot;&quot;,&quot;&quot;q7&quot;&quot;],[&quot;&quot;q5&quot;&quot;,&quot;&quot;q9&quot;&quot;]]&quot;,40,&quot;[[&quot;&quot;q3&quot;&quot;,&quot;&quot;q5&quot;&quot;,&quot;&quot;q6&quot;&quot;,&quot;&quot;q7&quot;&quot;],[&quot;&quot;q0&quot;&quot;,&quot;&quot;q2&quot;&quot;],[&quot;&quot;q1&quot;&quot;,&quot;&quot;q4&quot;&quot;,&quot;&quot;q8&quot;&quot;,&quot;&quot;q9&quot;&quot;]]&quot;,32,False,kl,&quot;[[&quot;&quot;q0&quot;&quot;,&quot;&quot;q1&quot;&quot;,&quot;&quot;q2&quot;&quot;,&quot;&quot;q9&quot;&quot;],[&quot;&quot;q3&quot;&quot;,&quot;&quot;q4&quot;&quot;,&quot;&quot;q5&quot;&quot;,&quot;&quot;q6&quot;&quot;],[&quot;&quot;q7&quot;&quot;,&quot;&quot;q8&quot;&quot;]]&quot;,38,metis,alice_researcher
</code></pre>
<h3 id="standard-partitioning-methods">Standard Partitioning Methods</h3>
<p>Here are the standard partitioning methods currently in the database:</p>
<h4 id="greedy-hdh"><strong>Greedy (HDH)</strong></h4>
<p>Partitions directly on the HDH hypergraph where each hyperedge captures one operation's dependency set.
We fill bins sequentially: order qubits by heuristic (e.g., incident cut weight, then degree), and place each into the earliest bin that (i) respects the logical-qubit capacity and (ii) gives the smallest marginal cut increase.
If nothing fits, open the next bin up to k.</p>
<p><strong>Cost</strong>: Sum of weights of hyperedges spanning &gt;1 bin (default weight 1 per op; domain weights optional).</p>
<h4 id="metis-telegate-graph"><strong>METIS (Telegate graph)</strong></h4>
<p>Converts the workload into a telegate qubit-interaction graph (nodes = logical qubits; edge weights = interaction pressure indicating a non-local gate would require a "telegate" communication if cut).
Uses the <a href="https://pypi.org/project/metis/">METIS library</a> to compute a k-way partition with balance constraints and minimal cut on this graph.
Partitions are then re-evaluated on the HDH cost for apples-to-apples comparison.</p>
<p><strong>Cost</strong>: Re-evaluated on HDH hypergraph cut metric.</p>
<h4 id="greedy-tg-telegate-graph"><strong>Greedy-TG (Telegate graph)</strong></h4>
<p>Same fill-first policy as Greedy (HDH), but decisions are made on the telegate graph.
Nodes are qubits; edge weights reflect how costly it is to separate two qubits (expected telegate load).
Each qubit goes to the earliest feasible bin that minimizes marginal cut on the telegate graph.</p>
<p><strong>Cost</strong>: Re-evaluated on HDH hypergraph cut metric.</p>
<h3 id="adding-your-results">Adding Your Results</h3>
<h4 id="step-1-run-your-partitioning-method">Step 1: Run Your Partitioning Method</h4>
<pre><code class="language-python">from hdh.passes import compute_cut, cost, partition_size, parallelism, fair_parallelism
import pickle
import csv

# Load HDH
with open(&quot;Database/HDHs/Circuits/MQTBench/pkl/qft_8.pkl&quot;, &quot;rb&quot;) as f:
    hdh = pickle.load(f)

# Run your partitioning method
bins, _, _, method_name = your_partitioning_method(hdh, k=3, capacity=3)

# Compute metrics
cost_q, cost_c = cost(hdh, bins)
sizes = partition_size(bins)
parallel_metrics = parallelism(hdh, bins)
fair_metrics = fair_parallelism(hdh, bins, capacities=[3, 3, 3])

# Prepare data for CSV
result = {
    'file': 'qft_8.qasm',
    'n_qubits': 8,
    'k_partitions': 3,
    'yourmethod_bins': str(bins),
    'yourmethod_cost': cost_q,
    'yourmethod_cost_q': cost_q,
    'yourmethod_cost_c': cost_c,
    'yourmethod_partition_sizes': str(sizes),
    'yourmethod_avg_parallelism': parallel_metrics['average_parallelism'],
    'yourmethod_fairness_ratio': fair_metrics['fairness_ratio'],
    'contributor': 'your_github_username'
}
</code></pre>
<h4 id="step-2-update-partitions_allcsv">Step 2: Update partitions_all.csv</h4>
<p>Add your results to the existing CSV file:</p>
<pre><code class="language-python">import pandas as pd

# Load existing results
df = pd.read_csv(&quot;Database/HDHs/Circuits/MQTBench/Partitions/partitions_all.csv&quot;)

# Add your new column(s) if they don't exist
# Update or append your row
# Recalculate 'best' column

df.to_csv(&quot;Database/HDHs/Circuits/MQTBench/Partitions/partitions_all.csv&quot;, index=False)
</code></pre>
<h4 id="step-3-document-your-method">Step 3: Document Your Method</h4>
<p>Create or update <code>Database/HDHs/&lt;Model&gt;/&lt;Origin&gt;/Partitions/README.md</code>:</p>
<pre><code class="language-markdown"># Partitioning Methods for &lt;Origin&gt;

## Your Method Name

**Contributor**: your_github_username
**Date**: YYYY-MM-DD

### Description
Brief description of your partitioning algorithm...

### Parameters
- Parameter 1: description
- Parameter 2: description

### Implementation Details
Link to code or detailed explanation...

### Performance Characteristics
- Time complexity: O(...)
- Space complexity: O(...)
- Works best for: ...
</code></pre>
<h4 id="step-4-capacity-constraints-and-failure-states">Step 4: Capacity Constraints and Failure States</h4>
<p><strong>Capacity</strong> = Total qubits ÷ number of partitions (rounded up)</p>
<p>If your partitioner cannot respect this capacity:
* Log a failure status in a <code>&lt;method&gt;_fails</code> column
* Set to <code>True</code> if capacity was violated
* Exclude from <code>best</code> evaluation if failed</p>
<p><strong>Important</strong>: Document in the Partitions README whether your method:
* Always respects capacity constraints
* May violate constraints (and how failures are handled)
* Requires specific capacity settings</p>
<h4 id="step-5-recalculate-best">Step 5: Recalculate Best</h4>
<p>The <code>best</code> column should identify the method with the <strong>lowest quantum cost</strong> among methods that:
1. Did not fail capacity constraints (where <code>&lt;method&gt;_fails</code> is False or not present)
2. Successfully completed partitioning</p>
<pre><code class="language-python"># Example: Recalculate best
cost_columns = [col for col in df.columns if col.endswith('_cost') and not col.endswith('_cost_c')]
methods = [col.replace('_cost', '') for col in cost_columns]

def get_best_method(row):
    valid_methods = []
    for method in methods:
        fails_col = f'{method}_fails'
        cost_col = f'{method}_cost'

        # Check if method failed
        if fails_col in row and row[fails_col] == True:
            continue

        # Check if cost is valid
        if pd.notna(row[cost_col]):
            valid_methods.append((method, row[cost_col]))

    if not valid_methods:
        return None

    # Return method with minimum cost
    return min(valid_methods, key=lambda x: x[1])[0]

df['best'] = df.apply(get_best_method, axis=1)
</code></pre>
<h4 id="step-6-submit-pr">Step 6: Submit PR</h4>
<p>Submit a PR to the <code>main</code> branch with:
* Updated <code>partitions_all.csv</code>
* Updated or new <code>README.md</code> in the Partitions folder
* Your GitHub username in the <code>contributor</code> column</p>
<hr />
<h2 id="database-usage-guidelines">Database Usage Guidelines</h2>
<h3 id="for-benchmarking">For Benchmarking</h3>
<pre><code class="language-python">import pickle
import pandas as pd
from pathlib import Path

# Load all HDHs from an origin
origin_path = Path(&quot;Database/HDHs/Circuits/MQTBench/pkl&quot;)
hdhs = {}
for pkl_file in origin_path.glob(&quot;*.pkl&quot;):
    with open(pkl_file, &quot;rb&quot;) as f:
        hdhs[pkl_file.stem] = pickle.load(f)

# Load partitioning results
results_df = pd.read_csv(&quot;Database/HDHs/Circuits/MQTBench/Partitions/partitions_all.csv&quot;)

# Benchmark your method against existing results
for name, hdh in hdhs.items():
    your_bins, _, _, _ = your_method(hdh, k=3)
    your_cost, _ = cost(hdh, your_bins)

    # Compare with best existing method
    existing_best = results_df[results_df['file'] == f&quot;{name}.qasm&quot;]['best'].values[0]
    existing_cost = results_df[results_df['file'] == f&quot;{name}.qasm&quot;][f&quot;{existing_best}_cost&quot;].values[0]

    print(f&quot;{name}: Your method: {your_cost}, Best existing: {existing_cost}&quot;)
</code></pre>
<h3 id="citation-and-acknowledgment">Citation and Acknowledgment</h3>
<p>When using this database in publications, please cite:
* The HDH library
* The Munich Quantum Benchmarking Dataset (for MQTBench workloads)
* Individual contributors whose partitioning results you use</p>
<h3 id="data-license">Data License</h3>
<p>The database is provided under the same license as the HDH library (MIT License).
Individual workloads may have their own licenses - check the origin-specific README files.</p>
<hr />
<h2 id="contributing">Contributing</h2>
<p>We welcome contributions! When adding to the database:</p>
<ol>
<li>✅ Use clear, descriptive commit messages</li>
<li>✅ Document your methods thoroughly in README files</li>
<li>✅ Include your GitHub username as contributor</li>
<li>✅ Verify your data loads correctly before submitting</li>
<li>✅ Update this documentation if adding new features</li>
</ol>
<p>For questions or discussions, please open an issue on the main repository.</p>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../vis/" class="btn btn-neutral float-left" title="Visualizing HDHs"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../passes/" class="btn btn-neutral float-right" title="Partitioning HDHs for distribution">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="../vis/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../passes/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "..";</script>
    <script src="../js/theme_extra.js"></script>
    <script src="../js/theme.js"></script>
      <script src="../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
