{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the HDH library HDH refers to Hybrid Dependency Hypergraph , an abstraction developped to enable the partitioning of quantum computations in the context of Distributed Quantum Computing. HDHs are a directed hypergraph based abstraction that encodes the dependencies generated by entangling quantum operations displaying the state transformations performed along the computation. They aim to serve as a unifying abstraction capable of encoding any quantum workload regardless of the computational model it is designed in, that enables all valid partitions of a computation (superseding telegate and teledata abstractions). Furthermore HDHs, as their name implies, also encode classical information enabling the outline of natural classical partitioning points, such as mid-circuit measurements. You can find an in depth description of HDHs as an abstraction here: An introduction to HDHs . A more in-depth explanation of why HDHs make sense can be found here: Why HDHs? Further explanations of how HDHs are generated from quantum computational models can be found here: Generation of HDHs from model instructions . The source code can be found: https://github.com/grageragarces/HDH . If you find any bugs or have any proposals for the library we encourage you to open an issue. A guide on how to do this can be found here . HDHs were originally developped by Maria Gragera Garces, Chris Heunen and Mahesh K. Marina. Publications, posters and talks related to the HDH project can be found here: Literature . The library is currently under MIT License. The development of this library was kindly supported by a Unitary Fund microgrant, as well as the Engineering and Physical Sciences Research Council (grant number EP/W524384/1), the University of Edinburgh, and VeriQloud .","title":"Home"},{"location":"#welcome-to-the-hdh-library","text":"HDH refers to Hybrid Dependency Hypergraph , an abstraction developped to enable the partitioning of quantum computations in the context of Distributed Quantum Computing. HDHs are a directed hypergraph based abstraction that encodes the dependencies generated by entangling quantum operations displaying the state transformations performed along the computation. They aim to serve as a unifying abstraction capable of encoding any quantum workload regardless of the computational model it is designed in, that enables all valid partitions of a computation (superseding telegate and teledata abstractions). Furthermore HDHs, as their name implies, also encode classical information enabling the outline of natural classical partitioning points, such as mid-circuit measurements. You can find an in depth description of HDHs as an abstraction here: An introduction to HDHs . A more in-depth explanation of why HDHs make sense can be found here: Why HDHs? Further explanations of how HDHs are generated from quantum computational models can be found here: Generation of HDHs from model instructions . The source code can be found: https://github.com/grageragarces/HDH . If you find any bugs or have any proposals for the library we encourage you to open an issue. A guide on how to do this can be found here . HDHs were originally developped by Maria Gragera Garces, Chris Heunen and Mahesh K. Marina. Publications, posters and talks related to the HDH project can be found here: Literature . The library is currently under MIT License. The development of this library was kindly supported by a Unitary Fund microgrant, as well as the Engineering and Physical Sciences Research Council (grant number EP/W524384/1), the University of Edinburgh, and VeriQloud .","title":"Welcome to the HDH library"},{"location":"hdh/","text":"Hybrid Dependency Hypergraphs HDHs are a special type of directed hypergraphs designed to encode temporal and spatial dependencies of quantum computations with the purpose of distributing originally monolothic quantum or hybrid workloads. Hypergraph abstractions are currently the standard approach for partitioning quantum computations in distributed quantum computing, [ 1 ]. HDHs extend this practice by providing a consistent and complete representation framework, intended to support the systematic evaluation and comparison of partitioning strategies. A hypergraph consists of a set of nodes and a family of subsets of these nodes, called hyperedges[ 2 ]. In HDHs these hyperedges can be refered to as connections, as they represent the connectivity requirements between nodes. Meaning that if a HDH is partitioned, the operations represented as inter-partition hyperedges would need to be replaced by equivalent communication primitives (such as a non-local gate or a teleportation protocol). Hypergraphs are made up of nodes and hyperedges that can either be quantum or classical (representing qubits or bits/cbits) and realised or potential . These types q/c and r/a enable the representation of any quantum of hybrid computation. When visualized they will be reprented by these symbols: Quantum operations may give rise to single-type operations, for instance a single qubit gate in the circuit model would give rise a quantum node and hyperedge, or two multi-type operations: the initialization of a qubit based on a previous measurement reading would lead to a classical node and a quantum node. The r/a types are useful for representing instructions that may \"potentially\" happen given previous results, such as IfElse operations found in dynamic circuits.","title":"A brief introduction to HDHs"},{"location":"hdh/#hybrid-dependency-hypergraphs","text":"HDHs are a special type of directed hypergraphs designed to encode temporal and spatial dependencies of quantum computations with the purpose of distributing originally monolothic quantum or hybrid workloads. Hypergraph abstractions are currently the standard approach for partitioning quantum computations in distributed quantum computing, [ 1 ]. HDHs extend this practice by providing a consistent and complete representation framework, intended to support the systematic evaluation and comparison of partitioning strategies. A hypergraph consists of a set of nodes and a family of subsets of these nodes, called hyperedges[ 2 ]. In HDHs these hyperedges can be refered to as connections, as they represent the connectivity requirements between nodes. Meaning that if a HDH is partitioned, the operations represented as inter-partition hyperedges would need to be replaced by equivalent communication primitives (such as a non-local gate or a teleportation protocol). Hypergraphs are made up of nodes and hyperedges that can either be quantum or classical (representing qubits or bits/cbits) and realised or potential . These types q/c and r/a enable the representation of any quantum of hybrid computation. When visualized they will be reprented by these symbols: Quantum operations may give rise to single-type operations, for instance a single qubit gate in the circuit model would give rise a quantum node and hyperedge, or two multi-type operations: the initialization of a qubit based on a previous measurement reading would lead to a classical node and a quantum node. The r/a types are useful for representing instructions that may \"potentially\" happen given previous results, such as IfElse operations found in dynamic circuits.","title":"Hybrid Dependency Hypergraphs"},{"location":"intro/","text":"An introduction to Distributed Quantum Computing The main problem behind distributed quantum computing is how to map quantum workloads to a network of quantum devices. Unlike its classical counterpart, the so-called mapping problem must account for both classical and quantum data and operations within the workload, as well as quantum and classical devices and connections within the network. Two main strategies have arisem in this context: Bespoke algorithm design: splitting the workload into smaller sub-workloads defined by algorithmic tasks (as in distributed versions of Shor\u2019s and Grover\u2019s algorithms) or by subproblems (something more along the lines of partitioning a large Hamiltonian into sub-Hamiltonians and communication terms [ 1 ]). Hypergraph abstractions: abstracting a quantum circuit into a hypergraph to then partition it with max-cut heuristics [ 2 ], replacing every cut hyperedge with an equivalent communication primitive (a non-local gate when partitioning through a gate, or a teleportation protocol when partitioning through a wire). This library contributes to the second strategy, which unlike bespoke algorithmic designs, does not require knowledge of the problem structure or algorithmic logic, and can be applied to any compiled quantum program. In this context the mapping problem can be defined as: Even in a simplified setting where only communication minimization is considered, the network mapping problem is computationally intractable. In particular, the decision version of the problem is NP-hard, even for a fixed network topology of only two nodes with equal capacity, where the objective reduces to partitioning V_C into two equal-sized sets while minimizing the number of hyperedges crossing between them. This case is equivalent to the sparsest cut problem with unit capacities, which is NP-hard via a reduction from the max-cut problem [ 3 ]. Our best strategy, therefore, is to design fast and efficient heuristic partitioners. State-of-the-art partitioning strategies include Fiduccia\u2013Mattheyses and multilevel partitioners (such as KaHyPar ). We recommend this review if you want to learn more about how these strategies have been applied to circuit partitioning. Nonetheless, a few bottlenecks remain, making cross-partitioning method comparison difficult. For a start, hypergraphs built for abstracting quantum workloads can be constructed using different conventions, for example, mapping qubits to nodes and multi-qubit gates to hyperedges, or vice versa. These are known as telegate and teledata. This variety of representations has led to inconsistencies that complicate direct comparisons between partitioning methods for distributed quantum circuits. To address this, strategies that can utilize both abstractions have been proposed [ 4 , 5 ], but not an easy-to-use unifying abstraction. Beyond telegate and teledata, restricting to the circuit model also imposes a significant limitation. Photons, the most practical medium for long-range quantum communication\u2014are used in photonic quantum computers that do not operate under the circuit model. As a result, current compiler distribution frameworks are incompatible with key hardware platforms needed for near-term distributed quantum computing testbeds. They exclude alternative computational models required by some quantum devices and cannot accommodate hybrid quantum\u2013classical workloads. HDHs address both challenges and provide a unifying abstraction for representing partitioning across all models. This library is designed to make HDHs easy to construct and to offer useful benchmarking tools to evaluate partitioners. For a concise overview of how HDHs are built, see our brief introduction to HDHs .","title":"An introduction to DQC"},{"location":"intro/#an-introduction-to-distributed-quantum-computing","text":"The main problem behind distributed quantum computing is how to map quantum workloads to a network of quantum devices. Unlike its classical counterpart, the so-called mapping problem must account for both classical and quantum data and operations within the workload, as well as quantum and classical devices and connections within the network. Two main strategies have arisem in this context: Bespoke algorithm design: splitting the workload into smaller sub-workloads defined by algorithmic tasks (as in distributed versions of Shor\u2019s and Grover\u2019s algorithms) or by subproblems (something more along the lines of partitioning a large Hamiltonian into sub-Hamiltonians and communication terms [ 1 ]). Hypergraph abstractions: abstracting a quantum circuit into a hypergraph to then partition it with max-cut heuristics [ 2 ], replacing every cut hyperedge with an equivalent communication primitive (a non-local gate when partitioning through a gate, or a teleportation protocol when partitioning through a wire). This library contributes to the second strategy, which unlike bespoke algorithmic designs, does not require knowledge of the problem structure or algorithmic logic, and can be applied to any compiled quantum program. In this context the mapping problem can be defined as: Even in a simplified setting where only communication minimization is considered, the network mapping problem is computationally intractable. In particular, the decision version of the problem is NP-hard, even for a fixed network topology of only two nodes with equal capacity, where the objective reduces to partitioning V_C into two equal-sized sets while minimizing the number of hyperedges crossing between them. This case is equivalent to the sparsest cut problem with unit capacities, which is NP-hard via a reduction from the max-cut problem [ 3 ]. Our best strategy, therefore, is to design fast and efficient heuristic partitioners. State-of-the-art partitioning strategies include Fiduccia\u2013Mattheyses and multilevel partitioners (such as KaHyPar ). We recommend this review if you want to learn more about how these strategies have been applied to circuit partitioning. Nonetheless, a few bottlenecks remain, making cross-partitioning method comparison difficult. For a start, hypergraphs built for abstracting quantum workloads can be constructed using different conventions, for example, mapping qubits to nodes and multi-qubit gates to hyperedges, or vice versa. These are known as telegate and teledata. This variety of representations has led to inconsistencies that complicate direct comparisons between partitioning methods for distributed quantum circuits. To address this, strategies that can utilize both abstractions have been proposed [ 4 , 5 ], but not an easy-to-use unifying abstraction. Beyond telegate and teledata, restricting to the circuit model also imposes a significant limitation. Photons, the most practical medium for long-range quantum communication\u2014are used in photonic quantum computers that do not operate under the circuit model. As a result, current compiler distribution frameworks are incompatible with key hardware platforms needed for near-term distributed quantum computing testbeds. They exclude alternative computational models required by some quantum devices and cannot accommodate hybrid quantum\u2013classical workloads. HDHs address both challenges and provide a unifying abstraction for representing partitioning across all models. This library is designed to make HDHs easy to construct and to offer useful benchmarking tools to evaluate partitioners. For a concise overview of how HDHs are built, see our brief introduction to HDHs .","title":"An introduction to Distributed Quantum Computing"},{"location":"literature/","text":"HDHs and associated projects have been presented at the following conferences : SIGCOMM25 POSTER: Distributed Quantum Computing Across Heterogeneous Hardware with Hybrid Dependency Hypergraphs Extended Abstract Poster IWQC25 : Hybrid Dependency Hypergraphs: A model agnostic IR for distributed quantum computing at the compilation level","title":"Related publications and information"},{"location":"models/","text":"Computational models Quantum computation can be performed with various computational models that have different instruction sets, such as quantum circuits (which perform computation through gates) or measurement based patterns (which perform computation through corrections). The diversity between these models can be attributed to hardware constraints as well as to the age of the field. Although they are all equivalent and computations can theoretically be translated accross them, this process is inneficient and hard to due to a lack of clear set of cross-model translators. HDHs were designed to abstract all the models into a unified framework. As such, they are model agnostic and can be constructed from any set of instructions. Specific model classes can be found under the hdh/models folder. To facilitate the usage of HDHs, the library has a set of embedded model mappings which translate instruction sets from popular quantum computational models into reusable HDH motifs. You can find details and examples of how to use these model classes and SDK converters bellow. Circuits The circuit model is among the most widely adopted paradigms of quantum computation, particularly in implementations on industrial quantum hardware (with the notable exception of photonic qubits). Quantum circuits are a universal model of computation. They form the foundation of many leading quantum software packages, including Qiskit , OpenQASM , Cirq , Amazon Braket SDK , and PennyLane ; which you can directly map to the librarys' Circuit class and then to HDHs (see examples of how to use these converters bellow). A quantum circuit is composed of a sequence of quantum gates applied to a set of qubits (commonly represented as horizontal wires). Gates, visualized as boxes placed on these wires, may act on one or multiple qubits. Single-qubit gates correspond to rotations of the qubit\u2019s state vector on the Bloch sphere. For example, a Z-rotation by angle \u03c0 rotates the state vector by \u03c0 radians about the z-axis. Multi-qubit gates, such as controlled-X (CX), act conditionally on one qubit\u2019s state and thereby create dependencies among qubits. For instance, a CX gate applies an X gate to the target qubit only if the control qubit is in the \u22231\u27e9 state. Such gates generate entanglement, as discussed in the introduction to DQC . Beyond static gates, circuits also support classically conditioned operations. For example, an IfElse construct applies one subcircuit if a specified classical register holds a given value and another subcircuit otherwise. This enables hybrid quantum\u2013classical flow control within circuits. Finally, measurement operations project qubits into classical bits, irreversibly collapsing their quantum state. The goal of HDHs is to make explicit the transformations induced by all gates and measurements, enabling large circuits to be partitioned into smaller, distributable subcircuits. Mapping a quantum circuit into an HDH involves applying the correspondences summarized in the table below: Bellow is an example of how to build a circuit using the library\u2019s Circuit class and map it to an HDH: import hdh from hdh.models.circuit import Circuit from hdh.visualize import plot_hdh circuit = Circuit() # Set of instructions circuit.add_instruction(\"ccx\", [0, 1, 2]) circuit.add_instruction(\"h\", [3]) circuit.add_instruction(\"h\", [5]) circuit.add_instruction(\"cx\", [3, 4]) circuit.add_instruction(\"cx\", [2, 1]) circuit.add_conditional_gate(5, 4, \"z\") circuit.add_instruction(\"cx\", [0, 3]) circuit.add_instruction(\"measure\", [2]) circuit.add_instruction(\"measure\", [4]) hdh = circuit.build_hdh() # Generate HDH fig = plot_hdh(hdh) # Visualize HDH This code is equivalent to the following circuit: Which is mapped to HDH: The HDH is composed of the motifs shown in the mapping table, appearing in the temporal order of computation and indexed by the qubits they abstract. Note that a qubit is not initialized until the timestep immediately preceding its first operation. MBQC patterns Measurement based quantum computing is one of the alternative universal models to quantum circuits [ 3 ]. It can be implemented on photonic quantum computers, as is often paired alongside fusion gates [ 4 ]. MBQC patterns consist of 4 types of operations [ 5 ]: N: Auxiliary state preparations E: Entaglement operations M: Measurements C: Corrections Intuitively, MBQC patterns can be thought of as large cluster states built from entangled nodes. The computation proceeds by measuring these nodes in sequence, where later measurement bases are adapted according to the outcomes of earlier ones. Measurement based patterns may look like this (taken from 6 ): . Here the N operations form the nodes, and E operations make the Edges. Time flows from left to right; and the angles (0, \u03c0/8) correspond to correction angles. Equivalences between circuit gates and MBQC patterns can be drawn (in this case squares = output qubits) [ 6 ]: But they are not necessarily the most efficient use of the model. Many MBQC native algorithms have been proposed [ 7 , 8 ], and the model is very compatible with various error correction techniques. If you'd like a more in depth tutorial on the model we recommend this pennylane tutorial on MBQC . Distribution of MBQC patterns can be confused with the partitioning of the substrate cluster state over which they are computed (the network you see). This is a valid method for partitioning the computation, but it is not the one that HDHs offer. As in the circuit model, HDHs abstract the operations performed during the computation (atop the cluster or during cluster generation) in a way that reflects not only their entanglement dependencies but also their temporal dependencies (aka the order in which they are performed). Mapping MBQC patterns to HDHs is relatively simple: import hdh from hdh.models.mbqc import MBQC from hdh.visualize import plot_hdh mbqc = MBQC() # Set of instructions mbqc.add_operation(\"N\", [], \"q0\") mbqc.add_operation(\"N\", [], \"q1\") mbqc.add_operation(\"E\", [\"q0\", \"q1\"], \"q1\") mbqc.add_operation(\"M\", [\"q0\"], \"c0\") mbqc.add_operation(\"C\", [\"c0\"], \"q2\") hdh = mbqc.build_hdh() # Generate HDH fig = plot_hdh(hdh) # Visualize HDH This code generates HDH: Unlike in the circuit model, MBQC pattern operations are not \"automatically\" assigned to the q_ and c_ naming conventions. This is because a node in an MBQC pattern doesn't necessarily directly correspond to a qubit. For simplicity we have made it so in the example above, but feel free to name them something else. Note that if you do the visualize function may play some tricks on you depending on the version of the library you are working with. This is the corresponding motif to operation mapping table for the MBQC model: Quantum walks Quantum walks are another universal model of quantum computation [ 9 ]. Like quantum circuits, quantum walks can be expressed using different sets of unitary operations and are universal for quantum computation. However, unlike circuits, quantum walks are not inherently defined in terms of gate sequences, they are abstract evolution models that can be implemented via gates or analog dynamics depending on the platform. They come in various forms, typically categorized as discrete-time or continuous-time variants [ 10 ]. In discrete-time walks (DTQW), evolution proceeds in steps using a coin operator and a shift operator; while continuous-time walks (CTQW) evolve directly under a Hamiltonian. In both cases, the key idea is that the walker\u2019s amplitude spreads across positions in superposition, creating interference patterns that can be harnessed for computation. This interference, guided by the chosen coin or Hamiltonian, allows quantum walks to implement algorithms and decision processes that outperform their classical counterparts. Unlike classical random walks, where probabilities govern independent step outcomes, quantum walks evolve coherently under unitary dynamics, so amplitudes rather than probabilities interfere. Perhaps a useful visualisation of this is to view quantum walks as waves spreading across all paths simultaneously, as per the image bellow (taken from [ 11 ]), where the two peaks of the amplitude distribution at the final row can be seen at the edges, in contrast to the central Gaussian-like spread (red) of the classical walk: The reason why the distributions are different is that in a classical walk, each step is an independent random choice. Adding them up gives a binomial distribution, which converges to a Gaussian around the center. In a quantum walk, the walker is in a superposition of paths. At each step, amplitudes for moving left or right interfere. Constructive interference pushes weight toward the outer edges, while destructive interference cancels amplitudes near the center. As a result, the probability distribution spreads ballistically (linearly with the number of steps) rather than diffusively (square root of steps), and peaks form at the farthest positions the walker can reach. You can map QW to HDHs as per: import hdh from hdh.models.qw import QW from hdh.visualize import plot_hdh qw = QW(type=discrete) # Set of instructions q0 = \"q0\" q1 = qw.add_coin(q0) q2 = qw.add_shift(q1) qw.add_measurement(q2, \"c0\") hdh = qw.build_hdh() # Generate HDH fig = plot_hdh(hdh) # Visualize HDH This code generates HDH: This is the corresponding motif to operation mapping table for the QW model: Bellow you can find a short explanation explaining the differences between DTQW and CTQW implementations. If you're interested in QW, we recommend this book . Discrete Time Quantum Walks A DTQW is made up of a set of timed operations imposed by its coin and walker. If we where to equate this to the NEMC format for MBQC, DTQW operations would be of two types: Coin: local unitaries acting on internal degrees of freedom at each position, e.g., internal \"spin\" or coin states, Shift: conditional displacements of walker states in position space based on the coin state. The outputs of each coin toss serve as inputs to the subsequent shift. A DTQW is \"valid\" if all operations can be sequentially ordered. Their mapping to HDHs is straightforward: inputs and outputs align naturally with state nodes, while coin and shift operations correspond to channel hyperedges. In practice, each coin operation introduces a local unitary hyperedge attached to the state node representing the walker\u2019s internal degree of freedom at a given time. Each shift operation then acts as a conditional hyperedge spanning the relevant position nodes, effectively redirecting walker states according to the coin outcome. Because HDHs are time-respecting, the sequential interplay of coin and shift steps is preserved explicitly, making the walk\u2019s evolution appear as an alternating pattern of local and conditional hyperedges across timesteps. Continuous Time Quantum Walks In contrast, CTQWs describe physical evolution over time under a Hamiltonian and do not, by themselves, constitute a universal computational model. They lack a well-defined, manipulable set of time-indexed operations. Nonetheless, CTQWs can still be mapped to HDH constructions by interpreting their continuous evolution as a resource-driven process. HDHs accommodate this by using dedicated predictive constructs to represent these operations. In other words they discretize the CTQW, and map it using the DTQW mapping, whilst setting each operation to the predicted type. This logic can be extended to other prominent quantum frameworks, such as adiabatic quantum computing, which likewise lack native representations of sequential logical operations but can still be encoded into HDHs by discretizing their dynamics and treating them as structured evolution patterns. Quantum cellular automata Finally, the last model currently compatible with the HDH library is QCA (Quantum cellular automaton). QCAs originally arose as an alternative paradigm for quantum computation, though more recently they have found application in understanding topological phases of matter and have been proposed as models of periodically driven (Floquet) quantum systems [ 12 ]. A QCA consists of a lattice of cells, each holding a finite-dimensional quantum state, whose evolution is defined by repeated application of local, translation-invariant rules. They are basically the equivalent of Cellular Automatons, which are beautifully described in Wolfram MathWorld as: a collection of \"colored\" cells on a grid of specified shape that evolves through a number of discrete time steps according to a set of rules based on the states of neighboring cells. In QCAs evolution ensure that information propagates only within a bounded neighborhood per time step, enforcing causality and making QCAs suitable for modeling distributed quantum dynamics. Similarly to quantum walks, their dynamics can be reduced to two core operations: Unitary updates: local reversible transformations that map a neighborhood $A$ to output cells $B$, preserving causality and unitarity, Swap or shift layers: structural steps to rearrange or relabel cell contents for staggered updates. You can think of QCA operation evolution as a slow conquer of a substrate lattice: They can be mapped to HDHs through: import hdh from hdh.models.qca import QCA from hdh.visualize import plot_hdh # Topology of lattice over which QCA will evolve topology = { \"q0\": [\"q1\", \"q2\"], \"q1\": [\"q0\"], \"q2\": [\"q0\"] } measurements = {\"q1\", \"q2\"} qca = QCA(topology=topology, measurements=measurements, steps=3) hdh = qca.build_hdh() # Generate HDH fig = plot_hdh(hdh) # Visualize HDH This code generates HDH: This is the corresponding motif to operation mapping table for the QCA model: Built in Converters The library currently has converters from Qiskit , OpenQASM , Cirq , Amazon Braket SDK , and PennyLane . These converters take in computations written in these languages/SDKs and transform them into relevant HDHs. They can be used as follows: import hdh import qiskit from qiskit import QuantumCircuit from hdh.converters import from_qiskit from hdh.visualize import plot_hdh from hdh.passes.cut import compute_cut, cost, partition_sizes, compute_parallelism_by_time # Qiskit circuit qc = QuantumCircuit(3) qc.h(0) qc.cx(0, 1) qc.ccx(1, 2, 0) qc.measure_all() hdh_graph = from_qiskit(qc) # Generate HDH fig = plot_hdh(hdh) # Visualize HDH Make your own instruction set Beyond the given models and converters, you can also make your own model class by defining the desired HDH motif construction. A good rule of thumb is to consider inputs and outputs of operations as nodes and the operations themselves as hyperedges. This is how the Circuit class looks like: from typing import List, Tuple, Optional, Set, Dict, Literal import sys import os sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))) from hdh.hdh import HDH class Circuit: def __init__(self): self.instructions: List[ Tuple[str, List[int], List[int], List[bool], Literal[\"a\", \"p\"]] ] = [] # (name, qubits, bits, modifies_flags, cond_flag) def add_instruction( self, name: str, qubits: List[int], bits: Optional[List[int]] = None, modifies_flags: Optional[List[bool]] = None, cond_flag: Literal[\"a\", \"p\"] = \"a\" ): name = name.lower() if name == \"measure\": modifies_flags = [True] * len(qubits) else: bits = bits or [] modifies_flags = modifies_flags or [True] * len(qubits) self.instructions.append((name, qubits, bits, modifies_flags, cond_flag)) def build_hdh(self, hdh_cls=HDH) -> HDH: hdh = hdh_cls() qubit_time: Dict[int, int] = {} bit_time: Dict[int, int] = {} last_gate_input_time: Dict[int, int] = {} for name, qargs, cargs, modifies_flags, cond_flag in self.instructions: # --- Canonicalize inputs --- qargs = list(qargs or []) if name == \"measure\": cargs = list(cargs) if cargs is not None else qargs.copy() # 1:1 map if len(cargs) != len(qargs): raise ValueError(\"measure: len(bits) must equal len(qubits)\") modifies_flags = [True] * len(qargs) else: cargs = list(cargs or []) if modifies_flags is None: modifies_flags = [True] * len(qargs) elif len(modifies_flags) != len(qargs): raise ValueError(\"len(modifies_flags) must equal len(qubits)\") #print(f\"\\n=== Adding instruction: {name} on qubits {qargs} ===\") # for q in qargs: #print(f\" [before] qubit_time[{q}] = {qubit_time.get(q)}\") # Measurements if name == \"measure\": for i, qubit in enumerate(qargs): # Use current qubit time (default 0), do NOT advance it here t_in = qubit_time.get(qubit, 0) q_in = f\"q{qubit}_t{t_in}\" hdh.add_node(q_in, \"q\", t_in, node_real=cond_flag) bit = cargs[i] t_out = t_in + 1 # classical result at next tick c_out = f\"c{bit}_t{t_out}\" hdh.add_node(c_out, \"c\", t_out, node_real=cond_flag) hdh.add_hyperedge({q_in, c_out}, \"c\", name=\"measure\", node_real=cond_flag) # Next-free convention for this bit stream bit_time[bit] = t_out + 1 # Important: do NOT set qubit_time[qubit] = t_in + k # The quantum wire collapses; keep its last quantum tick unchanged. continue # Conditional gate handling if name != \"measure\" and cond_flag == \"p\" and cargs: # Supports 1 classical control; extend to many if you like ctrl = cargs[0] # Ensure times exist for q in qargs: if q not in qubit_time: qubit_time[q] = max(qubit_time.values(), default=0) last_gate_input_time[q] = qubit_time[q] # Classical node must already exist (e.g., produced by a prior measure) # bit_time points to \"next free\" slot; the latest existing node is at t = bit_time-1 c_latest = bit_time.get(ctrl, 1) - 1 cnode = f\"c{ctrl}_t{c_latest}\" hdh.add_node(cnode, \"c\", c_latest, node_real=cond_flag) edges = [] for tq in qargs: # gate happens at next tick after both inputs are ready t_in_q = qubit_time[tq] t_gate = max(t_in_q, c_latest) + 1 qname = f\"q{tq}\" qout = f\"{qname}_t{t_gate}\" # ensure the quantum output node exists at gate time hdh.add_node(qout, \"q\", t_gate, node_real=cond_flag) # add classical hyperedge feeding the quantum node e = hdh.add_hyperedge({cnode, qout}, \"c\", name=name, node_real=cond_flag) edges.append(e) # advance quantum time last_gate_input_time[tq] = t_in_q qubit_time[tq] = t_gate # store edge_args for reconstruction/debug q_with_time = [(q, qubit_time[q]) for q in qargs] c_with_time = [(ctrl, c_latest + 1)] # next-free convention; adjust if you track exact for e in edges: hdh.edge_args[e] = (q_with_time, c_with_time, modifies_flags or [True] * len(qargs)) continue #Actualized gate (non-conditional) for q in qargs: if q not in qubit_time: qubit_time[q] = max(qubit_time.values(), default=0) last_gate_input_time[q] = qubit_time[q] # initial input time active_times = [qubit_time[q] for q in qargs] time_step = max(active_times) + 1 if active_times else 0 in_nodes: List[str] = [] out_nodes: List[str] = [] intermediate_nodes: List[str] = [] final_nodes: List[str] = [] post_nodes: List[str] = [] #DEBUG #print(\" [after]\", {q: qubit_time[q] for q in qargs}) #print(\" [after]\", {q: qubit_time[q] for q in qargs}) multi_gate = (name != \"measure\" and len(qargs) > 1) common_start = max((qubit_time.get(q, 0) for q in qargs), default=0) if multi_gate else None for i, qubit in enumerate(qargs): t_in = qubit_time[qubit] qname = f\"q{qubit}\" in_id = f\"{qname}_t{t_in}\" hdh.add_node(in_id, \"q\", t_in, node_real=cond_flag) #print(f\" [+] Node added: {in_id} (type q, time {t_in})\") #print(f\" [+] Node added: {in_id} (type q, time {t_in})\") in_nodes.append(in_id) #print(f\" [qubit {qubit}] modifies_flag = {modifies_flags[i]}\") #print(f\" [qubit {qubit}] modifies_flag = {modifies_flags[i]}\") # choose timeline if multi_gate: t1 = common_start + 1 t2 = common_start + 2 t3 = common_start + 3 else: t1 = t_in + 1 t2 = t1 + 1 t3 = t2 + 1 # create mid/final/post nodes for BOTH cases mid_id = f\"{qname}_t{t1}\" final_id = f\"{qname}_t{t2}\" post_id = f\"{qname}_t{t3}\" hdh.add_node(mid_id, \"q\", t1, node_real=cond_flag) hdh.add_node(final_id, \"q\", t2, node_real=cond_flag) hdh.add_node(post_id, \"q\", t3, node_real=cond_flag) intermediate_nodes.append(mid_id) final_nodes.append(final_id) post_nodes.append(post_id) last_gate_input_time[qubit] = t_in qubit_time[qubit] = t3 edges = [] if len(qargs) > 1: # Stage 1: input \u2192 intermediate (1:1) for in_node, mid_node in zip(in_nodes, intermediate_nodes): e = hdh.add_hyperedge({in_node, mid_node}, \"q\", name=f\"{name}_stage1\", node_real=cond_flag) #print(f\" [~] stage1 {in_node} \u2192 {mid_node}\") #print(f\" [~] stage1 {in_node} \u2192 {mid_node}\") edges.append(e) # Stage 2: full multiqubit edge from intermediate \u2192 final e2 = hdh.add_hyperedge(set(intermediate_nodes) | set(final_nodes), \"q\", name=f\"{name}_stage2\", node_real=cond_flag) #print(f\" [~] stage2 |intermediate|={len(intermediate_nodes)} \u2192 |final|={len(final_nodes)}\") #print(f\" [~] stage2 |intermediate|={len(intermediate_nodes)} \u2192 |final|={len(final_nodes)}\") edges.append(e2) # Stage 3: final \u2192 post (1:1) for f_node, p_node in zip(final_nodes, post_nodes): e = hdh.add_hyperedge({f_node, p_node}, \"q\", name=f\"{name}_stage3\", node_real=cond_flag) #print(f\" [~] stage3 {f_node} \u2192 {p_node}\") #print(f\" [~] stage3 {f_node} \u2192 {p_node}\") edges.append(e) if name == \"measure\": for i, qubit in enumerate(qargs): t_in = qubit_time.get(qubit, 0) q_in = f\"q{qubit}_t{t_in}\" hdh.add_node(q_in, \"q\", t_in, node_real=cond_flag) bit = cargs[i] t_out = t_in + 1 c_out = f\"c{bit}_t{t_out}\" hdh.add_node(c_out, \"c\", t_out, node_real=cond_flag) hdh.add_hyperedge({q_in, c_out}, \"c\", name=\"measure\", node_real=cond_flag) bit_time[bit] = t_out + 1 continue if name != \"measure\": for bit in cargs: t = bit_time.get(bit, 0) cname = f\"c{bit}\" out_id = f\"{cname}_t{t + 1}\" hdh.add_node(out_id, \"c\", t + 1, node_real=cond_flag) out_nodes.append(out_id) bit_time[bit] = t + 1 all_nodes = set(in_nodes) | set(out_nodes) if all(n.startswith(\"c\") for n in all_nodes): edge_type = \"c\" elif any(n.startswith(\"c\") for n in all_nodes): edge_type = \"c\" else: edge_type = \"q\" edges = [] if len(qargs) > 1: # Multi-qubit gate # Stage 1: input \u2192 intermediate (1:1) for in_node, mid_node in zip(in_nodes, intermediate_nodes): edge = hdh.add_hyperedge({in_node, mid_node}, \"q\", name=f\"{name}_stage1\", node_real=cond_flag) # DEBUG #print(f\" [~] Hyperedge added over: {in_node} \u2192 {mid_node}, label: {name}_stage1\") #print(f\" [~] Hyperedge added over: {in_node} \u2192 {mid_node}, label: {name}_stage1\") edges.append(edge) # Stage 2: full multiqubit edge from intermediate \u2192 final edge2 = hdh.add_hyperedge(set(intermediate_nodes) | set(final_nodes), \"q\", name=f\"{name}_stage2\", node_real=cond_flag) # DEBUG #print(f\" [~] Hyperedge added over: {in_node} \u2192 {mid_node}, label: {name}_stage2\") #print(f\" [~] Hyperedge added over: {in_node} \u2192 {mid_node}, label: {name}_stage2\") edges.append(edge2) # Stage 3: final \u2192 post (1:1 again) for final_node, post_node in zip(final_nodes, post_nodes): edge = hdh.add_hyperedge({final_node, post_node}, \"q\", name=f\"{name}_stage3\", node_real=cond_flag) # DEBUG #print(f\" [~] Hyperedge added over: {in_node} \u2192 {mid_node}, label: {name}_stage1\") #print(f\" [~] Hyperedge added over: {in_node} \u2192 {mid_node}, label: {name}_stage1\") edges.append(edge) else: # Single-qubit gate for i, qubit in enumerate(qargs): if modifies_flags[i] and name != \"measure\": t_in = last_gate_input_time[qubit] t_out = t_in + 1 qname = f\"q{qubit}\" in_id = f\"{qname}_t{t_in}\" out_id = f\"{qname}_t{t_out}\" # DEBUG #print(f\"[{name}] Q{qubit} t_in = {t_in}, expected from qubit_time = {qubit_time[qubit]}\") #print(f\"[{name}] Q{qubit} t_in = {t_in}, expected from qubit_time = {qubit_time[qubit]}\") hdh.add_node(out_id, \"q\", t_out, node_real=cond_flag) # DEBUG #print(f\" [+] Node added: {in_id} (type q, time {t_in})\") #print(f\" [+] Node added: {in_id} (type q, time {t_in})\") edge = hdh.add_hyperedge({in_id, out_id}, \"q\", name=name, node_real=cond_flag) # DEBUG #print(f\" [~] Hyperedge added over: {in_id} \u2192 {out_id}, label: {name}_stage1\") #print(f\" [~] Hyperedge added over: {in_id} \u2192 {out_id}, label: {name}_stage1\") edges.append(edge) # Update time qubit_time[qubit] = t_out last_gate_input_time[qubit] = t_in q_with_time = [(q, qubit_time[q]) for q in qargs] c_with_time = [(c, bit_time.get(c, 0)) for c in cargs] for edge in edges: hdh.edge_args[edge] = (q_with_time, c_with_time, modifies_flags) return hdh","title":"Quantum Computational Model mappings to HDHs"},{"location":"models/#computational-models","text":"Quantum computation can be performed with various computational models that have different instruction sets, such as quantum circuits (which perform computation through gates) or measurement based patterns (which perform computation through corrections). The diversity between these models can be attributed to hardware constraints as well as to the age of the field. Although they are all equivalent and computations can theoretically be translated accross them, this process is inneficient and hard to due to a lack of clear set of cross-model translators. HDHs were designed to abstract all the models into a unified framework. As such, they are model agnostic and can be constructed from any set of instructions. Specific model classes can be found under the hdh/models folder. To facilitate the usage of HDHs, the library has a set of embedded model mappings which translate instruction sets from popular quantum computational models into reusable HDH motifs. You can find details and examples of how to use these model classes and SDK converters bellow.","title":"Computational models"},{"location":"models/#circuits","text":"The circuit model is among the most widely adopted paradigms of quantum computation, particularly in implementations on industrial quantum hardware (with the notable exception of photonic qubits). Quantum circuits are a universal model of computation. They form the foundation of many leading quantum software packages, including Qiskit , OpenQASM , Cirq , Amazon Braket SDK , and PennyLane ; which you can directly map to the librarys' Circuit class and then to HDHs (see examples of how to use these converters bellow). A quantum circuit is composed of a sequence of quantum gates applied to a set of qubits (commonly represented as horizontal wires). Gates, visualized as boxes placed on these wires, may act on one or multiple qubits. Single-qubit gates correspond to rotations of the qubit\u2019s state vector on the Bloch sphere. For example, a Z-rotation by angle \u03c0 rotates the state vector by \u03c0 radians about the z-axis. Multi-qubit gates, such as controlled-X (CX), act conditionally on one qubit\u2019s state and thereby create dependencies among qubits. For instance, a CX gate applies an X gate to the target qubit only if the control qubit is in the \u22231\u27e9 state. Such gates generate entanglement, as discussed in the introduction to DQC . Beyond static gates, circuits also support classically conditioned operations. For example, an IfElse construct applies one subcircuit if a specified classical register holds a given value and another subcircuit otherwise. This enables hybrid quantum\u2013classical flow control within circuits. Finally, measurement operations project qubits into classical bits, irreversibly collapsing their quantum state. The goal of HDHs is to make explicit the transformations induced by all gates and measurements, enabling large circuits to be partitioned into smaller, distributable subcircuits. Mapping a quantum circuit into an HDH involves applying the correspondences summarized in the table below: Bellow is an example of how to build a circuit using the library\u2019s Circuit class and map it to an HDH: import hdh from hdh.models.circuit import Circuit from hdh.visualize import plot_hdh circuit = Circuit() # Set of instructions circuit.add_instruction(\"ccx\", [0, 1, 2]) circuit.add_instruction(\"h\", [3]) circuit.add_instruction(\"h\", [5]) circuit.add_instruction(\"cx\", [3, 4]) circuit.add_instruction(\"cx\", [2, 1]) circuit.add_conditional_gate(5, 4, \"z\") circuit.add_instruction(\"cx\", [0, 3]) circuit.add_instruction(\"measure\", [2]) circuit.add_instruction(\"measure\", [4]) hdh = circuit.build_hdh() # Generate HDH fig = plot_hdh(hdh) # Visualize HDH This code is equivalent to the following circuit: Which is mapped to HDH: The HDH is composed of the motifs shown in the mapping table, appearing in the temporal order of computation and indexed by the qubits they abstract. Note that a qubit is not initialized until the timestep immediately preceding its first operation.","title":"Circuits"},{"location":"models/#mbqc-patterns","text":"Measurement based quantum computing is one of the alternative universal models to quantum circuits [ 3 ]. It can be implemented on photonic quantum computers, as is often paired alongside fusion gates [ 4 ]. MBQC patterns consist of 4 types of operations [ 5 ]: N: Auxiliary state preparations E: Entaglement operations M: Measurements C: Corrections Intuitively, MBQC patterns can be thought of as large cluster states built from entangled nodes. The computation proceeds by measuring these nodes in sequence, where later measurement bases are adapted according to the outcomes of earlier ones. Measurement based patterns may look like this (taken from 6 ): . Here the N operations form the nodes, and E operations make the Edges. Time flows from left to right; and the angles (0, \u03c0/8) correspond to correction angles. Equivalences between circuit gates and MBQC patterns can be drawn (in this case squares = output qubits) [ 6 ]: But they are not necessarily the most efficient use of the model. Many MBQC native algorithms have been proposed [ 7 , 8 ], and the model is very compatible with various error correction techniques. If you'd like a more in depth tutorial on the model we recommend this pennylane tutorial on MBQC . Distribution of MBQC patterns can be confused with the partitioning of the substrate cluster state over which they are computed (the network you see). This is a valid method for partitioning the computation, but it is not the one that HDHs offer. As in the circuit model, HDHs abstract the operations performed during the computation (atop the cluster or during cluster generation) in a way that reflects not only their entanglement dependencies but also their temporal dependencies (aka the order in which they are performed). Mapping MBQC patterns to HDHs is relatively simple: import hdh from hdh.models.mbqc import MBQC from hdh.visualize import plot_hdh mbqc = MBQC() # Set of instructions mbqc.add_operation(\"N\", [], \"q0\") mbqc.add_operation(\"N\", [], \"q1\") mbqc.add_operation(\"E\", [\"q0\", \"q1\"], \"q1\") mbqc.add_operation(\"M\", [\"q0\"], \"c0\") mbqc.add_operation(\"C\", [\"c0\"], \"q2\") hdh = mbqc.build_hdh() # Generate HDH fig = plot_hdh(hdh) # Visualize HDH This code generates HDH: Unlike in the circuit model, MBQC pattern operations are not \"automatically\" assigned to the q_ and c_ naming conventions. This is because a node in an MBQC pattern doesn't necessarily directly correspond to a qubit. For simplicity we have made it so in the example above, but feel free to name them something else. Note that if you do the visualize function may play some tricks on you depending on the version of the library you are working with. This is the corresponding motif to operation mapping table for the MBQC model:","title":"MBQC patterns"},{"location":"models/#quantum-walks","text":"Quantum walks are another universal model of quantum computation [ 9 ]. Like quantum circuits, quantum walks can be expressed using different sets of unitary operations and are universal for quantum computation. However, unlike circuits, quantum walks are not inherently defined in terms of gate sequences, they are abstract evolution models that can be implemented via gates or analog dynamics depending on the platform. They come in various forms, typically categorized as discrete-time or continuous-time variants [ 10 ]. In discrete-time walks (DTQW), evolution proceeds in steps using a coin operator and a shift operator; while continuous-time walks (CTQW) evolve directly under a Hamiltonian. In both cases, the key idea is that the walker\u2019s amplitude spreads across positions in superposition, creating interference patterns that can be harnessed for computation. This interference, guided by the chosen coin or Hamiltonian, allows quantum walks to implement algorithms and decision processes that outperform their classical counterparts. Unlike classical random walks, where probabilities govern independent step outcomes, quantum walks evolve coherently under unitary dynamics, so amplitudes rather than probabilities interfere. Perhaps a useful visualisation of this is to view quantum walks as waves spreading across all paths simultaneously, as per the image bellow (taken from [ 11 ]), where the two peaks of the amplitude distribution at the final row can be seen at the edges, in contrast to the central Gaussian-like spread (red) of the classical walk: The reason why the distributions are different is that in a classical walk, each step is an independent random choice. Adding them up gives a binomial distribution, which converges to a Gaussian around the center. In a quantum walk, the walker is in a superposition of paths. At each step, amplitudes for moving left or right interfere. Constructive interference pushes weight toward the outer edges, while destructive interference cancels amplitudes near the center. As a result, the probability distribution spreads ballistically (linearly with the number of steps) rather than diffusively (square root of steps), and peaks form at the farthest positions the walker can reach. You can map QW to HDHs as per: import hdh from hdh.models.qw import QW from hdh.visualize import plot_hdh qw = QW(type=discrete) # Set of instructions q0 = \"q0\" q1 = qw.add_coin(q0) q2 = qw.add_shift(q1) qw.add_measurement(q2, \"c0\") hdh = qw.build_hdh() # Generate HDH fig = plot_hdh(hdh) # Visualize HDH This code generates HDH: This is the corresponding motif to operation mapping table for the QW model: Bellow you can find a short explanation explaining the differences between DTQW and CTQW implementations. If you're interested in QW, we recommend this book .","title":"Quantum walks"},{"location":"models/#discrete-time-quantum-walks","text":"A DTQW is made up of a set of timed operations imposed by its coin and walker. If we where to equate this to the NEMC format for MBQC, DTQW operations would be of two types: Coin: local unitaries acting on internal degrees of freedom at each position, e.g., internal \"spin\" or coin states, Shift: conditional displacements of walker states in position space based on the coin state. The outputs of each coin toss serve as inputs to the subsequent shift. A DTQW is \"valid\" if all operations can be sequentially ordered. Their mapping to HDHs is straightforward: inputs and outputs align naturally with state nodes, while coin and shift operations correspond to channel hyperedges. In practice, each coin operation introduces a local unitary hyperedge attached to the state node representing the walker\u2019s internal degree of freedom at a given time. Each shift operation then acts as a conditional hyperedge spanning the relevant position nodes, effectively redirecting walker states according to the coin outcome. Because HDHs are time-respecting, the sequential interplay of coin and shift steps is preserved explicitly, making the walk\u2019s evolution appear as an alternating pattern of local and conditional hyperedges across timesteps.","title":"Discrete Time Quantum Walks"},{"location":"models/#continuous-time-quantum-walks","text":"In contrast, CTQWs describe physical evolution over time under a Hamiltonian and do not, by themselves, constitute a universal computational model. They lack a well-defined, manipulable set of time-indexed operations. Nonetheless, CTQWs can still be mapped to HDH constructions by interpreting their continuous evolution as a resource-driven process. HDHs accommodate this by using dedicated predictive constructs to represent these operations. In other words they discretize the CTQW, and map it using the DTQW mapping, whilst setting each operation to the predicted type. This logic can be extended to other prominent quantum frameworks, such as adiabatic quantum computing, which likewise lack native representations of sequential logical operations but can still be encoded into HDHs by discretizing their dynamics and treating them as structured evolution patterns.","title":"Continuous Time Quantum Walks"},{"location":"models/#quantum-cellular-automata","text":"Finally, the last model currently compatible with the HDH library is QCA (Quantum cellular automaton). QCAs originally arose as an alternative paradigm for quantum computation, though more recently they have found application in understanding topological phases of matter and have been proposed as models of periodically driven (Floquet) quantum systems [ 12 ]. A QCA consists of a lattice of cells, each holding a finite-dimensional quantum state, whose evolution is defined by repeated application of local, translation-invariant rules. They are basically the equivalent of Cellular Automatons, which are beautifully described in Wolfram MathWorld as: a collection of \"colored\" cells on a grid of specified shape that evolves through a number of discrete time steps according to a set of rules based on the states of neighboring cells. In QCAs evolution ensure that information propagates only within a bounded neighborhood per time step, enforcing causality and making QCAs suitable for modeling distributed quantum dynamics. Similarly to quantum walks, their dynamics can be reduced to two core operations: Unitary updates: local reversible transformations that map a neighborhood $A$ to output cells $B$, preserving causality and unitarity, Swap or shift layers: structural steps to rearrange or relabel cell contents for staggered updates. You can think of QCA operation evolution as a slow conquer of a substrate lattice: They can be mapped to HDHs through: import hdh from hdh.models.qca import QCA from hdh.visualize import plot_hdh # Topology of lattice over which QCA will evolve topology = { \"q0\": [\"q1\", \"q2\"], \"q1\": [\"q0\"], \"q2\": [\"q0\"] } measurements = {\"q1\", \"q2\"} qca = QCA(topology=topology, measurements=measurements, steps=3) hdh = qca.build_hdh() # Generate HDH fig = plot_hdh(hdh) # Visualize HDH This code generates HDH: This is the corresponding motif to operation mapping table for the QCA model:","title":"Quantum cellular automata"},{"location":"models/#built-in-converters","text":"The library currently has converters from Qiskit , OpenQASM , Cirq , Amazon Braket SDK , and PennyLane . These converters take in computations written in these languages/SDKs and transform them into relevant HDHs. They can be used as follows: import hdh import qiskit from qiskit import QuantumCircuit from hdh.converters import from_qiskit from hdh.visualize import plot_hdh from hdh.passes.cut import compute_cut, cost, partition_sizes, compute_parallelism_by_time # Qiskit circuit qc = QuantumCircuit(3) qc.h(0) qc.cx(0, 1) qc.ccx(1, 2, 0) qc.measure_all() hdh_graph = from_qiskit(qc) # Generate HDH fig = plot_hdh(hdh) # Visualize HDH","title":"Built in Converters"},{"location":"models/#make-your-own-instruction-set","text":"Beyond the given models and converters, you can also make your own model class by defining the desired HDH motif construction. A good rule of thumb is to consider inputs and outputs of operations as nodes and the operations themselves as hyperedges. This is how the Circuit class looks like: from typing import List, Tuple, Optional, Set, Dict, Literal import sys import os sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))) from hdh.hdh import HDH class Circuit: def __init__(self): self.instructions: List[ Tuple[str, List[int], List[int], List[bool], Literal[\"a\", \"p\"]] ] = [] # (name, qubits, bits, modifies_flags, cond_flag) def add_instruction( self, name: str, qubits: List[int], bits: Optional[List[int]] = None, modifies_flags: Optional[List[bool]] = None, cond_flag: Literal[\"a\", \"p\"] = \"a\" ): name = name.lower() if name == \"measure\": modifies_flags = [True] * len(qubits) else: bits = bits or [] modifies_flags = modifies_flags or [True] * len(qubits) self.instructions.append((name, qubits, bits, modifies_flags, cond_flag)) def build_hdh(self, hdh_cls=HDH) -> HDH: hdh = hdh_cls() qubit_time: Dict[int, int] = {} bit_time: Dict[int, int] = {} last_gate_input_time: Dict[int, int] = {} for name, qargs, cargs, modifies_flags, cond_flag in self.instructions: # --- Canonicalize inputs --- qargs = list(qargs or []) if name == \"measure\": cargs = list(cargs) if cargs is not None else qargs.copy() # 1:1 map if len(cargs) != len(qargs): raise ValueError(\"measure: len(bits) must equal len(qubits)\") modifies_flags = [True] * len(qargs) else: cargs = list(cargs or []) if modifies_flags is None: modifies_flags = [True] * len(qargs) elif len(modifies_flags) != len(qargs): raise ValueError(\"len(modifies_flags) must equal len(qubits)\") #print(f\"\\n=== Adding instruction: {name} on qubits {qargs} ===\") # for q in qargs: #print(f\" [before] qubit_time[{q}] = {qubit_time.get(q)}\") # Measurements if name == \"measure\": for i, qubit in enumerate(qargs): # Use current qubit time (default 0), do NOT advance it here t_in = qubit_time.get(qubit, 0) q_in = f\"q{qubit}_t{t_in}\" hdh.add_node(q_in, \"q\", t_in, node_real=cond_flag) bit = cargs[i] t_out = t_in + 1 # classical result at next tick c_out = f\"c{bit}_t{t_out}\" hdh.add_node(c_out, \"c\", t_out, node_real=cond_flag) hdh.add_hyperedge({q_in, c_out}, \"c\", name=\"measure\", node_real=cond_flag) # Next-free convention for this bit stream bit_time[bit] = t_out + 1 # Important: do NOT set qubit_time[qubit] = t_in + k # The quantum wire collapses; keep its last quantum tick unchanged. continue # Conditional gate handling if name != \"measure\" and cond_flag == \"p\" and cargs: # Supports 1 classical control; extend to many if you like ctrl = cargs[0] # Ensure times exist for q in qargs: if q not in qubit_time: qubit_time[q] = max(qubit_time.values(), default=0) last_gate_input_time[q] = qubit_time[q] # Classical node must already exist (e.g., produced by a prior measure) # bit_time points to \"next free\" slot; the latest existing node is at t = bit_time-1 c_latest = bit_time.get(ctrl, 1) - 1 cnode = f\"c{ctrl}_t{c_latest}\" hdh.add_node(cnode, \"c\", c_latest, node_real=cond_flag) edges = [] for tq in qargs: # gate happens at next tick after both inputs are ready t_in_q = qubit_time[tq] t_gate = max(t_in_q, c_latest) + 1 qname = f\"q{tq}\" qout = f\"{qname}_t{t_gate}\" # ensure the quantum output node exists at gate time hdh.add_node(qout, \"q\", t_gate, node_real=cond_flag) # add classical hyperedge feeding the quantum node e = hdh.add_hyperedge({cnode, qout}, \"c\", name=name, node_real=cond_flag) edges.append(e) # advance quantum time last_gate_input_time[tq] = t_in_q qubit_time[tq] = t_gate # store edge_args for reconstruction/debug q_with_time = [(q, qubit_time[q]) for q in qargs] c_with_time = [(ctrl, c_latest + 1)] # next-free convention; adjust if you track exact for e in edges: hdh.edge_args[e] = (q_with_time, c_with_time, modifies_flags or [True] * len(qargs)) continue #Actualized gate (non-conditional) for q in qargs: if q not in qubit_time: qubit_time[q] = max(qubit_time.values(), default=0) last_gate_input_time[q] = qubit_time[q] # initial input time active_times = [qubit_time[q] for q in qargs] time_step = max(active_times) + 1 if active_times else 0 in_nodes: List[str] = [] out_nodes: List[str] = [] intermediate_nodes: List[str] = [] final_nodes: List[str] = [] post_nodes: List[str] = [] #DEBUG #print(\" [after]\", {q: qubit_time[q] for q in qargs}) #print(\" [after]\", {q: qubit_time[q] for q in qargs}) multi_gate = (name != \"measure\" and len(qargs) > 1) common_start = max((qubit_time.get(q, 0) for q in qargs), default=0) if multi_gate else None for i, qubit in enumerate(qargs): t_in = qubit_time[qubit] qname = f\"q{qubit}\" in_id = f\"{qname}_t{t_in}\" hdh.add_node(in_id, \"q\", t_in, node_real=cond_flag) #print(f\" [+] Node added: {in_id} (type q, time {t_in})\") #print(f\" [+] Node added: {in_id} (type q, time {t_in})\") in_nodes.append(in_id) #print(f\" [qubit {qubit}] modifies_flag = {modifies_flags[i]}\") #print(f\" [qubit {qubit}] modifies_flag = {modifies_flags[i]}\") # choose timeline if multi_gate: t1 = common_start + 1 t2 = common_start + 2 t3 = common_start + 3 else: t1 = t_in + 1 t2 = t1 + 1 t3 = t2 + 1 # create mid/final/post nodes for BOTH cases mid_id = f\"{qname}_t{t1}\" final_id = f\"{qname}_t{t2}\" post_id = f\"{qname}_t{t3}\" hdh.add_node(mid_id, \"q\", t1, node_real=cond_flag) hdh.add_node(final_id, \"q\", t2, node_real=cond_flag) hdh.add_node(post_id, \"q\", t3, node_real=cond_flag) intermediate_nodes.append(mid_id) final_nodes.append(final_id) post_nodes.append(post_id) last_gate_input_time[qubit] = t_in qubit_time[qubit] = t3 edges = [] if len(qargs) > 1: # Stage 1: input \u2192 intermediate (1:1) for in_node, mid_node in zip(in_nodes, intermediate_nodes): e = hdh.add_hyperedge({in_node, mid_node}, \"q\", name=f\"{name}_stage1\", node_real=cond_flag) #print(f\" [~] stage1 {in_node} \u2192 {mid_node}\") #print(f\" [~] stage1 {in_node} \u2192 {mid_node}\") edges.append(e) # Stage 2: full multiqubit edge from intermediate \u2192 final e2 = hdh.add_hyperedge(set(intermediate_nodes) | set(final_nodes), \"q\", name=f\"{name}_stage2\", node_real=cond_flag) #print(f\" [~] stage2 |intermediate|={len(intermediate_nodes)} \u2192 |final|={len(final_nodes)}\") #print(f\" [~] stage2 |intermediate|={len(intermediate_nodes)} \u2192 |final|={len(final_nodes)}\") edges.append(e2) # Stage 3: final \u2192 post (1:1) for f_node, p_node in zip(final_nodes, post_nodes): e = hdh.add_hyperedge({f_node, p_node}, \"q\", name=f\"{name}_stage3\", node_real=cond_flag) #print(f\" [~] stage3 {f_node} \u2192 {p_node}\") #print(f\" [~] stage3 {f_node} \u2192 {p_node}\") edges.append(e) if name == \"measure\": for i, qubit in enumerate(qargs): t_in = qubit_time.get(qubit, 0) q_in = f\"q{qubit}_t{t_in}\" hdh.add_node(q_in, \"q\", t_in, node_real=cond_flag) bit = cargs[i] t_out = t_in + 1 c_out = f\"c{bit}_t{t_out}\" hdh.add_node(c_out, \"c\", t_out, node_real=cond_flag) hdh.add_hyperedge({q_in, c_out}, \"c\", name=\"measure\", node_real=cond_flag) bit_time[bit] = t_out + 1 continue if name != \"measure\": for bit in cargs: t = bit_time.get(bit, 0) cname = f\"c{bit}\" out_id = f\"{cname}_t{t + 1}\" hdh.add_node(out_id, \"c\", t + 1, node_real=cond_flag) out_nodes.append(out_id) bit_time[bit] = t + 1 all_nodes = set(in_nodes) | set(out_nodes) if all(n.startswith(\"c\") for n in all_nodes): edge_type = \"c\" elif any(n.startswith(\"c\") for n in all_nodes): edge_type = \"c\" else: edge_type = \"q\" edges = [] if len(qargs) > 1: # Multi-qubit gate # Stage 1: input \u2192 intermediate (1:1) for in_node, mid_node in zip(in_nodes, intermediate_nodes): edge = hdh.add_hyperedge({in_node, mid_node}, \"q\", name=f\"{name}_stage1\", node_real=cond_flag) # DEBUG #print(f\" [~] Hyperedge added over: {in_node} \u2192 {mid_node}, label: {name}_stage1\") #print(f\" [~] Hyperedge added over: {in_node} \u2192 {mid_node}, label: {name}_stage1\") edges.append(edge) # Stage 2: full multiqubit edge from intermediate \u2192 final edge2 = hdh.add_hyperedge(set(intermediate_nodes) | set(final_nodes), \"q\", name=f\"{name}_stage2\", node_real=cond_flag) # DEBUG #print(f\" [~] Hyperedge added over: {in_node} \u2192 {mid_node}, label: {name}_stage2\") #print(f\" [~] Hyperedge added over: {in_node} \u2192 {mid_node}, label: {name}_stage2\") edges.append(edge2) # Stage 3: final \u2192 post (1:1 again) for final_node, post_node in zip(final_nodes, post_nodes): edge = hdh.add_hyperedge({final_node, post_node}, \"q\", name=f\"{name}_stage3\", node_real=cond_flag) # DEBUG #print(f\" [~] Hyperedge added over: {in_node} \u2192 {mid_node}, label: {name}_stage1\") #print(f\" [~] Hyperedge added over: {in_node} \u2192 {mid_node}, label: {name}_stage1\") edges.append(edge) else: # Single-qubit gate for i, qubit in enumerate(qargs): if modifies_flags[i] and name != \"measure\": t_in = last_gate_input_time[qubit] t_out = t_in + 1 qname = f\"q{qubit}\" in_id = f\"{qname}_t{t_in}\" out_id = f\"{qname}_t{t_out}\" # DEBUG #print(f\"[{name}] Q{qubit} t_in = {t_in}, expected from qubit_time = {qubit_time[qubit]}\") #print(f\"[{name}] Q{qubit} t_in = {t_in}, expected from qubit_time = {qubit_time[qubit]}\") hdh.add_node(out_id, \"q\", t_out, node_real=cond_flag) # DEBUG #print(f\" [+] Node added: {in_id} (type q, time {t_in})\") #print(f\" [+] Node added: {in_id} (type q, time {t_in})\") edge = hdh.add_hyperedge({in_id, out_id}, \"q\", name=name, node_real=cond_flag) # DEBUG #print(f\" [~] Hyperedge added over: {in_id} \u2192 {out_id}, label: {name}_stage1\") #print(f\" [~] Hyperedge added over: {in_id} \u2192 {out_id}, label: {name}_stage1\") edges.append(edge) # Update time qubit_time[qubit] = t_out last_gate_input_time[qubit] = t_in q_with_time = [(q, qubit_time[q]) for q in qargs] c_with_time = [(c, bit_time.get(c, 0)) for c in cargs] for edge in edges: hdh.edge_args[edge] = (q_with_time, c_with_time, modifies_flags) return hdh","title":"Make your own instruction set"},{"location":"passes/","text":"HDH Partitioning Utilities Here is an overview of the partitioning utilities available in the HDH library, designed to distribute quantum computations across multiple devices. Partitioning HDHs for Distribution The hdh/passes directory contains scripts for partitioning and manipulating HDH graphs. The primary file for partitioning is cut.py , which offers two main approaches: a greedy, HDH-aware method and a METIS-based method operating on a qubit graph representation (telegate). Greedy HDH Partitioner The main partitioning function is compute_cut , which implements the Capacity-Aware Greedy HDH Partitioner (time-aware, node-level), matching the current implementation in cut.py . \ue200filecite\ue202turn4file9\ue201 Core mechanics (what changed vs the older version) Node-level assignment: individual HDH nodes like q7_t16 are assigned, not whole logical qubits. \ue200filecite\ue202turn4file4\ue201 Capacity is in unique logical qubits: each bin tracks the set of q indices present; adding a node only \u201ccosts capacity\u201d if it introduces a new logical qubit into that bin. \ue200filecite\ue202turn4file4\ue201 No automatic sibling co-location: temporal siblings of the same qubit are not forced into the same bin (teledata-style cuts are therefore possible). \ue200filecite\ue202turn4file9\ue201 Temporal validity: candidates are expanded through a time-respecting frontier (edges \u201cactivate\u201d at the max time of their pins). \ue200filecite\ue202turn4file18\ue201 High-level algorithm (3 phases) This is easiest to read as three phases, mirroring the code structure. \ue200filecite\ue202turn4file4\ue201 Phase 1: Seed selection (per bin) For each bin (QPU), pick the earliest unassigned node as the seed (ties broken deterministically by node id), place it, and initialise a frontier of temporally valid neighbours. \ue200filecite\ue202turn4file4\ue202turn4file18\ue201 Phase 2: Temporal greedy expansion (priority frontier + beam-k scoring) While the bin still has capacity left: Maintain a min-heap frontier keyed by earliest valid connection time. Pop up to beam_k earliest candidates (skipping candidates already rejected for this bin). Score those candidates by delta cut cost (lower is better, negative means it reduces cuts). Pick the best-scoring candidate; if it would exceed capacity (by introducing a new qubit when already at cap ), reject it for this bin and try again. This is \u201ctemporal greedy\u201d for reach, with \u201cbeam-k\u201d only used for selection among the earliest few frontier nodes , not a global beam search over all unassigned nodes. \ue200filecite\ue202turn4file4\ue202turn4file6\ue202turn4file18\ue201 Phase 3: Residual best-fit placement (delta-cost) Once the per-bin expansions are done, remaining nodes are placed using a best-fit rule: For the next earliest unassigned node, compute its delta cost into every bin that can accept it under capacity. Place it into the bin that minimises delta cost. If no bin can accept it under capacity, try a teledata fallback by placing it into any bin that already contains its logical qubit (so it does not consume additional capacity). If that is also impossible, mark it as unplaceable and continue. This replaces the older \u201cround-robin mop-up\u201d. \ue200filecite\ue202turn4file5\ue201 Function signature def compute_cut(hdh_graph, k: int, cap: int, *, beam_k: int = 3, backtrack_window: int = 0, polish_1swap_budget: int = 0, restarts: int = 1, reserve_frac: float = 0.08, predictive_reject: bool = True, seed: int = 0) -> Tuple[List[Set[str]], int] Note: in the current implementation, only beam_k is operational; the other keyword params are accepted for compatibility but not used. \ue200filecite\ue202turn4file9\ue201 METIS Telegate Partitioner For an alternative approach to partitioning, the library provides the metis_telegate function, which leverages the METIS algorithm (with a fallback to the Kernighan-Lin algorithm if METIS is not available). Telegate Graph Construction Graph transformation: This method first converts the HDH into a \"telegate\" graph using the telegate_hdh function. In this representation: Nodes are the qubits of the quantum circuit (labeled as q{idx} ). Edges represent quantum operations between qubits (i.e., their co-appearance in a quantum hyperedge). Edge weights correspond to the multiplicity of interactions between two qubits. Quantum operation filtering: Only hyperedges marked as quantum operations (with tau attribute = \"q\") are considered when building the telegate graph. Partitioning Process METIS partitioning: The telegate graph is partitioned using the nxmetis library, which provides Python bindings to the highly efficient METIS graph partitioning tool. If METIS is unavailable, the algorithm automatically falls back to the Kernighan-Lin bisection algorithm from NetworkX. METIS attempts to respect capacity constraints through the tpwgts (target partition weights) and ubvec (unbalance vector) parameters. Overflow repair: Since METIS does not guarantee perfectly balanced partitions, a greedy rebalancing algorithm ( _repair_overflow ) is used to adjust the partitions and ensure that no bin exceeds its qubit capacity. The repair algorithm uses a heuristic gain function ( _best_move_for_node ) to choose which qubits to move between bins. It prioritizes moving qubits that minimize the increase in cut edges. KaHyPar hypergraph partitioner cut.py also includes a KaHyPar-based partitioner, taken from the \\href{https://kahypar.org}{KaHyPar library}. kahypar_cutter (qubit-level hypergraph): Vertices are logical qubits . Each HDH hyperedge contributes a hyperedge over the qubits that appear in it. KaHyPar then runs its multilevel hypergraph partitioning pipeline (coarsening \u2192 initial partition \u2192 refinement), configured by an INI file (e.g., km1_kKaHyPar_sea20.ini ). Capacity is expressed as a balance constraint via KaHyPar\u2019s epsilon (derived from cap relative to the ideal target size n/k ). \ue200filecite\ue202turn4file3\ue201 This means the partitioner primarily \u201cknows\u201d about balancing qubit counts ; it does not model HDH-specific capacity nuances (for example, heterogeneous per-QPU capacities, or time-expanded node effects), and any \u201ccapacity\u201d notion lives inside the balance constraint. kahypar_cutter_nodebalanced (HDH-node-level hypergraph): Vertices are HDH nodes (time-expanded). Balance is therefore in node count , not in unique logical qubits. As a result, it can produce partitions that look well-balanced to KaHyPar but do not respect logical-qubit capacity (it is intentionally a balance-only baseline for comparison). \ue200filecite\ue202turn4file12\ue201 Function Signature def metis_telegate(hdh: \"HDH\", partitions: int, capacities: int) -> Tuple[List[Set[str]], int, bool, str] Parameters: * hdh : The HDH graph to partition * partitions : Number of partitions (k) * capacities : Capacity per partition (in qubits) Returns: * A tuple of (bins_qubits, cut_cost, respects_capacity, method) where: * bins_qubits is a list of sets, each containing qubit IDs (as strings like \"q0\" , \"q1\" ) in that partition * cut_cost is the number of edges crossing between partitions (unweighted) * respects_capacity is a boolean indicating whether all bins satisfy the capacity constraint * method is either \"metis\" or \"kl\" indicating which algorithm was used Cut Cost Evaluation The quality of a partition is determined by the number of \"cuts\"\u2014that is, the number of hyperedges that span across multiple bins. The library provides two functions for this purpose: _total_cost_hdh Calculates the total cost of a partition on an HDH graph. This function: * Iterates through all hyperedges in the HDH * Counts a hyperedge as \"cut\" if its pins (nodes) are distributed across 2 or more bins * Returns the sum of weights of all cut hyperedges * Respects edge weights if defined in the HDH graph (defaults to 1 if not specified) Algorithm: For each hyperedge, count the number of distinct bins containing at least one pin. If this count is \u2265 2, add the edge's weight to the total cost. _cut_edges_unweighted Counts the number of edges that cross between different bins in a standard graph (used for evaluating telegate graph partitions). This function: * Takes a NetworkX graph and a partition assignment * Counts edges where the two endpoints are in different bins * Returns an unweighted count (each cut edge counts as 1) Use case: This is specifically used by metis_telegate to evaluate the quality of qubit-graph partitions. Helper Functions and Internal Components The cut.py file contains helper utilities for the partitioners (note that some older helpers/classes remain in the file as legacy code paths). Temporal incidence + frontier utilities (greedy partitioner) _build_temporal_incidence : builds inc[node] -> [(hyperedge, edge_time)] and pins[hyperedge] -> {nodes} , where edge_time = max(pin_times) ; used to enforce temporal validity. \ue200filecite\ue202turn4file18\ue201 _push_next_valid_neighbors : expands a node into the min-heap frontier, pushing only temporally valid neighbour candidates. \ue200filecite\ue202turn4file18\ue201 _select_best_from_frontier_with_rejected : takes the earliest beam_k frontier items (skipping rejected), evaluates delta cost, and returns the best candidate. \ue200filecite\ue202turn4file6\ue201 _compute_delta_cost_simple : delta in (unweighted) cut-hyperedge count if a node is added to a specific bin. \ue200filecite\ue202turn4file18\ue201 _extract_qubit_id : parses q{idx}_t{t} to extract the logical qubit index for capacity accounting. \ue200filecite\ue202turn4file18\ue201 METIS utilities telegate_hdh : converts the HDH to a qubit interaction graph (\u201ctelegate graph\u201d). \ue200filecite\ue202turn4file16\ue201 _repair_overflow , _best_move_for_node , _over_under : post-processing used to fix capacity violations after METIS/KL. \ue200filecite\ue202turn4file10\ue201 _cut_edges_unweighted : unweighted cut-edge count for the telegate graph. \ue200filecite\ue202turn4file10\ue201 Notes on Evaluating Partitioners on Random Circuits We would like to warn users and partitioning strategy developers that we have found partitioners to behave differently on real quantum workloads when compared to randomly generated ones. As such, we recommend not testing partitioners on randomly generated workloads unless that is specifically your goal. Key considerations: * Circuit structure matters: Real quantum algorithms often have characteristic patterns (e.g., layered structures, specific qubit interaction patterns) that random circuits lack. * Connectivity patterns: Random circuits may not reflect the typical connectivity found in QAOA, VQE, quantum simulation, or other structured quantum algorithms. * Tagging in the database: In the database, you can specify the origin of your workloads, tagging them as \"random\" if appropriate. This helps others understand the context of benchmark results. When developing new partitioning strategies, we strongly encourage testing on workloads representative of your target applications rather than relying solely on random circuit benchmarks. Future Enhancements The current cut.py implementation includes several parameters that are reserved for future enhancements: Backtracking: The backtrack_window parameter is currently unused but reserved for implementing backtracking search Local search: The polish_1swap_budget parameter is disabled for HDH partitioning (as moving whole qubits is computationally heavier than single-node swaps) Predictive rejection: The predictive_reject parameter is reserved for future heuristics These features may be enabled in future versions of the library as the partitioning algorithms continue to evolve.","title":"Partitioning HDHs for distribution"},{"location":"passes/#hdh-partitioning-utilities","text":"Here is an overview of the partitioning utilities available in the HDH library, designed to distribute quantum computations across multiple devices.","title":"HDH Partitioning Utilities"},{"location":"passes/#partitioning-hdhs-for-distribution","text":"The hdh/passes directory contains scripts for partitioning and manipulating HDH graphs. The primary file for partitioning is cut.py , which offers two main approaches: a greedy, HDH-aware method and a METIS-based method operating on a qubit graph representation (telegate).","title":"Partitioning HDHs for Distribution"},{"location":"passes/#greedy-hdh-partitioner","text":"The main partitioning function is compute_cut , which implements the Capacity-Aware Greedy HDH Partitioner (time-aware, node-level), matching the current implementation in cut.py . \ue200filecite\ue202turn4file9\ue201","title":"Greedy HDH Partitioner"},{"location":"passes/#core-mechanics-what-changed-vs-the-older-version","text":"Node-level assignment: individual HDH nodes like q7_t16 are assigned, not whole logical qubits. \ue200filecite\ue202turn4file4\ue201 Capacity is in unique logical qubits: each bin tracks the set of q indices present; adding a node only \u201ccosts capacity\u201d if it introduces a new logical qubit into that bin. \ue200filecite\ue202turn4file4\ue201 No automatic sibling co-location: temporal siblings of the same qubit are not forced into the same bin (teledata-style cuts are therefore possible). \ue200filecite\ue202turn4file9\ue201 Temporal validity: candidates are expanded through a time-respecting frontier (edges \u201cactivate\u201d at the max time of their pins). \ue200filecite\ue202turn4file18\ue201","title":"Core mechanics (what changed vs the older version)"},{"location":"passes/#high-level-algorithm-3-phases","text":"This is easiest to read as three phases, mirroring the code structure. \ue200filecite\ue202turn4file4\ue201","title":"High-level algorithm (3 phases)"},{"location":"passes/#phase-1-seed-selection-per-bin","text":"For each bin (QPU), pick the earliest unassigned node as the seed (ties broken deterministically by node id), place it, and initialise a frontier of temporally valid neighbours. \ue200filecite\ue202turn4file4\ue202turn4file18\ue201","title":"Phase 1: Seed selection (per bin)"},{"location":"passes/#phase-2-temporal-greedy-expansion-priority-frontier-beam-k-scoring","text":"While the bin still has capacity left: Maintain a min-heap frontier keyed by earliest valid connection time. Pop up to beam_k earliest candidates (skipping candidates already rejected for this bin). Score those candidates by delta cut cost (lower is better, negative means it reduces cuts). Pick the best-scoring candidate; if it would exceed capacity (by introducing a new qubit when already at cap ), reject it for this bin and try again. This is \u201ctemporal greedy\u201d for reach, with \u201cbeam-k\u201d only used for selection among the earliest few frontier nodes , not a global beam search over all unassigned nodes. \ue200filecite\ue202turn4file4\ue202turn4file6\ue202turn4file18\ue201","title":"Phase 2: Temporal greedy expansion (priority frontier + beam-k scoring)"},{"location":"passes/#phase-3-residual-best-fit-placement-delta-cost","text":"Once the per-bin expansions are done, remaining nodes are placed using a best-fit rule: For the next earliest unassigned node, compute its delta cost into every bin that can accept it under capacity. Place it into the bin that minimises delta cost. If no bin can accept it under capacity, try a teledata fallback by placing it into any bin that already contains its logical qubit (so it does not consume additional capacity). If that is also impossible, mark it as unplaceable and continue. This replaces the older \u201cround-robin mop-up\u201d. \ue200filecite\ue202turn4file5\ue201","title":"Phase 3: Residual best-fit placement (delta-cost)"},{"location":"passes/#function-signature","text":"def compute_cut(hdh_graph, k: int, cap: int, *, beam_k: int = 3, backtrack_window: int = 0, polish_1swap_budget: int = 0, restarts: int = 1, reserve_frac: float = 0.08, predictive_reject: bool = True, seed: int = 0) -> Tuple[List[Set[str]], int] Note: in the current implementation, only beam_k is operational; the other keyword params are accepted for compatibility but not used. \ue200filecite\ue202turn4file9\ue201","title":"Function signature"},{"location":"passes/#metis-telegate-partitioner","text":"For an alternative approach to partitioning, the library provides the metis_telegate function, which leverages the METIS algorithm (with a fallback to the Kernighan-Lin algorithm if METIS is not available).","title":"METIS Telegate Partitioner"},{"location":"passes/#telegate-graph-construction","text":"Graph transformation: This method first converts the HDH into a \"telegate\" graph using the telegate_hdh function. In this representation: Nodes are the qubits of the quantum circuit (labeled as q{idx} ). Edges represent quantum operations between qubits (i.e., their co-appearance in a quantum hyperedge). Edge weights correspond to the multiplicity of interactions between two qubits. Quantum operation filtering: Only hyperedges marked as quantum operations (with tau attribute = \"q\") are considered when building the telegate graph.","title":"Telegate Graph Construction"},{"location":"passes/#partitioning-process","text":"METIS partitioning: The telegate graph is partitioned using the nxmetis library, which provides Python bindings to the highly efficient METIS graph partitioning tool. If METIS is unavailable, the algorithm automatically falls back to the Kernighan-Lin bisection algorithm from NetworkX. METIS attempts to respect capacity constraints through the tpwgts (target partition weights) and ubvec (unbalance vector) parameters. Overflow repair: Since METIS does not guarantee perfectly balanced partitions, a greedy rebalancing algorithm ( _repair_overflow ) is used to adjust the partitions and ensure that no bin exceeds its qubit capacity. The repair algorithm uses a heuristic gain function ( _best_move_for_node ) to choose which qubits to move between bins. It prioritizes moving qubits that minimize the increase in cut edges.","title":"Partitioning Process"},{"location":"passes/#kahypar-hypergraph-partitioner","text":"cut.py also includes a KaHyPar-based partitioner, taken from the \\href{https://kahypar.org}{KaHyPar library}. kahypar_cutter (qubit-level hypergraph): Vertices are logical qubits . Each HDH hyperedge contributes a hyperedge over the qubits that appear in it. KaHyPar then runs its multilevel hypergraph partitioning pipeline (coarsening \u2192 initial partition \u2192 refinement), configured by an INI file (e.g., km1_kKaHyPar_sea20.ini ). Capacity is expressed as a balance constraint via KaHyPar\u2019s epsilon (derived from cap relative to the ideal target size n/k ). \ue200filecite\ue202turn4file3\ue201 This means the partitioner primarily \u201cknows\u201d about balancing qubit counts ; it does not model HDH-specific capacity nuances (for example, heterogeneous per-QPU capacities, or time-expanded node effects), and any \u201ccapacity\u201d notion lives inside the balance constraint. kahypar_cutter_nodebalanced (HDH-node-level hypergraph): Vertices are HDH nodes (time-expanded). Balance is therefore in node count , not in unique logical qubits. As a result, it can produce partitions that look well-balanced to KaHyPar but do not respect logical-qubit capacity (it is intentionally a balance-only baseline for comparison). \ue200filecite\ue202turn4file12\ue201","title":"KaHyPar hypergraph partitioner"},{"location":"passes/#function-signature_1","text":"def metis_telegate(hdh: \"HDH\", partitions: int, capacities: int) -> Tuple[List[Set[str]], int, bool, str] Parameters: * hdh : The HDH graph to partition * partitions : Number of partitions (k) * capacities : Capacity per partition (in qubits) Returns: * A tuple of (bins_qubits, cut_cost, respects_capacity, method) where: * bins_qubits is a list of sets, each containing qubit IDs (as strings like \"q0\" , \"q1\" ) in that partition * cut_cost is the number of edges crossing between partitions (unweighted) * respects_capacity is a boolean indicating whether all bins satisfy the capacity constraint * method is either \"metis\" or \"kl\" indicating which algorithm was used","title":"Function Signature"},{"location":"passes/#cut-cost-evaluation","text":"The quality of a partition is determined by the number of \"cuts\"\u2014that is, the number of hyperedges that span across multiple bins. The library provides two functions for this purpose:","title":"Cut Cost Evaluation"},{"location":"passes/#_total_cost_hdh","text":"Calculates the total cost of a partition on an HDH graph. This function: * Iterates through all hyperedges in the HDH * Counts a hyperedge as \"cut\" if its pins (nodes) are distributed across 2 or more bins * Returns the sum of weights of all cut hyperedges * Respects edge weights if defined in the HDH graph (defaults to 1 if not specified) Algorithm: For each hyperedge, count the number of distinct bins containing at least one pin. If this count is \u2265 2, add the edge's weight to the total cost.","title":"_total_cost_hdh"},{"location":"passes/#_cut_edges_unweighted","text":"Counts the number of edges that cross between different bins in a standard graph (used for evaluating telegate graph partitions). This function: * Takes a NetworkX graph and a partition assignment * Counts edges where the two endpoints are in different bins * Returns an unweighted count (each cut edge counts as 1) Use case: This is specifically used by metis_telegate to evaluate the quality of qubit-graph partitions.","title":"_cut_edges_unweighted"},{"location":"passes/#helper-functions-and-internal-components","text":"The cut.py file contains helper utilities for the partitioners (note that some older helpers/classes remain in the file as legacy code paths).","title":"Helper Functions and Internal Components"},{"location":"passes/#temporal-incidence-frontier-utilities-greedy-partitioner","text":"_build_temporal_incidence : builds inc[node] -> [(hyperedge, edge_time)] and pins[hyperedge] -> {nodes} , where edge_time = max(pin_times) ; used to enforce temporal validity. \ue200filecite\ue202turn4file18\ue201 _push_next_valid_neighbors : expands a node into the min-heap frontier, pushing only temporally valid neighbour candidates. \ue200filecite\ue202turn4file18\ue201 _select_best_from_frontier_with_rejected : takes the earliest beam_k frontier items (skipping rejected), evaluates delta cost, and returns the best candidate. \ue200filecite\ue202turn4file6\ue201 _compute_delta_cost_simple : delta in (unweighted) cut-hyperedge count if a node is added to a specific bin. \ue200filecite\ue202turn4file18\ue201 _extract_qubit_id : parses q{idx}_t{t} to extract the logical qubit index for capacity accounting. \ue200filecite\ue202turn4file18\ue201","title":"Temporal incidence + frontier utilities (greedy partitioner)"},{"location":"passes/#metis-utilities","text":"telegate_hdh : converts the HDH to a qubit interaction graph (\u201ctelegate graph\u201d). \ue200filecite\ue202turn4file16\ue201 _repair_overflow , _best_move_for_node , _over_under : post-processing used to fix capacity violations after METIS/KL. \ue200filecite\ue202turn4file10\ue201 _cut_edges_unweighted : unweighted cut-edge count for the telegate graph. \ue200filecite\ue202turn4file10\ue201","title":"METIS utilities"},{"location":"passes/#notes-on-evaluating-partitioners-on-random-circuits","text":"We would like to warn users and partitioning strategy developers that we have found partitioners to behave differently on real quantum workloads when compared to randomly generated ones. As such, we recommend not testing partitioners on randomly generated workloads unless that is specifically your goal. Key considerations: * Circuit structure matters: Real quantum algorithms often have characteristic patterns (e.g., layered structures, specific qubit interaction patterns) that random circuits lack. * Connectivity patterns: Random circuits may not reflect the typical connectivity found in QAOA, VQE, quantum simulation, or other structured quantum algorithms. * Tagging in the database: In the database, you can specify the origin of your workloads, tagging them as \"random\" if appropriate. This helps others understand the context of benchmark results. When developing new partitioning strategies, we strongly encourage testing on workloads representative of your target applications rather than relying solely on random circuit benchmarks.","title":"Notes on Evaluating Partitioners on Random Circuits"},{"location":"passes/#future-enhancements","text":"The current cut.py implementation includes several parameters that are reserved for future enhancements: Backtracking: The backtrack_window parameter is currently unused but reserved for implementing backtracking search Local search: The polish_1swap_budget parameter is disabled for HDH partitioning (as moving whole qubits is computationally heavier than single-node swaps) Predictive rejection: The predictive_reject parameter is reserved for future heuristics These features may be enabled in future versions of the library as the partitioning algorithms continue to evolve.","title":"Future Enhancements"},{"location":"vis/","text":"Visualising HDHs plot_hdh renders an HDH as a time-vs-index diagram, showing how q uantum and c lassical states evolve across timesteps and how hyperedges connect them. plot_hdh(hdh: HDH, save_path: str | None = None) -> None The function returns nothing, and shows the HDH in a python window unless a save_path = hdh.png is set. In this case the image will be directly saved to the input path. It can be used on any HDH, after it is generated from model instructions: import hdh from hdh.models.qca import QCA from hdh.visualize import plot_hdh # Topology of lattice over which QCA will evolve topology = { \"q0\": [\"q1\", \"q2\"], \"q1\": [\"q0\"], \"q2\": [\"q0\"] } measurements = {\"q1\", \"q2\"} qca = QCA(topology=topology, measurements=measurements, steps=3) hdh = qca.build_hdh() # Generate HDH fig = plot_hdh(hdh) # Visualize HDH or directly from \"manually\" defining all nodes and hyperedges of a HDH: import hdh from hdh.hdh import HDH from hdh.visualize import plot_hdh hdh = HDH() # swap hdh.add_node(\"q1_t0\",\"q\",0) hdh.add_node(\"q3_t0\",\"q\",0) hdh.add_node(\"q1_t1\",\"q\",1) hdh.add_node(\"q3_t1\",\"q\",1) hdh.add_node(\"q1_t2\",\"q\",2) hdh.add_node(\"q3_t2\",\"q\",2) hdh.add_node(\"q1_t3\",\"q\",3) hdh.add_node(\"q3_t3\",\"q\",3) hdh.add_hyperedge([\"q1_t0\", \"q1_t1\"], \"q\") hdh.add_hyperedge([\"q3_t0\", \"q3_t1\"], \"q\") hdh.add_hyperedge([\"q1_t1\", \"q3_t1\", \"q1_t2\", \"q3_t2\"], \"q\") hdh.add_hyperedge([\"q1_t2\", \"q1_t3\"], \"q\") hdh.add_hyperedge([\"q3_t2\", \"q3_t3\"], \"q\") # # cnot hdh.add_node(\"q0_t4\",\"q\",4) hdh.add_node(\"q0_t3\",\"q\",3) hdh.add_node(\"q0_t2\",\"q\",2) hdh.add_node(\"q1_t4\",\"q\",4) hdh.add_node(\"q0_t5\",\"q\",5) hdh.add_node(\"q1_t5\",\"q\",5) hdh.add_hyperedge([\"q0_t2\", \"q0_t3\"], \"q\") hdh.add_hyperedge([\"q1_t3\", \"q1_t4\", \"q0_t3\", \"q0_t4\"], \"q\") hdh.add_hyperedge([\"q0_t4\", \"q0_t5\"], \"q\") hdh.add_hyperedge([\"q1_t4\", \"q1_t5\"], \"q\") # meas hdh.add_node(\"c1_t6\",\"c\",6) hdh.add_node(\"q3_t7\",\"q\",7) hdh.add_node(\"q2_t7\",\"q\",7) hdh.add_hyperedge([\"c1_t6\", \"q1_t5\"], \"c\") hdh.add_hyperedge([\"c1_t6\", \"q3_t7\"], \"c\") hdh.add_hyperedge([\"c1_t6\", \"q2_t7\"], \"c\") # target cnot hdh.add_node(\"q3_t8\",\"q\",8) hdh.add_node(\"q4_t8\",\"q\",8) hdh.add_node(\"q3_t9\",\"q\",9) hdh.add_node(\"q4_t9\",\"q\",9) hdh.add_node(\"q4_t7\",\"q\",7) hdh.add_node(\"q4_t10\",\"q\",10) hdh.add_node(\"q3_t10\",\"q\",10) hdh.add_hyperedge([\"q3_t8\", \"q4_t8\",\"q3_t9\", \"q4_t9\"], \"q\") hdh.add_hyperedge([\"q3_t8\", \"q3_t7\"], \"q\") hdh.add_hyperedge([\"q4_t8\", \"q4_t7\"], \"q\") hdh.add_hyperedge([\"q4_t9\", \"q4_t10\"], \"q\") hdh.add_hyperedge([\"q3_t9\", \"q3_t10\"], \"q\") # h gate hdh.add_node(\"q3_t11\",\"q\",11) hdh.add_hyperedge([\"q3_t10\",\"q3_t11\"], \"q\") # meas hdh.add_node(\"q0_t13\",\"q\",13) hdh.add_node(\"c3_t12\",\"c\",12) hdh.add_hyperedge([\"c3_t12\", \"q3_t11\"], \"c\") hdh.add_hyperedge([\"c3_t12\", \"q0_t13\"], \"c\") hdh.add_hyperedge([\"q0_t5\", \"q0_t13\"], \"q\") fig = plot_hdh(hdh,save_path=\"test2.png\") # Visualize HDH A few things to note about the HDH visualizations: Index ordering : y-positions are \u201cflipped\u201d so that the largest index appears at the bottom of the axis, while tick labels still increase upward. This matches typical circuit diagrams where q0 is drawn at the top. This is consistent with popular quantum packages, and allows for users to \"read\" from top to bottom. Participation filter : nodes not present in any edge are omitted from the plot to reduce clutter. Style inference : if the hyperedge type is not defined for an edge, it is inferred from its output nodes (if one of the nodes is classical it assumes classical, otherwise it defaults to quantum). Same-timestep edges : are not drawn (to avoid the misconception that cuts can occur \"instantaneously\" -> they require communication primitives and thus operations spanning different partitions must have experienced a cut previous to their execution). Lazy qubit appearance : qubits appear in the diagram only when first used in an operation. Their initialization motifs (e.g. in the Circuit model) are delayed until one timestep before the first gate on that qubit. This reduces clutter from unused wires. Example: in the HDH above, qubit 4 only begins appearing from timestep 7.","title":"Visualizing HDHs"},{"location":"vis/#visualising-hdhs","text":"plot_hdh renders an HDH as a time-vs-index diagram, showing how q uantum and c lassical states evolve across timesteps and how hyperedges connect them. plot_hdh(hdh: HDH, save_path: str | None = None) -> None The function returns nothing, and shows the HDH in a python window unless a save_path = hdh.png is set. In this case the image will be directly saved to the input path. It can be used on any HDH, after it is generated from model instructions: import hdh from hdh.models.qca import QCA from hdh.visualize import plot_hdh # Topology of lattice over which QCA will evolve topology = { \"q0\": [\"q1\", \"q2\"], \"q1\": [\"q0\"], \"q2\": [\"q0\"] } measurements = {\"q1\", \"q2\"} qca = QCA(topology=topology, measurements=measurements, steps=3) hdh = qca.build_hdh() # Generate HDH fig = plot_hdh(hdh) # Visualize HDH or directly from \"manually\" defining all nodes and hyperedges of a HDH: import hdh from hdh.hdh import HDH from hdh.visualize import plot_hdh hdh = HDH() # swap hdh.add_node(\"q1_t0\",\"q\",0) hdh.add_node(\"q3_t0\",\"q\",0) hdh.add_node(\"q1_t1\",\"q\",1) hdh.add_node(\"q3_t1\",\"q\",1) hdh.add_node(\"q1_t2\",\"q\",2) hdh.add_node(\"q3_t2\",\"q\",2) hdh.add_node(\"q1_t3\",\"q\",3) hdh.add_node(\"q3_t3\",\"q\",3) hdh.add_hyperedge([\"q1_t0\", \"q1_t1\"], \"q\") hdh.add_hyperedge([\"q3_t0\", \"q3_t1\"], \"q\") hdh.add_hyperedge([\"q1_t1\", \"q3_t1\", \"q1_t2\", \"q3_t2\"], \"q\") hdh.add_hyperedge([\"q1_t2\", \"q1_t3\"], \"q\") hdh.add_hyperedge([\"q3_t2\", \"q3_t3\"], \"q\") # # cnot hdh.add_node(\"q0_t4\",\"q\",4) hdh.add_node(\"q0_t3\",\"q\",3) hdh.add_node(\"q0_t2\",\"q\",2) hdh.add_node(\"q1_t4\",\"q\",4) hdh.add_node(\"q0_t5\",\"q\",5) hdh.add_node(\"q1_t5\",\"q\",5) hdh.add_hyperedge([\"q0_t2\", \"q0_t3\"], \"q\") hdh.add_hyperedge([\"q1_t3\", \"q1_t4\", \"q0_t3\", \"q0_t4\"], \"q\") hdh.add_hyperedge([\"q0_t4\", \"q0_t5\"], \"q\") hdh.add_hyperedge([\"q1_t4\", \"q1_t5\"], \"q\") # meas hdh.add_node(\"c1_t6\",\"c\",6) hdh.add_node(\"q3_t7\",\"q\",7) hdh.add_node(\"q2_t7\",\"q\",7) hdh.add_hyperedge([\"c1_t6\", \"q1_t5\"], \"c\") hdh.add_hyperedge([\"c1_t6\", \"q3_t7\"], \"c\") hdh.add_hyperedge([\"c1_t6\", \"q2_t7\"], \"c\") # target cnot hdh.add_node(\"q3_t8\",\"q\",8) hdh.add_node(\"q4_t8\",\"q\",8) hdh.add_node(\"q3_t9\",\"q\",9) hdh.add_node(\"q4_t9\",\"q\",9) hdh.add_node(\"q4_t7\",\"q\",7) hdh.add_node(\"q4_t10\",\"q\",10) hdh.add_node(\"q3_t10\",\"q\",10) hdh.add_hyperedge([\"q3_t8\", \"q4_t8\",\"q3_t9\", \"q4_t9\"], \"q\") hdh.add_hyperedge([\"q3_t8\", \"q3_t7\"], \"q\") hdh.add_hyperedge([\"q4_t8\", \"q4_t7\"], \"q\") hdh.add_hyperedge([\"q4_t9\", \"q4_t10\"], \"q\") hdh.add_hyperedge([\"q3_t9\", \"q3_t10\"], \"q\") # h gate hdh.add_node(\"q3_t11\",\"q\",11) hdh.add_hyperedge([\"q3_t10\",\"q3_t11\"], \"q\") # meas hdh.add_node(\"q0_t13\",\"q\",13) hdh.add_node(\"c3_t12\",\"c\",12) hdh.add_hyperedge([\"c3_t12\", \"q3_t11\"], \"c\") hdh.add_hyperedge([\"c3_t12\", \"q0_t13\"], \"c\") hdh.add_hyperedge([\"q0_t5\", \"q0_t13\"], \"q\") fig = plot_hdh(hdh,save_path=\"test2.png\") # Visualize HDH A few things to note about the HDH visualizations: Index ordering : y-positions are \u201cflipped\u201d so that the largest index appears at the bottom of the axis, while tick labels still increase upward. This matches typical circuit diagrams where q0 is drawn at the top. This is consistent with popular quantum packages, and allows for users to \"read\" from top to bottom. Participation filter : nodes not present in any edge are omitted from the plot to reduce clutter. Style inference : if the hyperedge type is not defined for an edge, it is inferred from its output nodes (if one of the nodes is classical it assumes classical, otherwise it defaults to quantum). Same-timestep edges : are not drawn (to avoid the misconception that cuts can occur \"instantaneously\" -> they require communication primitives and thus operations spanning different partitions must have experienced a cut previous to their execution). Lazy qubit appearance : qubits appear in the diagram only when first used in an operation. Their initialization motifs (e.g. in the Circuit model) are delayed until one timestep before the first gate on that qubit. This reduces clutter from unused wires. Example: in the HDH above, qubit 4 only begins appearing from timestep 7.","title":"Visualising HDHs"},{"location":"why/","text":"Why HDHs? Hybrid Dependency Hypergraphs provide a model-agnostic abstraction for quantum computation.Their purpose is to enable distributed quantum computing, meaning the collaboration of various quantum computers to complete a task greater than their individual capacities. Different quantum computers work with different computational models due to physical constraints or architectural choices.For example, photonic systems do not naturally support the circuit model because photons interact weakly, while superconducting devices commonly use circuits but can support cellular automata or quantum walks when those models map better to the task. HDHs encode not only these models, but also the hybrid nature of quantum processing, where classical control and classical-quantum feedback loops are intrinsic.This also enables the representation of hybrid algorithms, which currently deliver the most practical gains. HDHs sit between high-level quantum programming languages and machine-level instructions.Because they are based on quantum models, they provide a familiar representation for developers and accurately capture workload structure and weight. These properties make the abstraction powerful, as it enables the creation of a dependency pattern from any quantum workload.These patterns can then be partitioned (or cut) and distributed to the different devices collaborating to complete the task. The notion of partitioning computation at a hypergraph level predates quantum computing and has been explored in HPC, without much success.The difference is that quantum computation uniquely exposes explicit dependency structures in its models.This enables well-defined and meaningful partitioning, contrary to classical settings where dependencies can be implicit and costly to infer.The idea of using hypergraph partitioning for distribution has circulated in the quantum community since at least 2018.HDHs aim to serve as a baseline abstraction over which partitioning techniques can be evaluated fairly. State-of-the-art hypergraph partitioning techniques used in this context include KaHyPar and Fiduccia\u2013Mattheyses.However, we do not yet have a good understanding of how well these and other partitioners compare across the recurring dependency structures that arise in quantum computing.Current efforts rely on ad-hoc, independently constructed representations, which makes cross-method evaluation difficult.HDHs aim to make the construction of these patterns simple, transparent, and fast, enabling systematic comparison of existing partitioners and supporting the development of specialised techniques for distributed quantum workloads. It is important to note that the goal of HDHs is not to establish or model the physical quantum communication layer, nor to define classical channels between devices when no quantum connection exists.Those aspects are valuable for full-stack evaluation, and future versions will include quantum communication primitives, but HDHs focus first on the abstraction of computation, not the transport layer. Similarly, HDHs do not prescribe partitioning strategies.Their role is to provide a fair, consistent substrate on which partitioning techniques can be applied and compared.We will provide reference implementations for standard and emerging partitioners, and record partitioning outcomes in our database to enable systematic, cross-method evaluation.","title":"Why HDHs?"},{"location":"why/#why-hdhs","text":"Hybrid Dependency Hypergraphs provide a model-agnostic abstraction for quantum computation.Their purpose is to enable distributed quantum computing, meaning the collaboration of various quantum computers to complete a task greater than their individual capacities. Different quantum computers work with different computational models due to physical constraints or architectural choices.For example, photonic systems do not naturally support the circuit model because photons interact weakly, while superconducting devices commonly use circuits but can support cellular automata or quantum walks when those models map better to the task. HDHs encode not only these models, but also the hybrid nature of quantum processing, where classical control and classical-quantum feedback loops are intrinsic.This also enables the representation of hybrid algorithms, which currently deliver the most practical gains. HDHs sit between high-level quantum programming languages and machine-level instructions.Because they are based on quantum models, they provide a familiar representation for developers and accurately capture workload structure and weight. These properties make the abstraction powerful, as it enables the creation of a dependency pattern from any quantum workload.These patterns can then be partitioned (or cut) and distributed to the different devices collaborating to complete the task. The notion of partitioning computation at a hypergraph level predates quantum computing and has been explored in HPC, without much success.The difference is that quantum computation uniquely exposes explicit dependency structures in its models.This enables well-defined and meaningful partitioning, contrary to classical settings where dependencies can be implicit and costly to infer.The idea of using hypergraph partitioning for distribution has circulated in the quantum community since at least 2018.HDHs aim to serve as a baseline abstraction over which partitioning techniques can be evaluated fairly. State-of-the-art hypergraph partitioning techniques used in this context include KaHyPar and Fiduccia\u2013Mattheyses.However, we do not yet have a good understanding of how well these and other partitioners compare across the recurring dependency structures that arise in quantum computing.Current efforts rely on ad-hoc, independently constructed representations, which makes cross-method evaluation difficult.HDHs aim to make the construction of these patterns simple, transparent, and fast, enabling systematic comparison of existing partitioners and supporting the development of specialised techniques for distributed quantum workloads. It is important to note that the goal of HDHs is not to establish or model the physical quantum communication layer, nor to define classical channels between devices when no quantum connection exists.Those aspects are valuable for full-stack evaluation, and future versions will include quantum communication primitives, but HDHs focus first on the abstraction of computation, not the transport layer. Similarly, HDHs do not prescribe partitioning strategies.Their role is to provide a fair, consistent substrate on which partitioning techniques can be applied and compared.We will provide reference implementations for standard and emerging partitioners, and record partitioning outcomes in our database to enable systematic, cross-method evaluation.","title":"Why HDHs?"}]}