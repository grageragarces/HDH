{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the HDH library HDH refers to Hybrid Dependency Hypergraph , an abstraction developped to enable the partitioning of quantum computations in the context of Distributed Quantum Computing. HDHs are a directed hypergraph based abstraction that encodes the dependencies generated by entangling quantum operations displaying the state transformations performed along the computation. They aim to serve as a unifying abstraction capable of encoding any quantum workload regardless of the computational model it is designed in, that enables all valid partitions of a computation (superseding telegate and teledata abstractions). Furthermore HDHs, as their name implies, also encode classical information enabling the outline of natural classical partitioning points, such as mid-circuit measurements. You can find an in depth description of HDHs as an abstraction here: An introduction to HDHs . Further explanations of how HDHs are generated from quantum computational models can be found here: Generation of HDHs from model instructions . You can also find a Database with over 2000 HDHs here: HDH Database . The source code can be found: https://github.com/grageragarces/HDH . If you find any bugs or have any proposals for the library we encourage you to open an issue. A guide on how to do this can be found here . HDHs were originally developped by Maria Gragera Garces, Chris Heunen and Mahesh K. Marina. Publications, posters and talks related to the HDH project can be found here: Literature . The library is currently under MIT License. The development of this library was kindly supported by a Unitary Fund microgrant, as well as the Engineering and Physical Sciences Research Council (grant number EP/W524384/1), the University of Edinburgh, and VeriQloud .","title":"Home"},{"location":"#welcome-to-the-hdh-library","text":"HDH refers to Hybrid Dependency Hypergraph , an abstraction developped to enable the partitioning of quantum computations in the context of Distributed Quantum Computing. HDHs are a directed hypergraph based abstraction that encodes the dependencies generated by entangling quantum operations displaying the state transformations performed along the computation. They aim to serve as a unifying abstraction capable of encoding any quantum workload regardless of the computational model it is designed in, that enables all valid partitions of a computation (superseding telegate and teledata abstractions). Furthermore HDHs, as their name implies, also encode classical information enabling the outline of natural classical partitioning points, such as mid-circuit measurements. You can find an in depth description of HDHs as an abstraction here: An introduction to HDHs . Further explanations of how HDHs are generated from quantum computational models can be found here: Generation of HDHs from model instructions . You can also find a Database with over 2000 HDHs here: HDH Database . The source code can be found: https://github.com/grageragarces/HDH . If you find any bugs or have any proposals for the library we encourage you to open an issue. A guide on how to do this can be found here . HDHs were originally developped by Maria Gragera Garces, Chris Heunen and Mahesh K. Marina. Publications, posters and talks related to the HDH project can be found here: Literature . The library is currently under MIT License. The development of this library was kindly supported by a Unitary Fund microgrant, as well as the Engineering and Physical Sciences Research Council (grant number EP/W524384/1), the University of Edinburgh, and VeriQloud .","title":"Welcome to the HDH library"},{"location":"database/","text":"The HDH Database To support reproducible evaluation and training of partitioning strategies, this library includes a database of pre-generated HDHs. We aim for this resource to facilitate benchmarking across diverse workloads and enables the development of learning-based distribution agents. Our goal is to extend the database with performance metrics of partitioning techniques for each workload. This will allow the community to build a data-driven understanding of which hypergraph partitioning methods perform best under different conditions. We encourage users to contribute results from their own partitioning methods when working with this database. Instructions for how to upload results can be found below. The database is available in the database-branch of the repository . This separation ensures that users of the main library don\u2019t need to download unnecessary files. The database currently contains: HDHs derived from the Munich Quantum Benchmarking Dataset . The database is organized into two mirrored top-level folders: Workloads : contains the raw workload commands (currently QASM files). HDHs : contains the corresponding Hybrid Dependency Hypergraphs for each workload. Both folders share the same internal structure: Model/Origin_of_Workload/ . where: Model = computational model (e.g., Circuit, MBQC, QW, QCA). Origin_of_Workload = source of the workload (e.g., benchmark suite, custom circuit). File formats Workloads/ QASM files representing the quantum workloads. HDHs/ .pkl : Python*pickled HDH objects for programmatic use. .txt : human*readable text files with annotated metadata. HDH text metadata Each .txt file includes metadata lines before the hypergraph specification: Model type : which computational model the HDH was generated from. Workload origin ; reference to the source workload. Hybrid status : whether the HDH contains both quantum and classical nodes. Node count : total number of nodes in the hypergraph. Connectivity degree : average connectivity of the hypergraph. Disconnected subgraphs : number of disconnected components. Extending the dataset We encourage users to: Add new workloads (QASM or other supported formats ). Generate corresponding HDHs. Propose and document new metrics (e.g., depth, cut size, entanglement width). Pull requests that expand the benchmark set or enrich metadata are very welcome! How to add to this database There are two ways to contribute: 1) Add new workloads + HDHs 1) Place workloads Put your workload origin files under: Workloads/<Model>/<Origin>/ This could be anything from a qasm file to circuit generation code. If the HDH is not generated from functions within the library, we request you add a README.md to your origin folder explaining how the HDHs were generated. Example: Workloads/Circuits/MQTBench/qft_8.qasm 2) Run the converter Convert the files (qasm strings, qiskit circuits, ...) to HDHs. HDHs/<Model>/<Origin>/pkl/.pkl HDHs/<Model>/<Origin>/text/__nodes.csv HDHs/<Model>/<Origin>/text/__edges.csv HDHs/<Model>/<Origin>/text/__edge_members.csv The example script below converts QASM \u2192 HDH and writes them as expected (it can be adapted for other models): Converter script (QASM \u2192 HDH \u2192 {pkl,csv}) Requirements: tqdm, the HDH library available on PYTHONPATH, and your QASM converter (hdh.converters.qasm.from_qasm). #!/usr/bin/env python3 import sys import os import csv import json import pickle from pathlib import Path from tqdm import tqdm import argparse # Repo import path (one level up) sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))) from hdh.converters.qasm import from_qasm BASE_DIR = Path(__file__).resolve().parent def ensure_dir(p: Path): p.mkdir(parents=True, exist_ok=True) def save_pkl(hdh_graph, out_base: Path): p = out_base.with_suffix(\".pkl\") with open(p, \"wb\") as f: pickle.dump(hdh_graph, f) return p def save_nodes_csv(hdh_graph, out_path: Path): with open(out_path, \"w\", newline=\"\", encoding=\"utf-8\") as f: w = csv.writer(f) w.writerow([\"node_id\", \"type\", \"time\", \"realisation\"]) # a|p for nid in sorted(hdh_graph.S): w.writerow([ nid, hdh_graph.sigma.get(nid, \"\"), getattr(hdh_graph, \"time_map\", {}).get(nid, \"\"), hdh_graph.upsilon.get(nid, \"\") ]) def save_edges_csvs(hdh_graph, edges_path: Path, members_path: Path): # Stable ordering to assign edge_index deterministically edges_sorted = sorted(hdh_graph.C, key=lambda e: tuple(sorted(e))) # edges table (one row per edge) with open(edges_path, \"w\", newline=\"\", encoding=\"utf-8\") as f: w = csv.writer(f) w.writerow([ \"edge_index\", \"type\", # q|c \"realisation\", # a|p \"gate_name\", \"role\", # teledata|telegate|'' \"edge_args\", # JSON \"edge_metadata\" # JSON ]) for idx, e in enumerate(edges_sorted): w.writerow([ idx, hdh_graph.tau.get(e, \"\"), hdh_graph.phi.get(e, \"\"), getattr(hdh_graph, \"gate_name\", {}).get(e, \"\"), getattr(hdh_graph, \"edge_role\", {}).get(e, \"\"), json.dumps(getattr(hdh_graph, \"edge_args\", {}).get(e, None), ensure_ascii=False) if e in getattr(hdh_graph, \"edge_args\", {}) else \"\", json.dumps(getattr(hdh_graph, \"edge_metadata\", {}).get(e, None), ensure_ascii=False) if e in getattr(hdh_graph, \"edge_metadata\", {}) else \"\" ]) # edge_members table (one row per (edge,node)) with open(members_path, \"w\", newline=\"\", encoding=\"utf-8\") as f: w = csv.writer(f) w.writerow([\"edge_index\", \"node_id\"]) for idx, e in enumerate(edges_sorted): for nid in sorted(e): w.writerow([idx, nid]) def main(): ap = argparse.ArgumentParser(description=\"Convert QASM workloads to HDH artifacts\") ap.add_argument(\"--model\", default=\"Circuits\", help=\"Model folder under Workloads/ and HDHs/\") ap.add_argument(\"--origin\", default=\"MQTBench\", help=\"Origin folder under Workloads/<Model>/ and HDHs/<Model>/\") ap.add_argument(\"--limit\", type=int, default=None, help=\"Max number of files to convert (useful for large datasets)\") ap.add_argument(\"--src-root\", default=None, help=\"Override source root. Default: Workloads/<Model>/<Origin>\") args = ap.parse_args() SRC_DIR = Path(args.src_root) if args.src_root else BASE_DIR / \"Workloads\" / args.model / args.origin DST_ROOT = BASE_DIR / \"HDHs\" / args.model / args.origin PKL_ROOT = DST_ROOT / \"pkl\" TXT_ROOT = DST_ROOT / \"text\" IMG_ROOT = DST_ROOT / \"images\" # reserved for future visual exports if not SRC_DIR.exists(): print(f\"[error] Source directory not found: {SRC_DIR}\") sys.exit(1) for d in (PKL_ROOT, TXT_ROOT, IMG_ROOT): ensure_dir(d) qasm_files = sorted(SRC_DIR.rglob(\"*.qasm\")) if not qasm_files: print(f\"[info] No .qasm files found under {SRC_DIR}\") return if args.limit is not None: qasm_files = qasm_files[: args.limit] ok = fail = 0 with tqdm(total=len(qasm_files), desc=\"Converting QASM \u2192 HDH\", unit=\"file\", dynamic_ncols=True, mininterval=0.2) as pbar: for qf in qasm_files: rel = qf.relative_to(SRC_DIR) stem = rel.stem pkl_dir = PKL_ROOT / rel.parent txt_dir = TXT_ROOT / rel.parent for d in (pkl_dir, txt_dir): ensure_dir(d) pbar.set_postfix_str(str(rel)) pbar.refresh() try: hdh_graph = from_qasm(\"file\", str(qf)) # pkl save_pkl(hdh_graph, pkl_dir / stem) # text (CSV) save_nodes_csv(hdh_graph, (txt_dir / f\"{stem}__nodes.csv\")) save_edges_csvs( hdh_graph, edges_path=(txt_dir / f\"{stem}__edges.csv\"), members_path=(txt_dir / f\"{stem}__edge_members.csv\"), ) ok += 1 except Exception as e: tqdm.write(f\"[fail] {qf}: {e}\") fail += 1 finally: pbar.update(1) print(f\"[done] Converted: {ok} | Failed: {fail} | Total processed: {len(qasm_files)}\") if __name__ == \"__main__\": main() 3) Verify & inspect Please open at least one of the text/*.csv files (human-readable) and load at least one of the pkl/*.pkl objects in Python, to check everything works! 4) Submit a PR If all went smoothly and you're happy to, submit a PR with your workloads, HDHs, and metrics back to the repository so we can keep growing our testing database. If you are going to also submit partitioning method results we recommend to wait and do it all in one! Note that the datafiles might be a bit too big to directly upload. In that is the case try doing so with LFS : macOS: brew install git-lfs git lfs install Debian/Ubuntu: sudo apt-get update sudo apt-get install git-lfs git lfs install Windows: winget install Git.GitLFS git lfs install From repo root (make sure to change <Origin> for your Origin name): git lfs track \"*.csv\" git add .gitattributes git commit -m \"Adding <Origin> HDHs to database\" git push -u origin main You may need to add your origin git remote add origin https://github.com/<you>/<repo>.git Once you've got it commited to your fork you can push it through a pull request as per usual. 2) Add partitioning method results If you want to share partitioning method results , you can do so by adding adding the partitioning method metadata to the HDHs/<Model>/<Origin>/Partitions/partitions_all.csv file. These files may look like: file,n_qubits,k_partitions,greedy_bins,greedy_cost,metis_bins,metis_cost,metis_fails,metis_method,greedytg_bins,greedytg_bins_cost,best ae_indep_qiskit_10.qasm,10,2,\"[[\"\"q0\"\",\"\"q1\"\",\"\"q2\"\",\"\"q3\"\",\"\"q8\"\"],[\"\"q4\"\",\"\"q5\"\",\"\"q6\"\",\"\"q7\"\",\"\"q9\"\"]]\",30,\"[[\"\"q1\"\",\"\"q3\"\",\"\"q5\"\",\"\"q6\"\",\"\"q7\"\"],[\"\"q0\"\",\"\"q2\"\",\"\"q4\"\",\"\"q8\"\",\"\"q9\"\"]]\",25,False,kl,\"[[\"\"q0\"\",\"\"q1\"\",\"\"q2\"\",\"\"q3\"\",\"\"q9\"\"],[\"\"q4\"\",\"\"q5\"\",\"\"q6\"\",\"\"q7\"\",\"\"q8\"\"]]\",30,metis_tl ae_indep_qiskit_10.qasm,10,3,\"[[\"\"q0\"\",\"\"q1\"\",\"\"q2\"\",\"\"q8\"\"],[\"\"q3\"\",\"\"q4\"\",\"\"q6\"\",\"\"q7\"\"],[\"\"q5\"\",\"\"q9\"\"]]\",40,\"[[\"\"q3\"\",\"\"q5\"\",\"\"q6\"\",\"\"q7\"\"],[\"\"q0\"\",\"\"q2\"\"],[\"\"q1\"\",\"\"q4\"\",\"\"q8\"\",\"\"q9\"\"]]\",32,False,kl,\"[[\"\"q0\"\",\"\"q1\"\",\"\"q2\"\",\"\"q9\"\"],[\"\"q3\"\",\"\"q4\"\",\"\"q5\"\",\"\"q6\"\"],[\"\"q7\"\",\"\"q8\"\"]]\",38,metis_tl ae_indep_qiskit_10.qasm,10,4,\"[[\"\"q0\"\",\"\"q1\"\",\"\"q8\"\"],[\"\"q2\"\",\"\"q3\"\",\"\"q7\"\"],[\"\"q4\"\",\"\"q5\"\",\"\"q6\"\"],[\"\"q9\"\"]]\",45,\"[[\"\"q5\"\",\"\"q7\"\",\"\"q9\"\"],[\"\"q2\"\",\"\"q8\"\"],[\"\"q0\"\",\"\"q4\"\"],[\"\"q1\"\",\"\"q3\"\",\"\"q6\"\"]]\",37,False,kl,\"[[\"\"q0\"\",\"\"q1\"\",\"\"q9\"\"],[\"\"q2\"\",\"\"q3\"\",\"\"q4\"\"],[\"\"q5\"\",\"\"q6\"\",\"\"q7\"\"],[\"\"q8\"\"]]\",43,metis_tl First the metadata: file : name of the origin file n_qubits : number of qubits in workload k_partitions : number of partitions made by the method. For instance if you cut your workload once you only create 2 partitions Then partitioners results can be added. In this example we can see 3 partitioning strategies: greedy, metis and greedytg. They correspond to: Greedy (HDH) : Partitions directly on the HDH hypergraph where each hyperedge captures one operation\u2019s dependency set. We fill bins sequentially: order qubits by a heuristic (e.g., incident cut weight, then degree), and place each into the earliest bin that (i) respects the logical-qubit capacity and (ii) gives the smallest marginal cut increase. If nothing fits, open the next bin up to k. Cost = sum of weights of hyperedges spanning >1 bin (default weight 1 per op; domain weights optional). METIS (Telegate graph) : Converts the workload into a telegate qubit-interaction graph (nodes = logical qubits; edge weights = interaction pressure indicating a non-local gate would require a \u201ctelegate\u201d communication if cut). Uses the METIS library to compute a k-way partition with balance constraints and minimal cut on this graph. Partitions are then re-evaluated on the HDH cost for apples-to-apples comparison. We typically set edge weights from interaction counts; you can also up-weight edges representing expensive non-local primitives to steer METIS away from cutting them. Greedy-TG (Telegate graph) : Same fill-first policy as Greedy (HDH), but decisions are made on the telegate graph. Nodes are qubits; edge weights reflect how costly it is to separate two qubits (i.e., expected telegate load). Each qubit goes to the earliest feasible bin that minimizes marginal cut on the telegate graph, ensuring a fair, representation-matched comparison with the HDH greedy approach. All this information regarding the method used and its origin must be saved in HDHs/<Model>/<Origin>/Partitions/README.md , otherwise the data will not be merged. As you can see in the example depending on the strategy you can have more or less saved data. Mandatory columns include: quantum communication cost achieved ( cost ) = number of quantum partitioned hyperedges the sets of qubits per partitions ( bins ) Additionally, best must be re-calculated. Best corresponds to the name of the method with the lowest cost. In this example, additional metadata includes the sub-method used within the METIS partitioner (it can default to various sub-methods if the original fails), as well as a failure state for METIS. The failure state is very important for methods that do not assure the ability to respect a given capacity. Capacity (i.e., the maximum number of qubits allowed in one partition) should be set to the total number of qubits divided by the number of partitions (rounded up to the next integer). If the partitioner cannot respect this capacity, the potential failure status should be logged, and if true the method should not be considered in the best evaluation. An explanation on whether these types of additional logs are necessary must be added to any commit adding new data to the database. If they are needed, an explanation of what is added is also required.","title":"The HDH Database"},{"location":"database/#the-hdh-database","text":"To support reproducible evaluation and training of partitioning strategies, this library includes a database of pre-generated HDHs. We aim for this resource to facilitate benchmarking across diverse workloads and enables the development of learning-based distribution agents. Our goal is to extend the database with performance metrics of partitioning techniques for each workload. This will allow the community to build a data-driven understanding of which hypergraph partitioning methods perform best under different conditions. We encourage users to contribute results from their own partitioning methods when working with this database. Instructions for how to upload results can be found below. The database is available in the database-branch of the repository . This separation ensures that users of the main library don\u2019t need to download unnecessary files. The database currently contains: HDHs derived from the Munich Quantum Benchmarking Dataset . The database is organized into two mirrored top-level folders: Workloads : contains the raw workload commands (currently QASM files). HDHs : contains the corresponding Hybrid Dependency Hypergraphs for each workload. Both folders share the same internal structure: Model/Origin_of_Workload/ . where: Model = computational model (e.g., Circuit, MBQC, QW, QCA). Origin_of_Workload = source of the workload (e.g., benchmark suite, custom circuit).","title":"The HDH Database"},{"location":"database/#file-formats","text":"Workloads/ QASM files representing the quantum workloads. HDHs/ .pkl : Python*pickled HDH objects for programmatic use. .txt : human*readable text files with annotated metadata.","title":"File formats"},{"location":"database/#hdh-text-metadata","text":"Each .txt file includes metadata lines before the hypergraph specification: Model type : which computational model the HDH was generated from. Workload origin ; reference to the source workload. Hybrid status : whether the HDH contains both quantum and classical nodes. Node count : total number of nodes in the hypergraph. Connectivity degree : average connectivity of the hypergraph. Disconnected subgraphs : number of disconnected components.","title":"HDH text metadata"},{"location":"database/#extending-the-dataset","text":"We encourage users to: Add new workloads (QASM or other supported formats ). Generate corresponding HDHs. Propose and document new metrics (e.g., depth, cut size, entanglement width). Pull requests that expand the benchmark set or enrich metadata are very welcome!","title":"Extending the dataset"},{"location":"database/#how-to-add-to-this-database","text":"There are two ways to contribute:","title":"How to add to this database"},{"location":"database/#1-add-new-workloads-hdhs","text":"","title":"1) Add new workloads + HDHs"},{"location":"database/#1-place-workloads","text":"Put your workload origin files under: Workloads/<Model>/<Origin>/ This could be anything from a qasm file to circuit generation code. If the HDH is not generated from functions within the library, we request you add a README.md to your origin folder explaining how the HDHs were generated. Example: Workloads/Circuits/MQTBench/qft_8.qasm","title":"1) Place workloads"},{"location":"database/#2-run-the-converter","text":"Convert the files (qasm strings, qiskit circuits, ...) to HDHs. HDHs/<Model>/<Origin>/pkl/.pkl HDHs/<Model>/<Origin>/text/__nodes.csv HDHs/<Model>/<Origin>/text/__edges.csv HDHs/<Model>/<Origin>/text/__edge_members.csv The example script below converts QASM \u2192 HDH and writes them as expected (it can be adapted for other models):","title":"2) Run the converter"},{"location":"database/#converter-script-qasm-hdh-pklcsv","text":"Requirements: tqdm, the HDH library available on PYTHONPATH, and your QASM converter (hdh.converters.qasm.from_qasm). #!/usr/bin/env python3 import sys import os import csv import json import pickle from pathlib import Path from tqdm import tqdm import argparse # Repo import path (one level up) sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))) from hdh.converters.qasm import from_qasm BASE_DIR = Path(__file__).resolve().parent def ensure_dir(p: Path): p.mkdir(parents=True, exist_ok=True) def save_pkl(hdh_graph, out_base: Path): p = out_base.with_suffix(\".pkl\") with open(p, \"wb\") as f: pickle.dump(hdh_graph, f) return p def save_nodes_csv(hdh_graph, out_path: Path): with open(out_path, \"w\", newline=\"\", encoding=\"utf-8\") as f: w = csv.writer(f) w.writerow([\"node_id\", \"type\", \"time\", \"realisation\"]) # a|p for nid in sorted(hdh_graph.S): w.writerow([ nid, hdh_graph.sigma.get(nid, \"\"), getattr(hdh_graph, \"time_map\", {}).get(nid, \"\"), hdh_graph.upsilon.get(nid, \"\") ]) def save_edges_csvs(hdh_graph, edges_path: Path, members_path: Path): # Stable ordering to assign edge_index deterministically edges_sorted = sorted(hdh_graph.C, key=lambda e: tuple(sorted(e))) # edges table (one row per edge) with open(edges_path, \"w\", newline=\"\", encoding=\"utf-8\") as f: w = csv.writer(f) w.writerow([ \"edge_index\", \"type\", # q|c \"realisation\", # a|p \"gate_name\", \"role\", # teledata|telegate|'' \"edge_args\", # JSON \"edge_metadata\" # JSON ]) for idx, e in enumerate(edges_sorted): w.writerow([ idx, hdh_graph.tau.get(e, \"\"), hdh_graph.phi.get(e, \"\"), getattr(hdh_graph, \"gate_name\", {}).get(e, \"\"), getattr(hdh_graph, \"edge_role\", {}).get(e, \"\"), json.dumps(getattr(hdh_graph, \"edge_args\", {}).get(e, None), ensure_ascii=False) if e in getattr(hdh_graph, \"edge_args\", {}) else \"\", json.dumps(getattr(hdh_graph, \"edge_metadata\", {}).get(e, None), ensure_ascii=False) if e in getattr(hdh_graph, \"edge_metadata\", {}) else \"\" ]) # edge_members table (one row per (edge,node)) with open(members_path, \"w\", newline=\"\", encoding=\"utf-8\") as f: w = csv.writer(f) w.writerow([\"edge_index\", \"node_id\"]) for idx, e in enumerate(edges_sorted): for nid in sorted(e): w.writerow([idx, nid]) def main(): ap = argparse.ArgumentParser(description=\"Convert QASM workloads to HDH artifacts\") ap.add_argument(\"--model\", default=\"Circuits\", help=\"Model folder under Workloads/ and HDHs/\") ap.add_argument(\"--origin\", default=\"MQTBench\", help=\"Origin folder under Workloads/<Model>/ and HDHs/<Model>/\") ap.add_argument(\"--limit\", type=int, default=None, help=\"Max number of files to convert (useful for large datasets)\") ap.add_argument(\"--src-root\", default=None, help=\"Override source root. Default: Workloads/<Model>/<Origin>\") args = ap.parse_args() SRC_DIR = Path(args.src_root) if args.src_root else BASE_DIR / \"Workloads\" / args.model / args.origin DST_ROOT = BASE_DIR / \"HDHs\" / args.model / args.origin PKL_ROOT = DST_ROOT / \"pkl\" TXT_ROOT = DST_ROOT / \"text\" IMG_ROOT = DST_ROOT / \"images\" # reserved for future visual exports if not SRC_DIR.exists(): print(f\"[error] Source directory not found: {SRC_DIR}\") sys.exit(1) for d in (PKL_ROOT, TXT_ROOT, IMG_ROOT): ensure_dir(d) qasm_files = sorted(SRC_DIR.rglob(\"*.qasm\")) if not qasm_files: print(f\"[info] No .qasm files found under {SRC_DIR}\") return if args.limit is not None: qasm_files = qasm_files[: args.limit] ok = fail = 0 with tqdm(total=len(qasm_files), desc=\"Converting QASM \u2192 HDH\", unit=\"file\", dynamic_ncols=True, mininterval=0.2) as pbar: for qf in qasm_files: rel = qf.relative_to(SRC_DIR) stem = rel.stem pkl_dir = PKL_ROOT / rel.parent txt_dir = TXT_ROOT / rel.parent for d in (pkl_dir, txt_dir): ensure_dir(d) pbar.set_postfix_str(str(rel)) pbar.refresh() try: hdh_graph = from_qasm(\"file\", str(qf)) # pkl save_pkl(hdh_graph, pkl_dir / stem) # text (CSV) save_nodes_csv(hdh_graph, (txt_dir / f\"{stem}__nodes.csv\")) save_edges_csvs( hdh_graph, edges_path=(txt_dir / f\"{stem}__edges.csv\"), members_path=(txt_dir / f\"{stem}__edge_members.csv\"), ) ok += 1 except Exception as e: tqdm.write(f\"[fail] {qf}: {e}\") fail += 1 finally: pbar.update(1) print(f\"[done] Converted: {ok} | Failed: {fail} | Total processed: {len(qasm_files)}\") if __name__ == \"__main__\": main()","title":"Converter script (QASM \u2192 HDH \u2192 {pkl,csv})"},{"location":"database/#3-verify-inspect","text":"Please open at least one of the text/*.csv files (human-readable) and load at least one of the pkl/*.pkl objects in Python, to check everything works!","title":"3) Verify &amp; inspect"},{"location":"database/#4-submit-a-pr","text":"If all went smoothly and you're happy to, submit a PR with your workloads, HDHs, and metrics back to the repository so we can keep growing our testing database. If you are going to also submit partitioning method results we recommend to wait and do it all in one! Note that the datafiles might be a bit too big to directly upload. In that is the case try doing so with LFS : macOS: brew install git-lfs git lfs install Debian/Ubuntu: sudo apt-get update sudo apt-get install git-lfs git lfs install Windows: winget install Git.GitLFS git lfs install From repo root (make sure to change <Origin> for your Origin name): git lfs track \"*.csv\" git add .gitattributes git commit -m \"Adding <Origin> HDHs to database\" git push -u origin main You may need to add your origin git remote add origin https://github.com/<you>/<repo>.git Once you've got it commited to your fork you can push it through a pull request as per usual.","title":"4) Submit a PR"},{"location":"database/#2-add-partitioning-method-results","text":"If you want to share partitioning method results , you can do so by adding adding the partitioning method metadata to the HDHs/<Model>/<Origin>/Partitions/partitions_all.csv file. These files may look like: file,n_qubits,k_partitions,greedy_bins,greedy_cost,metis_bins,metis_cost,metis_fails,metis_method,greedytg_bins,greedytg_bins_cost,best ae_indep_qiskit_10.qasm,10,2,\"[[\"\"q0\"\",\"\"q1\"\",\"\"q2\"\",\"\"q3\"\",\"\"q8\"\"],[\"\"q4\"\",\"\"q5\"\",\"\"q6\"\",\"\"q7\"\",\"\"q9\"\"]]\",30,\"[[\"\"q1\"\",\"\"q3\"\",\"\"q5\"\",\"\"q6\"\",\"\"q7\"\"],[\"\"q0\"\",\"\"q2\"\",\"\"q4\"\",\"\"q8\"\",\"\"q9\"\"]]\",25,False,kl,\"[[\"\"q0\"\",\"\"q1\"\",\"\"q2\"\",\"\"q3\"\",\"\"q9\"\"],[\"\"q4\"\",\"\"q5\"\",\"\"q6\"\",\"\"q7\"\",\"\"q8\"\"]]\",30,metis_tl ae_indep_qiskit_10.qasm,10,3,\"[[\"\"q0\"\",\"\"q1\"\",\"\"q2\"\",\"\"q8\"\"],[\"\"q3\"\",\"\"q4\"\",\"\"q6\"\",\"\"q7\"\"],[\"\"q5\"\",\"\"q9\"\"]]\",40,\"[[\"\"q3\"\",\"\"q5\"\",\"\"q6\"\",\"\"q7\"\"],[\"\"q0\"\",\"\"q2\"\"],[\"\"q1\"\",\"\"q4\"\",\"\"q8\"\",\"\"q9\"\"]]\",32,False,kl,\"[[\"\"q0\"\",\"\"q1\"\",\"\"q2\"\",\"\"q9\"\"],[\"\"q3\"\",\"\"q4\"\",\"\"q5\"\",\"\"q6\"\"],[\"\"q7\"\",\"\"q8\"\"]]\",38,metis_tl ae_indep_qiskit_10.qasm,10,4,\"[[\"\"q0\"\",\"\"q1\"\",\"\"q8\"\"],[\"\"q2\"\",\"\"q3\"\",\"\"q7\"\"],[\"\"q4\"\",\"\"q5\"\",\"\"q6\"\"],[\"\"q9\"\"]]\",45,\"[[\"\"q5\"\",\"\"q7\"\",\"\"q9\"\"],[\"\"q2\"\",\"\"q8\"\"],[\"\"q0\"\",\"\"q4\"\"],[\"\"q1\"\",\"\"q3\"\",\"\"q6\"\"]]\",37,False,kl,\"[[\"\"q0\"\",\"\"q1\"\",\"\"q9\"\"],[\"\"q2\"\",\"\"q3\"\",\"\"q4\"\"],[\"\"q5\"\",\"\"q6\"\",\"\"q7\"\"],[\"\"q8\"\"]]\",43,metis_tl First the metadata: file : name of the origin file n_qubits : number of qubits in workload k_partitions : number of partitions made by the method. For instance if you cut your workload once you only create 2 partitions Then partitioners results can be added. In this example we can see 3 partitioning strategies: greedy, metis and greedytg. They correspond to: Greedy (HDH) : Partitions directly on the HDH hypergraph where each hyperedge captures one operation\u2019s dependency set. We fill bins sequentially: order qubits by a heuristic (e.g., incident cut weight, then degree), and place each into the earliest bin that (i) respects the logical-qubit capacity and (ii) gives the smallest marginal cut increase. If nothing fits, open the next bin up to k. Cost = sum of weights of hyperedges spanning >1 bin (default weight 1 per op; domain weights optional). METIS (Telegate graph) : Converts the workload into a telegate qubit-interaction graph (nodes = logical qubits; edge weights = interaction pressure indicating a non-local gate would require a \u201ctelegate\u201d communication if cut). Uses the METIS library to compute a k-way partition with balance constraints and minimal cut on this graph. Partitions are then re-evaluated on the HDH cost for apples-to-apples comparison. We typically set edge weights from interaction counts; you can also up-weight edges representing expensive non-local primitives to steer METIS away from cutting them. Greedy-TG (Telegate graph) : Same fill-first policy as Greedy (HDH), but decisions are made on the telegate graph. Nodes are qubits; edge weights reflect how costly it is to separate two qubits (i.e., expected telegate load). Each qubit goes to the earliest feasible bin that minimizes marginal cut on the telegate graph, ensuring a fair, representation-matched comparison with the HDH greedy approach. All this information regarding the method used and its origin must be saved in HDHs/<Model>/<Origin>/Partitions/README.md , otherwise the data will not be merged. As you can see in the example depending on the strategy you can have more or less saved data. Mandatory columns include: quantum communication cost achieved ( cost ) = number of quantum partitioned hyperedges the sets of qubits per partitions ( bins ) Additionally, best must be re-calculated. Best corresponds to the name of the method with the lowest cost. In this example, additional metadata includes the sub-method used within the METIS partitioner (it can default to various sub-methods if the original fails), as well as a failure state for METIS. The failure state is very important for methods that do not assure the ability to respect a given capacity. Capacity (i.e., the maximum number of qubits allowed in one partition) should be set to the total number of qubits divided by the number of partitions (rounded up to the next integer). If the partitioner cannot respect this capacity, the potential failure status should be logged, and if true the method should not be considered in the best evaluation. An explanation on whether these types of additional logs are necessary must be added to any commit adding new data to the database. If they are needed, an explanation of what is added is also required.","title":"2) Add partitioning method results"},{"location":"hdh/","text":"Hybrid Dependency Hypergraphs HDHs are a special type of directed hypergraphs designed to encode temporal and spatial dependencies of quantum computations with the purpose of distributing originally monolothic quantum or hybrid workloads. Hypergraph abstractions are currently the standard approach for partitioning quantum computations in distributed quantum computing, [ 1 ]. HDHs extend this practice by providing a consistent and complete representation framework, intended to support the systematic evaluation and comparison of partitioning strategies. A hypergraph consists of a set of nodes and a family of subsets of these nodes, called hyperedges[ 2 ]. In HDHs these hyperedges can be refered to as connections, as they represent the connectivity requirements between nodes. Meaning that if a HDH is partitioned, the operations represented as inter-partition hyperedges would need to be replaced by equivalent communication primitives (such as a non-local gate or a teleportation protocol). Hypergraphs are made up of nodes and hyperedges that can either be quantum or classical (representing qubits or bits/cbits) and realised or potential . These types q/c and r/a enable the representation of any quantum of hybrid computation. When visualized they will be reprented by these symbols: Quantum operations may give rise to single-type operations, for instance a single qubit gate in the circuit model would give rise a quantum node and hyperedge, or two multi-type operations: the initialization of a qubit based on a previous measurement reading would lead to a classical node and a quantum node. The r/a types are useful for representing instructions that may \"potentially\" happen given previous results, such as IfElse operations found in dynamic circuits.","title":"A brief introduction to HDHs"},{"location":"hdh/#hybrid-dependency-hypergraphs","text":"HDHs are a special type of directed hypergraphs designed to encode temporal and spatial dependencies of quantum computations with the purpose of distributing originally monolothic quantum or hybrid workloads. Hypergraph abstractions are currently the standard approach for partitioning quantum computations in distributed quantum computing, [ 1 ]. HDHs extend this practice by providing a consistent and complete representation framework, intended to support the systematic evaluation and comparison of partitioning strategies. A hypergraph consists of a set of nodes and a family of subsets of these nodes, called hyperedges[ 2 ]. In HDHs these hyperedges can be refered to as connections, as they represent the connectivity requirements between nodes. Meaning that if a HDH is partitioned, the operations represented as inter-partition hyperedges would need to be replaced by equivalent communication primitives (such as a non-local gate or a teleportation protocol). Hypergraphs are made up of nodes and hyperedges that can either be quantum or classical (representing qubits or bits/cbits) and realised or potential . These types q/c and r/a enable the representation of any quantum of hybrid computation. When visualized they will be reprented by these symbols: Quantum operations may give rise to single-type operations, for instance a single qubit gate in the circuit model would give rise a quantum node and hyperedge, or two multi-type operations: the initialization of a qubit based on a previous measurement reading would lead to a classical node and a quantum node. The r/a types are useful for representing instructions that may \"potentially\" happen given previous results, such as IfElse operations found in dynamic circuits.","title":"Hybrid Dependency Hypergraphs"},{"location":"intro/","text":"An introduction to Distributed Quantum Computing The main problem behind distributed quantum computing is how to map quantum workloads to a network of quantum devices. Unlike its classical counterpart, the so-called mapping problem must account for both classical and quantum data and operations within the workload, as well as quantum and classical devices and connections within the network. Two main strategies have arisem in this context: Bespoke algorithm design: splitting the workload into smaller sub-workloads defined by algorithmic tasks (as in distributed versions of Shor\u2019s and Grover\u2019s algorithms) or by subproblems (something more along the lines of partitioning a large Hamiltonian into sub-Hamiltonians and communication terms [ 1 ]). Hypergraph abstractions: abstracting a quantum circuit into a hypergraph to then partition it with max-cut heuristics [ 2 ], replacing every cut hyperedge with an equivalent communication primitive (a non-local gate when partitioning through a gate, or a teleportation protocol when partitioning through a wire). This library contributes to the second strategy, which unlike bespoke algorithmic designs, does not require knowledge of the problem structure or algorithmic logic, and can be applied to any compiled quantum program. In this context the mapping problem can be defined as: Even in a simplified setting where only communication minimization is considered, the network mapping problem is computationally intractable. In particular, the decision version of the problem is NP-hard, even for a fixed network topology of only two nodes with equal capacity, where the objective reduces to partitioning V_C into two equal-sized sets while minimizing the number of hyperedges crossing between them. This case is equivalent to the sparsest cut problem with unit capacities, which is NP-hard via a reduction from the max-cut problem [ 3 ]. Our best strategy, therefore, is to design fast and efficient heuristic partitioners. State-of-the-art partitioning strategies include Fiduccia\u2013Mattheyses and multilevel partitioners (such as KaHyPar ). We recommend this review if you want to learn more about how these strategies have been applied to circuit partitioning. Nonetheless, a few bottlenecks remain, making cross-partitioning method comparison difficult. For a start, hypergraphs built for abstracting quantum workloads can be constructed using different conventions, for example, mapping qubits to nodes and multi-qubit gates to hyperedges, or vice versa. These are known as telegate and teledata. This variety of representations has led to inconsistencies that complicate direct comparisons between partitioning methods for distributed quantum circuits. To address this, strategies that can utilize both abstractions have been proposed [ 4 , 5 ], but not an easy-to-use unifying abstraction. Beyond telegate and teledata, restricting to the circuit model also imposes a significant limitation. Photons, the most practical medium for long-range quantum communication\u2014are used in photonic quantum computers that do not operate under the circuit model. As a result, current compiler distribution frameworks are incompatible with key hardware platforms needed for near-term distributed quantum computing testbeds. They exclude alternative computational models required by some quantum devices and cannot accommodate hybrid quantum\u2013classical workloads. HDHs address both challenges and provide a unifying abstraction for representing partitioning across all models. This library is designed to make HDHs easy to construct and to offer a database where state-of-the-art and future techniques can be benchmarked against each other in one common framework. For a concise overview of how HDHs are built, see our brief introduction to HDHs .","title":"An introduction to DQC"},{"location":"intro/#an-introduction-to-distributed-quantum-computing","text":"The main problem behind distributed quantum computing is how to map quantum workloads to a network of quantum devices. Unlike its classical counterpart, the so-called mapping problem must account for both classical and quantum data and operations within the workload, as well as quantum and classical devices and connections within the network. Two main strategies have arisem in this context: Bespoke algorithm design: splitting the workload into smaller sub-workloads defined by algorithmic tasks (as in distributed versions of Shor\u2019s and Grover\u2019s algorithms) or by subproblems (something more along the lines of partitioning a large Hamiltonian into sub-Hamiltonians and communication terms [ 1 ]). Hypergraph abstractions: abstracting a quantum circuit into a hypergraph to then partition it with max-cut heuristics [ 2 ], replacing every cut hyperedge with an equivalent communication primitive (a non-local gate when partitioning through a gate, or a teleportation protocol when partitioning through a wire). This library contributes to the second strategy, which unlike bespoke algorithmic designs, does not require knowledge of the problem structure or algorithmic logic, and can be applied to any compiled quantum program. In this context the mapping problem can be defined as: Even in a simplified setting where only communication minimization is considered, the network mapping problem is computationally intractable. In particular, the decision version of the problem is NP-hard, even for a fixed network topology of only two nodes with equal capacity, where the objective reduces to partitioning V_C into two equal-sized sets while minimizing the number of hyperedges crossing between them. This case is equivalent to the sparsest cut problem with unit capacities, which is NP-hard via a reduction from the max-cut problem [ 3 ]. Our best strategy, therefore, is to design fast and efficient heuristic partitioners. State-of-the-art partitioning strategies include Fiduccia\u2013Mattheyses and multilevel partitioners (such as KaHyPar ). We recommend this review if you want to learn more about how these strategies have been applied to circuit partitioning. Nonetheless, a few bottlenecks remain, making cross-partitioning method comparison difficult. For a start, hypergraphs built for abstracting quantum workloads can be constructed using different conventions, for example, mapping qubits to nodes and multi-qubit gates to hyperedges, or vice versa. These are known as telegate and teledata. This variety of representations has led to inconsistencies that complicate direct comparisons between partitioning methods for distributed quantum circuits. To address this, strategies that can utilize both abstractions have been proposed [ 4 , 5 ], but not an easy-to-use unifying abstraction. Beyond telegate and teledata, restricting to the circuit model also imposes a significant limitation. Photons, the most practical medium for long-range quantum communication\u2014are used in photonic quantum computers that do not operate under the circuit model. As a result, current compiler distribution frameworks are incompatible with key hardware platforms needed for near-term distributed quantum computing testbeds. They exclude alternative computational models required by some quantum devices and cannot accommodate hybrid quantum\u2013classical workloads. HDHs address both challenges and provide a unifying abstraction for representing partitioning across all models. This library is designed to make HDHs easy to construct and to offer a database where state-of-the-art and future techniques can be benchmarked against each other in one common framework. For a concise overview of how HDHs are built, see our brief introduction to HDHs .","title":"An introduction to Distributed Quantum Computing"},{"location":"literature/","text":"HDHs and associated projects have been presented at the following conferences : SIGCOMM25 POSTER: Distributed Quantum Computing Across Heterogeneous Hardware with Hybrid Dependency Hypergraphs Extended Abstract Poster IWQC25 : Hybrid Dependency Hypergraphs: A model agnostic IR for distributed quantum computing at the compilation level","title":"Related publications and information"},{"location":"models/","text":"Computational models Quantum computation can be performed with various computational models that have different instruction sets, such as quantum circuits (which perform computation through gates) or measurement based patterns (which perform computation through corrections). The diversity between these models can be attributed to hardware constraints as well as to the age of the field. Although they are all equivalent and computations can theoretically be translated accross them, this process is inneficient and hard to due to a lack of clear set of cross-model translators. HDHs were designed to abstract all the models into a unified framework. As such, they are model agnostic and can be constructed from any set of instructions. Specific model classes can be found under the hdh/models folder. To facilitate the usage of HDHs, the library has a set of embedded model mappings which translate instruction sets from popular quantum computational models into reusable HDH motifs. You can find details and examples of how to use these model classes and SDK converters bellow. Circuits The circuit model is among the most widely adopted paradigms of quantum computation, particularly in implementations on industrial quantum hardware (with the notable exception of photonic qubits). Quantum circuits are a universal model of computation. They form the foundation of many leading quantum software packages, including Qiskit , OpenQASM , Cirq , Amazon Braket SDK , and PennyLane ; which you can directly map to the librarys' Circuit class and then to HDHs (see examples of how to use these converters bellow). A quantum circuit is composed of a sequence of quantum gates applied to a set of qubits (commonly represented as horizontal wires). Gates, visualized as boxes placed on these wires, may act on one or multiple qubits. Single-qubit gates correspond to rotations of the qubit\u2019s state vector on the Bloch sphere. For example, a Z-rotation by angle \u03c0 rotates the state vector by \u03c0 radians about the z-axis. Multi-qubit gates, such as controlled-X (CX), act conditionally on one qubit\u2019s state and thereby create dependencies among qubits. For instance, a CX gate applies an X gate to the target qubit only if the control qubit is in the \u22231\u27e9 state. Such gates generate entanglement, as discussed in the introduction to DQC . Beyond static gates, circuits also support classically conditioned operations. For example, an IfElse construct applies one subcircuit if a specified classical register holds a given value and another subcircuit otherwise. This enables hybrid quantum\u2013classical flow control within circuits. Finally, measurement operations project qubits into classical bits, irreversibly collapsing their quantum state. The goal of HDHs is to make explicit the transformations induced by all gates and measurements, enabling large circuits to be partitioned into smaller, distributable subcircuits. Mapping a quantum circuit into an HDH involves applying the correspondences summarized in the table below: Bellow is an example of how to build a circuit using the library\u2019s Circuit class and map it to an HDH: import hdh from hdh.models.circuit import Circuit from hdh.visualize import plot_hdh circuit = Circuit() # Set of instructions circuit.add_instruction(\"ccx\", [0, 1, 2]) circuit.add_instruction(\"h\", [3]) circuit.add_instruction(\"h\", [5]) circuit.add_instruction(\"cx\", [3, 4]) circuit.add_instruction(\"cx\", [2, 1]) circuit.add_conditional_gate(5, 4, \"z\") circuit.add_instruction(\"cx\", [0, 3]) circuit.add_instruction(\"measure\", [2]) circuit.add_instruction(\"measure\", [4]) hdh = circuit.build_hdh() # Generate HDH fig = plot_hdh(hdh) # Visualize HDH This code is equivalent to the following circuit: Which is mapped to HDH: The HDH is composed of the motifs shown in the mapping table, appearing in the temporal order of computation and indexed by the qubits they abstract. Note that a qubit is not initialized until the timestep immediately preceding its first operation. MBQC patterns Measurement based quantum computing is one of the alternative universal models to quantum circuits [ 3 ]. It can be implemented on photonic quantum computers, as is often paired alongside fusion gates [ 4 ]. MBQC patterns consist of 4 types of operations [ 5 ]: N: Auxiliary state preparations E: Entaglement operations M: Measurements C: Corrections Intuitively, MBQC patterns can be thought of as large cluster states built from entangled nodes. The computation proceeds by measuring these nodes in sequence, where later measurement bases are adapted according to the outcomes of earlier ones. Measurement based patterns may look like this (taken from 6 ): . Here the N operations form the nodes, and E operations make the Edges. Time flows from left to right; and the angles (0, \u03c0/8) correspond to correction angles. Equivalences between circuit gates and MBQC patterns can be drawn (in this case squares = output qubits) [ 6 ]: But they are not necessarily the most efficient use of the model. Many MBQC native algorithms have been proposed [ 7 , 8 ], and the model is very compatible with various error correction techniques. If you'd like a more in depth tutorial on the model we recommend this pennylane tutorial on MBQC . Distribution of MBQC patterns can be confused with the partitioning of the substrate cluster state over which they are computed (the network you see). This is a valid method for partitioning the computation, but it is not the one that HDHs offer. As in the circuit model, HDHs abstract the operations performed during the computation (atop the cluster or during cluster generation) in a way that reflects not only their entanglement dependencies but also their temporal dependencies (aka the order in which they are performed). Mapping MBQC patterns to HDHs is relatively simple: import hdh from hdh.models.mbqc import MBQC from hdh.visualize import plot_hdh mbqc = MBQC() # Set of instructions mbqc.add_operation(\"N\", [], \"q0\") mbqc.add_operation(\"N\", [], \"q1\") mbqc.add_operation(\"E\", [\"q0\", \"q1\"], \"q1\") mbqc.add_operation(\"M\", [\"q0\"], \"c0\") mbqc.add_operation(\"C\", [\"c0\"], \"q2\") hdh = mbqc.build_hdh() # Generate HDH fig = plot_hdh(hdh) # Visualize HDH This code generates HDH: Unlike in the circuit model, MBQC pattern operations are not \"automatically\" assigned to the q_ and c_ naming conventions. This is because a node in an MBQC pattern doesn't necessarily directly correspond to a qubit. For simplicity we have made it so in the example above, but feel free to name them something else. Note that if you do the visualize function may play some tricks on you depending on the version of the library you are working with. Quantum walks Quantum walks are another universal model of quantum computation [ 9 ]. Like quantum circuits, quantum walks can be expressed using different sets of unitary operations and are universal for quantum computation. However, unlike circuits, quantum walks are not inherently defined in terms of gate sequences, they are abstract evolution models that can be implemented via gates or analog dynamics depending on the platform. They come in various forms, typically categorized as discrete-time or continuous-time variants [ 10 ]. In discrete-time walks (DTQW), evolution proceeds in steps using a coin operator and a shift operator; while continuous-time walks (CTQW) evolve directly under a Hamiltonian. In both cases, the key idea is that the walker\u2019s amplitude spreads across positions in superposition, creating interference patterns that can be harnessed for computation. This interference, guided by the chosen coin or Hamiltonian, allows quantum walks to implement algorithms and decision processes that outperform their classical counterparts. Unlike classical random walks, where probabilities govern independent step outcomes, quantum walks evolve coherently under unitary dynamics, so amplitudes rather than probabilities interfere. Perhaps a useful visualisation of this is to view quantum walks as waves spreading across all paths simultaneously, as per the image bellow (taken from [ 11 ]), where the two peaks of the amplitude distribution at the final row can be seen at the edges, in contrast to the central Gaussian-like spread (red) of the classical walk: The reason why the distributions are different is that in a classical walk, each step is an independent random choice. Adding them up gives a binomial distribution, which converges to a Gaussian around the center. In a quantum walk, the walker is in a superposition of paths. At each step, amplitudes for moving left or right interfere. Constructive interference pushes weight toward the outer edges, while destructive interference cancels amplitudes near the center. As a result, the probability distribution spreads ballistically (linearly with the number of steps) rather than diffusively (square root of steps), and peaks form at the farthest positions the walker can reach. You can map QW to HDHs as per: import hdh from hdh.models.qw import QW from hdh.visualize import plot_hdh qw = QW(type=discrete) # Set of instructions q0 = \"q0\" q1 = qw.add_coin(q0) q2 = qw.add_shift(q1) qw.add_measurement(q2, \"c0\") hdh = qw.build_hdh() # Generate HDH fig = plot_hdh(hdh) # Visualize HDH This code generates HDH: Bellow you can find a short explanation explaining the differences between DTQW and CTQW implementations. If you're interested in QW, we recommend this book . Discrete Time Quantum Walks A DTQW is made up of a set of timed operations imposed by its coin and walker. If we where to equate this to the NEMC format for MBQC, DTQW operations would be of two types: Coin: local unitaries acting on internal degrees of freedom at each position, e.g., internal \"spin\" or coin states, Shift: conditional displacements of walker states in position space based on the coin state. The outputs of each coin toss serve as inputs to the subsequent shift. A DTQW is \"valid\" if all operations can be sequentially ordered. Their mapping to HDHs is straightforward: inputs and outputs align naturally with state nodes, while coin and shift operations correspond to channel hyperedges. In practice, each coin operation introduces a local unitary hyperedge attached to the state node representing the walker\u2019s internal degree of freedom at a given time. Each shift operation then acts as a conditional hyperedge spanning the relevant position nodes, effectively redirecting walker states according to the coin outcome. Because HDHs are time-respecting, the sequential interplay of coin and shift steps is preserved explicitly, making the walk\u2019s evolution appear as an alternating pattern of local and conditional hyperedges across timesteps. Continuous Time Quantum Walks In contrast, CTQWs describe physical evolution over time under a Hamiltonian and do not, by themselves, constitute a universal computational model. They lack a well-defined, manipulable set of time-indexed operations. Nonetheless, CTQWs can still be mapped to HDH constructions by interpreting their continuous evolution as a resource-driven process. HDHs accommodate this by using dedicated predictive constructs to represent these operations. In other words they discretize the CTQW, and map it using the DTQW mapping, whilst setting each operation to the predicted type. This logic can be extended to other prominent quantum frameworks, such as adiabatic quantum computing, which likewise lack native representations of sequential logical operations but can still be encoded into HDHs by discretizing their dynamics and treating them as structured evolution patterns. Quantum cellular automata Finally, the last model currently compatible with the HDH library is QCA (Quantum cellular automaton). QCAs originally arose as an alternative paradigm for quantum computation, though more recently they have found application in understanding topological phases of matter and have been proposed as models of periodically driven (Floquet) quantum systems [ 12 ]. A QCA consists of a lattice of cells, each holding a finite-dimensional quantum state, whose evolution is defined by repeated application of local, translation-invariant rules. They are basically the equivalent of Cellular Automatons, which are beautifully described in Wolfram MathWorld as: a collection of \"colored\" cells on a grid of specified shape that evolves through a number of discrete time steps according to a set of rules based on the states of neighboring cells. In QCAs evolution ensure that information propagates only within a bounded neighborhood per time step, enforcing causality and making QCAs suitable for modeling distributed quantum dynamics. Similarly to quantum walks, their dynamics can be reduced to two core operations: Unitary updates: local reversible transformations that map a neighborhood $A$ to output cells $B$, preserving causality and unitarity, Swap or shift layers: structural steps to rearrange or relabel cell contents for staggered updates. You can think of QCA operation evolution as a slow conquer of a substrate lattice: They can be mapped to HDHs through: import hdh from hdh.models.qca import QCA from hdh.visualize import plot_hdh # Topology of lattice over which QCA will evolve topology = { \"q0\": [\"q1\", \"q2\"], \"q1\": [\"q0\"], \"q2\": [\"q0\"] } measurements = {\"q1\", \"q2\"} qca = QCA(topology=topology, measurements=measurements, steps=3) hdh = qca.build_hdh() # Generate HDH fig = plot_hdh(hdh) # Visualize HDH This code generates HDH: Built in Converters The library currently has converters from Qiskit , OpenQASM , Cirq , Amazon Braket SDK , and PennyLane . These converters take in computations written in these languages/SDKs and transform them into relevant HDHs. They can be used as follows: import hdh import qiskit from qiskit import QuantumCircuit from hdh.converters.qiskit import from_qiskit from hdh.visualize import plot_hdh from hdh.passes.cut import compute_cut, cost, partition_sizes, compute_parallelism_by_time # Qiskit circuit qc = QuantumCircuit(3) qc.h(0) qc.cx(0, 1) qc.ccx(1, 2, 0) qc.measure_all() hdh_graph = from_qiskit(qc) # Generate HDH fig = plot_hdh(hdh) # Visualize HDH Make your own instruction set Beyond the given models and converters, you can also make your own model class by defining the desired HDH motif construction. A good rule of thumb is to consider inputs and outputs of operations as nodes and the operations themselves as hyperedges. This is how the Circuit class looks like: from typing import List, Tuple, Optional, Set, Dict, Literal import sys import os sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))) from hdh.hdh import HDH class Circuit: def __init__(self): self.instructions: List[ Tuple[str, List[int], List[int], List[bool], Literal[\"a\", \"p\"]] ] = [] # (name, qubits, bits, modifies_flags, cond_flag) def add_instruction( self, name: str, qubits: List[int], bits: Optional[List[int]] = None, modifies_flags: Optional[List[bool]] = None, cond_flag: Literal[\"a\", \"p\"] = \"a\" ): name = name.lower() if name == \"measure\": modifies_flags = [True] * len(qubits) else: bits = bits or [] modifies_flags = modifies_flags or [True] * len(qubits) self.instructions.append((name, qubits, bits, modifies_flags, cond_flag)) def build_hdh(self, hdh_cls=HDH) -> HDH: hdh = hdh_cls() qubit_time: Dict[int, int] = {} bit_time: Dict[int, int] = {} last_gate_input_time: Dict[int, int] = {} for name, qargs, cargs, modifies_flags, cond_flag in self.instructions: # --- Canonicalize inputs --- qargs = list(qargs or []) if name == \"measure\": cargs = list(cargs) if cargs is not None else qargs.copy() # 1:1 map if len(cargs) != len(qargs): raise ValueError(\"measure: len(bits) must equal len(qubits)\") modifies_flags = [True] * len(qargs) else: cargs = list(cargs or []) if modifies_flags is None: modifies_flags = [True] * len(qargs) elif len(modifies_flags) != len(qargs): raise ValueError(\"len(modifies_flags) must equal len(qubits)\") #print(f\"\\n=== Adding instruction: {name} on qubits {qargs} ===\") # for q in qargs: #print(f\" [before] qubit_time[{q}] = {qubit_time.get(q)}\") # Measurements if name == \"measure\": for i, qubit in enumerate(qargs): # Use current qubit time (default 0), do NOT advance it here t_in = qubit_time.get(qubit, 0) q_in = f\"q{qubit}_t{t_in}\" hdh.add_node(q_in, \"q\", t_in, node_real=cond_flag) bit = cargs[i] t_out = t_in + 1 # classical result at next tick c_out = f\"c{bit}_t{t_out}\" hdh.add_node(c_out, \"c\", t_out, node_real=cond_flag) hdh.add_hyperedge({q_in, c_out}, \"c\", name=\"measure\", node_real=cond_flag) # Next-free convention for this bit stream bit_time[bit] = t_out + 1 # Important: do NOT set qubit_time[qubit] = t_in + k # The quantum wire collapses; keep its last quantum tick unchanged. continue # Conditional gate handling if name != \"measure\" and cond_flag == \"p\" and cargs: # Supports 1 classical control; extend to many if you like ctrl = cargs[0] # Ensure times exist for q in qargs: if q not in qubit_time: qubit_time[q] = max(qubit_time.values(), default=0) last_gate_input_time[q] = qubit_time[q] # Classical node must already exist (e.g., produced by a prior measure) # bit_time points to \"next free\" slot; the latest existing node is at t = bit_time-1 c_latest = bit_time.get(ctrl, 1) - 1 cnode = f\"c{ctrl}_t{c_latest}\" hdh.add_node(cnode, \"c\", c_latest, node_real=cond_flag) edges = [] for tq in qargs: # gate happens at next tick after both inputs are ready t_in_q = qubit_time[tq] t_gate = max(t_in_q, c_latest) + 1 qname = f\"q{tq}\" qout = f\"{qname}_t{t_gate}\" # ensure the quantum output node exists at gate time hdh.add_node(qout, \"q\", t_gate, node_real=cond_flag) # add classical hyperedge feeding the quantum node e = hdh.add_hyperedge({cnode, qout}, \"c\", name=name, node_real=cond_flag) edges.append(e) # advance quantum time last_gate_input_time[tq] = t_in_q qubit_time[tq] = t_gate # store edge_args for reconstruction/debug q_with_time = [(q, qubit_time[q]) for q in qargs] c_with_time = [(ctrl, c_latest + 1)] # next-free convention; adjust if you track exact for e in edges: hdh.edge_args[e] = (q_with_time, c_with_time, modifies_flags or [True] * len(qargs)) continue #Actualized gate (non-conditional) for q in qargs: if q not in qubit_time: qubit_time[q] = max(qubit_time.values(), default=0) last_gate_input_time[q] = qubit_time[q] # initial input time active_times = [qubit_time[q] for q in qargs] time_step = max(active_times) + 1 if active_times else 0 in_nodes: List[str] = [] out_nodes: List[str] = [] intermediate_nodes: List[str] = [] final_nodes: List[str] = [] post_nodes: List[str] = [] #DEBUG #print(\" [after]\", {q: qubit_time[q] for q in qargs}) #print(\" [after]\", {q: qubit_time[q] for q in qargs}) multi_gate = (name != \"measure\" and len(qargs) > 1) common_start = max((qubit_time.get(q, 0) for q in qargs), default=0) if multi_gate else None for i, qubit in enumerate(qargs): t_in = qubit_time[qubit] qname = f\"q{qubit}\" in_id = f\"{qname}_t{t_in}\" hdh.add_node(in_id, \"q\", t_in, node_real=cond_flag) #print(f\" [+] Node added: {in_id} (type q, time {t_in})\") #print(f\" [+] Node added: {in_id} (type q, time {t_in})\") in_nodes.append(in_id) #print(f\" [qubit {qubit}] modifies_flag = {modifies_flags[i]}\") #print(f\" [qubit {qubit}] modifies_flag = {modifies_flags[i]}\") # choose timeline if multi_gate: t1 = common_start + 1 t2 = common_start + 2 t3 = common_start + 3 else: t1 = t_in + 1 t2 = t1 + 1 t3 = t2 + 1 # create mid/final/post nodes for BOTH cases mid_id = f\"{qname}_t{t1}\" final_id = f\"{qname}_t{t2}\" post_id = f\"{qname}_t{t3}\" hdh.add_node(mid_id, \"q\", t1, node_real=cond_flag) hdh.add_node(final_id, \"q\", t2, node_real=cond_flag) hdh.add_node(post_id, \"q\", t3, node_real=cond_flag) intermediate_nodes.append(mid_id) final_nodes.append(final_id) post_nodes.append(post_id) last_gate_input_time[qubit] = t_in qubit_time[qubit] = t3 edges = [] if len(qargs) > 1: # Stage 1: input \u2192 intermediate (1:1) for in_node, mid_node in zip(in_nodes, intermediate_nodes): e = hdh.add_hyperedge({in_node, mid_node}, \"q\", name=f\"{name}_stage1\", node_real=cond_flag) #print(f\" [~] stage1 {in_node} \u2192 {mid_node}\") #print(f\" [~] stage1 {in_node} \u2192 {mid_node}\") edges.append(e) # Stage 2: full multiqubit edge from intermediate \u2192 final e2 = hdh.add_hyperedge(set(intermediate_nodes) | set(final_nodes), \"q\", name=f\"{name}_stage2\", node_real=cond_flag) #print(f\" [~] stage2 |intermediate|={len(intermediate_nodes)} \u2192 |final|={len(final_nodes)}\") #print(f\" [~] stage2 |intermediate|={len(intermediate_nodes)} \u2192 |final|={len(final_nodes)}\") edges.append(e2) # Stage 3: final \u2192 post (1:1) for f_node, p_node in zip(final_nodes, post_nodes): e = hdh.add_hyperedge({f_node, p_node}, \"q\", name=f\"{name}_stage3\", node_real=cond_flag) #print(f\" [~] stage3 {f_node} \u2192 {p_node}\") #print(f\" [~] stage3 {f_node} \u2192 {p_node}\") edges.append(e) if name == \"measure\": for i, qubit in enumerate(qargs): t_in = qubit_time.get(qubit, 0) q_in = f\"q{qubit}_t{t_in}\" hdh.add_node(q_in, \"q\", t_in, node_real=cond_flag) bit = cargs[i] t_out = t_in + 1 c_out = f\"c{bit}_t{t_out}\" hdh.add_node(c_out, \"c\", t_out, node_real=cond_flag) hdh.add_hyperedge({q_in, c_out}, \"c\", name=\"measure\", node_real=cond_flag) bit_time[bit] = t_out + 1 continue if name != \"measure\": for bit in cargs: t = bit_time.get(bit, 0) cname = f\"c{bit}\" out_id = f\"{cname}_t{t + 1}\" hdh.add_node(out_id, \"c\", t + 1, node_real=cond_flag) out_nodes.append(out_id) bit_time[bit] = t + 1 all_nodes = set(in_nodes) | set(out_nodes) if all(n.startswith(\"c\") for n in all_nodes): edge_type = \"c\" elif any(n.startswith(\"c\") for n in all_nodes): edge_type = \"c\" else: edge_type = \"q\" edges = [] if len(qargs) > 1: # Multi-qubit gate # Stage 1: input \u2192 intermediate (1:1) for in_node, mid_node in zip(in_nodes, intermediate_nodes): edge = hdh.add_hyperedge({in_node, mid_node}, \"q\", name=f\"{name}_stage1\", node_real=cond_flag) # DEBUG #print(f\" [~] Hyperedge added over: {in_node} \u2192 {mid_node}, label: {name}_stage1\") #print(f\" [~] Hyperedge added over: {in_node} \u2192 {mid_node}, label: {name}_stage1\") edges.append(edge) # Stage 2: full multiqubit edge from intermediate \u2192 final edge2 = hdh.add_hyperedge(set(intermediate_nodes) | set(final_nodes), \"q\", name=f\"{name}_stage2\", node_real=cond_flag) # DEBUG #print(f\" [~] Hyperedge added over: {in_node} \u2192 {mid_node}, label: {name}_stage2\") #print(f\" [~] Hyperedge added over: {in_node} \u2192 {mid_node}, label: {name}_stage2\") edges.append(edge2) # Stage 3: final \u2192 post (1:1 again) for final_node, post_node in zip(final_nodes, post_nodes): edge = hdh.add_hyperedge({final_node, post_node}, \"q\", name=f\"{name}_stage3\", node_real=cond_flag) # DEBUG #print(f\" [~] Hyperedge added over: {in_node} \u2192 {mid_node}, label: {name}_stage1\") #print(f\" [~] Hyperedge added over: {in_node} \u2192 {mid_node}, label: {name}_stage1\") edges.append(edge) else: # Single-qubit gate for i, qubit in enumerate(qargs): if modifies_flags[i] and name != \"measure\": t_in = last_gate_input_time[qubit] t_out = t_in + 1 qname = f\"q{qubit}\" in_id = f\"{qname}_t{t_in}\" out_id = f\"{qname}_t{t_out}\" # DEBUG #print(f\"[{name}] Q{qubit} t_in = {t_in}, expected from qubit_time = {qubit_time[qubit]}\") #print(f\"[{name}] Q{qubit} t_in = {t_in}, expected from qubit_time = {qubit_time[qubit]}\") hdh.add_node(out_id, \"q\", t_out, node_real=cond_flag) # DEBUG #print(f\" [+] Node added: {in_id} (type q, time {t_in})\") #print(f\" [+] Node added: {in_id} (type q, time {t_in})\") edge = hdh.add_hyperedge({in_id, out_id}, \"q\", name=name, node_real=cond_flag) # DEBUG #print(f\" [~] Hyperedge added over: {in_id} \u2192 {out_id}, label: {name}_stage1\") #print(f\" [~] Hyperedge added over: {in_id} \u2192 {out_id}, label: {name}_stage1\") edges.append(edge) # Update time qubit_time[qubit] = t_out last_gate_input_time[qubit] = t_in q_with_time = [(q, qubit_time[q]) for q in qargs] c_with_time = [(c, bit_time.get(c, 0)) for c in cargs] for edge in edges: hdh.edge_args[edge] = (q_with_time, c_with_time, modifies_flags) return hdh","title":"Quantum Computational Model mappings to HDHs"},{"location":"models/#computational-models","text":"Quantum computation can be performed with various computational models that have different instruction sets, such as quantum circuits (which perform computation through gates) or measurement based patterns (which perform computation through corrections). The diversity between these models can be attributed to hardware constraints as well as to the age of the field. Although they are all equivalent and computations can theoretically be translated accross them, this process is inneficient and hard to due to a lack of clear set of cross-model translators. HDHs were designed to abstract all the models into a unified framework. As such, they are model agnostic and can be constructed from any set of instructions. Specific model classes can be found under the hdh/models folder. To facilitate the usage of HDHs, the library has a set of embedded model mappings which translate instruction sets from popular quantum computational models into reusable HDH motifs. You can find details and examples of how to use these model classes and SDK converters bellow.","title":"Computational models"},{"location":"models/#circuits","text":"The circuit model is among the most widely adopted paradigms of quantum computation, particularly in implementations on industrial quantum hardware (with the notable exception of photonic qubits). Quantum circuits are a universal model of computation. They form the foundation of many leading quantum software packages, including Qiskit , OpenQASM , Cirq , Amazon Braket SDK , and PennyLane ; which you can directly map to the librarys' Circuit class and then to HDHs (see examples of how to use these converters bellow). A quantum circuit is composed of a sequence of quantum gates applied to a set of qubits (commonly represented as horizontal wires). Gates, visualized as boxes placed on these wires, may act on one or multiple qubits. Single-qubit gates correspond to rotations of the qubit\u2019s state vector on the Bloch sphere. For example, a Z-rotation by angle \u03c0 rotates the state vector by \u03c0 radians about the z-axis. Multi-qubit gates, such as controlled-X (CX), act conditionally on one qubit\u2019s state and thereby create dependencies among qubits. For instance, a CX gate applies an X gate to the target qubit only if the control qubit is in the \u22231\u27e9 state. Such gates generate entanglement, as discussed in the introduction to DQC . Beyond static gates, circuits also support classically conditioned operations. For example, an IfElse construct applies one subcircuit if a specified classical register holds a given value and another subcircuit otherwise. This enables hybrid quantum\u2013classical flow control within circuits. Finally, measurement operations project qubits into classical bits, irreversibly collapsing their quantum state. The goal of HDHs is to make explicit the transformations induced by all gates and measurements, enabling large circuits to be partitioned into smaller, distributable subcircuits. Mapping a quantum circuit into an HDH involves applying the correspondences summarized in the table below: Bellow is an example of how to build a circuit using the library\u2019s Circuit class and map it to an HDH: import hdh from hdh.models.circuit import Circuit from hdh.visualize import plot_hdh circuit = Circuit() # Set of instructions circuit.add_instruction(\"ccx\", [0, 1, 2]) circuit.add_instruction(\"h\", [3]) circuit.add_instruction(\"h\", [5]) circuit.add_instruction(\"cx\", [3, 4]) circuit.add_instruction(\"cx\", [2, 1]) circuit.add_conditional_gate(5, 4, \"z\") circuit.add_instruction(\"cx\", [0, 3]) circuit.add_instruction(\"measure\", [2]) circuit.add_instruction(\"measure\", [4]) hdh = circuit.build_hdh() # Generate HDH fig = plot_hdh(hdh) # Visualize HDH This code is equivalent to the following circuit: Which is mapped to HDH: The HDH is composed of the motifs shown in the mapping table, appearing in the temporal order of computation and indexed by the qubits they abstract. Note that a qubit is not initialized until the timestep immediately preceding its first operation.","title":"Circuits"},{"location":"models/#mbqc-patterns","text":"Measurement based quantum computing is one of the alternative universal models to quantum circuits [ 3 ]. It can be implemented on photonic quantum computers, as is often paired alongside fusion gates [ 4 ]. MBQC patterns consist of 4 types of operations [ 5 ]: N: Auxiliary state preparations E: Entaglement operations M: Measurements C: Corrections Intuitively, MBQC patterns can be thought of as large cluster states built from entangled nodes. The computation proceeds by measuring these nodes in sequence, where later measurement bases are adapted according to the outcomes of earlier ones. Measurement based patterns may look like this (taken from 6 ): . Here the N operations form the nodes, and E operations make the Edges. Time flows from left to right; and the angles (0, \u03c0/8) correspond to correction angles. Equivalences between circuit gates and MBQC patterns can be drawn (in this case squares = output qubits) [ 6 ]: But they are not necessarily the most efficient use of the model. Many MBQC native algorithms have been proposed [ 7 , 8 ], and the model is very compatible with various error correction techniques. If you'd like a more in depth tutorial on the model we recommend this pennylane tutorial on MBQC . Distribution of MBQC patterns can be confused with the partitioning of the substrate cluster state over which they are computed (the network you see). This is a valid method for partitioning the computation, but it is not the one that HDHs offer. As in the circuit model, HDHs abstract the operations performed during the computation (atop the cluster or during cluster generation) in a way that reflects not only their entanglement dependencies but also their temporal dependencies (aka the order in which they are performed). Mapping MBQC patterns to HDHs is relatively simple: import hdh from hdh.models.mbqc import MBQC from hdh.visualize import plot_hdh mbqc = MBQC() # Set of instructions mbqc.add_operation(\"N\", [], \"q0\") mbqc.add_operation(\"N\", [], \"q1\") mbqc.add_operation(\"E\", [\"q0\", \"q1\"], \"q1\") mbqc.add_operation(\"M\", [\"q0\"], \"c0\") mbqc.add_operation(\"C\", [\"c0\"], \"q2\") hdh = mbqc.build_hdh() # Generate HDH fig = plot_hdh(hdh) # Visualize HDH This code generates HDH: Unlike in the circuit model, MBQC pattern operations are not \"automatically\" assigned to the q_ and c_ naming conventions. This is because a node in an MBQC pattern doesn't necessarily directly correspond to a qubit. For simplicity we have made it so in the example above, but feel free to name them something else. Note that if you do the visualize function may play some tricks on you depending on the version of the library you are working with.","title":"MBQC patterns"},{"location":"models/#quantum-walks","text":"Quantum walks are another universal model of quantum computation [ 9 ]. Like quantum circuits, quantum walks can be expressed using different sets of unitary operations and are universal for quantum computation. However, unlike circuits, quantum walks are not inherently defined in terms of gate sequences, they are abstract evolution models that can be implemented via gates or analog dynamics depending on the platform. They come in various forms, typically categorized as discrete-time or continuous-time variants [ 10 ]. In discrete-time walks (DTQW), evolution proceeds in steps using a coin operator and a shift operator; while continuous-time walks (CTQW) evolve directly under a Hamiltonian. In both cases, the key idea is that the walker\u2019s amplitude spreads across positions in superposition, creating interference patterns that can be harnessed for computation. This interference, guided by the chosen coin or Hamiltonian, allows quantum walks to implement algorithms and decision processes that outperform their classical counterparts. Unlike classical random walks, where probabilities govern independent step outcomes, quantum walks evolve coherently under unitary dynamics, so amplitudes rather than probabilities interfere. Perhaps a useful visualisation of this is to view quantum walks as waves spreading across all paths simultaneously, as per the image bellow (taken from [ 11 ]), where the two peaks of the amplitude distribution at the final row can be seen at the edges, in contrast to the central Gaussian-like spread (red) of the classical walk: The reason why the distributions are different is that in a classical walk, each step is an independent random choice. Adding them up gives a binomial distribution, which converges to a Gaussian around the center. In a quantum walk, the walker is in a superposition of paths. At each step, amplitudes for moving left or right interfere. Constructive interference pushes weight toward the outer edges, while destructive interference cancels amplitudes near the center. As a result, the probability distribution spreads ballistically (linearly with the number of steps) rather than diffusively (square root of steps), and peaks form at the farthest positions the walker can reach. You can map QW to HDHs as per: import hdh from hdh.models.qw import QW from hdh.visualize import plot_hdh qw = QW(type=discrete) # Set of instructions q0 = \"q0\" q1 = qw.add_coin(q0) q2 = qw.add_shift(q1) qw.add_measurement(q2, \"c0\") hdh = qw.build_hdh() # Generate HDH fig = plot_hdh(hdh) # Visualize HDH This code generates HDH: Bellow you can find a short explanation explaining the differences between DTQW and CTQW implementations. If you're interested in QW, we recommend this book .","title":"Quantum walks"},{"location":"models/#discrete-time-quantum-walks","text":"A DTQW is made up of a set of timed operations imposed by its coin and walker. If we where to equate this to the NEMC format for MBQC, DTQW operations would be of two types: Coin: local unitaries acting on internal degrees of freedom at each position, e.g., internal \"spin\" or coin states, Shift: conditional displacements of walker states in position space based on the coin state. The outputs of each coin toss serve as inputs to the subsequent shift. A DTQW is \"valid\" if all operations can be sequentially ordered. Their mapping to HDHs is straightforward: inputs and outputs align naturally with state nodes, while coin and shift operations correspond to channel hyperedges. In practice, each coin operation introduces a local unitary hyperedge attached to the state node representing the walker\u2019s internal degree of freedom at a given time. Each shift operation then acts as a conditional hyperedge spanning the relevant position nodes, effectively redirecting walker states according to the coin outcome. Because HDHs are time-respecting, the sequential interplay of coin and shift steps is preserved explicitly, making the walk\u2019s evolution appear as an alternating pattern of local and conditional hyperedges across timesteps.","title":"Discrete Time Quantum Walks"},{"location":"models/#continuous-time-quantum-walks","text":"In contrast, CTQWs describe physical evolution over time under a Hamiltonian and do not, by themselves, constitute a universal computational model. They lack a well-defined, manipulable set of time-indexed operations. Nonetheless, CTQWs can still be mapped to HDH constructions by interpreting their continuous evolution as a resource-driven process. HDHs accommodate this by using dedicated predictive constructs to represent these operations. In other words they discretize the CTQW, and map it using the DTQW mapping, whilst setting each operation to the predicted type. This logic can be extended to other prominent quantum frameworks, such as adiabatic quantum computing, which likewise lack native representations of sequential logical operations but can still be encoded into HDHs by discretizing their dynamics and treating them as structured evolution patterns.","title":"Continuous Time Quantum Walks"},{"location":"models/#quantum-cellular-automata","text":"Finally, the last model currently compatible with the HDH library is QCA (Quantum cellular automaton). QCAs originally arose as an alternative paradigm for quantum computation, though more recently they have found application in understanding topological phases of matter and have been proposed as models of periodically driven (Floquet) quantum systems [ 12 ]. A QCA consists of a lattice of cells, each holding a finite-dimensional quantum state, whose evolution is defined by repeated application of local, translation-invariant rules. They are basically the equivalent of Cellular Automatons, which are beautifully described in Wolfram MathWorld as: a collection of \"colored\" cells on a grid of specified shape that evolves through a number of discrete time steps according to a set of rules based on the states of neighboring cells. In QCAs evolution ensure that information propagates only within a bounded neighborhood per time step, enforcing causality and making QCAs suitable for modeling distributed quantum dynamics. Similarly to quantum walks, their dynamics can be reduced to two core operations: Unitary updates: local reversible transformations that map a neighborhood $A$ to output cells $B$, preserving causality and unitarity, Swap or shift layers: structural steps to rearrange or relabel cell contents for staggered updates. You can think of QCA operation evolution as a slow conquer of a substrate lattice: They can be mapped to HDHs through: import hdh from hdh.models.qca import QCA from hdh.visualize import plot_hdh # Topology of lattice over which QCA will evolve topology = { \"q0\": [\"q1\", \"q2\"], \"q1\": [\"q0\"], \"q2\": [\"q0\"] } measurements = {\"q1\", \"q2\"} qca = QCA(topology=topology, measurements=measurements, steps=3) hdh = qca.build_hdh() # Generate HDH fig = plot_hdh(hdh) # Visualize HDH This code generates HDH:","title":"Quantum cellular automata"},{"location":"models/#built-in-converters","text":"The library currently has converters from Qiskit , OpenQASM , Cirq , Amazon Braket SDK , and PennyLane . These converters take in computations written in these languages/SDKs and transform them into relevant HDHs. They can be used as follows: import hdh import qiskit from qiskit import QuantumCircuit from hdh.converters.qiskit import from_qiskit from hdh.visualize import plot_hdh from hdh.passes.cut import compute_cut, cost, partition_sizes, compute_parallelism_by_time # Qiskit circuit qc = QuantumCircuit(3) qc.h(0) qc.cx(0, 1) qc.ccx(1, 2, 0) qc.measure_all() hdh_graph = from_qiskit(qc) # Generate HDH fig = plot_hdh(hdh) # Visualize HDH","title":"Built in Converters"},{"location":"models/#make-your-own-instruction-set","text":"Beyond the given models and converters, you can also make your own model class by defining the desired HDH motif construction. A good rule of thumb is to consider inputs and outputs of operations as nodes and the operations themselves as hyperedges. This is how the Circuit class looks like: from typing import List, Tuple, Optional, Set, Dict, Literal import sys import os sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))) from hdh.hdh import HDH class Circuit: def __init__(self): self.instructions: List[ Tuple[str, List[int], List[int], List[bool], Literal[\"a\", \"p\"]] ] = [] # (name, qubits, bits, modifies_flags, cond_flag) def add_instruction( self, name: str, qubits: List[int], bits: Optional[List[int]] = None, modifies_flags: Optional[List[bool]] = None, cond_flag: Literal[\"a\", \"p\"] = \"a\" ): name = name.lower() if name == \"measure\": modifies_flags = [True] * len(qubits) else: bits = bits or [] modifies_flags = modifies_flags or [True] * len(qubits) self.instructions.append((name, qubits, bits, modifies_flags, cond_flag)) def build_hdh(self, hdh_cls=HDH) -> HDH: hdh = hdh_cls() qubit_time: Dict[int, int] = {} bit_time: Dict[int, int] = {} last_gate_input_time: Dict[int, int] = {} for name, qargs, cargs, modifies_flags, cond_flag in self.instructions: # --- Canonicalize inputs --- qargs = list(qargs or []) if name == \"measure\": cargs = list(cargs) if cargs is not None else qargs.copy() # 1:1 map if len(cargs) != len(qargs): raise ValueError(\"measure: len(bits) must equal len(qubits)\") modifies_flags = [True] * len(qargs) else: cargs = list(cargs or []) if modifies_flags is None: modifies_flags = [True] * len(qargs) elif len(modifies_flags) != len(qargs): raise ValueError(\"len(modifies_flags) must equal len(qubits)\") #print(f\"\\n=== Adding instruction: {name} on qubits {qargs} ===\") # for q in qargs: #print(f\" [before] qubit_time[{q}] = {qubit_time.get(q)}\") # Measurements if name == \"measure\": for i, qubit in enumerate(qargs): # Use current qubit time (default 0), do NOT advance it here t_in = qubit_time.get(qubit, 0) q_in = f\"q{qubit}_t{t_in}\" hdh.add_node(q_in, \"q\", t_in, node_real=cond_flag) bit = cargs[i] t_out = t_in + 1 # classical result at next tick c_out = f\"c{bit}_t{t_out}\" hdh.add_node(c_out, \"c\", t_out, node_real=cond_flag) hdh.add_hyperedge({q_in, c_out}, \"c\", name=\"measure\", node_real=cond_flag) # Next-free convention for this bit stream bit_time[bit] = t_out + 1 # Important: do NOT set qubit_time[qubit] = t_in + k # The quantum wire collapses; keep its last quantum tick unchanged. continue # Conditional gate handling if name != \"measure\" and cond_flag == \"p\" and cargs: # Supports 1 classical control; extend to many if you like ctrl = cargs[0] # Ensure times exist for q in qargs: if q not in qubit_time: qubit_time[q] = max(qubit_time.values(), default=0) last_gate_input_time[q] = qubit_time[q] # Classical node must already exist (e.g., produced by a prior measure) # bit_time points to \"next free\" slot; the latest existing node is at t = bit_time-1 c_latest = bit_time.get(ctrl, 1) - 1 cnode = f\"c{ctrl}_t{c_latest}\" hdh.add_node(cnode, \"c\", c_latest, node_real=cond_flag) edges = [] for tq in qargs: # gate happens at next tick after both inputs are ready t_in_q = qubit_time[tq] t_gate = max(t_in_q, c_latest) + 1 qname = f\"q{tq}\" qout = f\"{qname}_t{t_gate}\" # ensure the quantum output node exists at gate time hdh.add_node(qout, \"q\", t_gate, node_real=cond_flag) # add classical hyperedge feeding the quantum node e = hdh.add_hyperedge({cnode, qout}, \"c\", name=name, node_real=cond_flag) edges.append(e) # advance quantum time last_gate_input_time[tq] = t_in_q qubit_time[tq] = t_gate # store edge_args for reconstruction/debug q_with_time = [(q, qubit_time[q]) for q in qargs] c_with_time = [(ctrl, c_latest + 1)] # next-free convention; adjust if you track exact for e in edges: hdh.edge_args[e] = (q_with_time, c_with_time, modifies_flags or [True] * len(qargs)) continue #Actualized gate (non-conditional) for q in qargs: if q not in qubit_time: qubit_time[q] = max(qubit_time.values(), default=0) last_gate_input_time[q] = qubit_time[q] # initial input time active_times = [qubit_time[q] for q in qargs] time_step = max(active_times) + 1 if active_times else 0 in_nodes: List[str] = [] out_nodes: List[str] = [] intermediate_nodes: List[str] = [] final_nodes: List[str] = [] post_nodes: List[str] = [] #DEBUG #print(\" [after]\", {q: qubit_time[q] for q in qargs}) #print(\" [after]\", {q: qubit_time[q] for q in qargs}) multi_gate = (name != \"measure\" and len(qargs) > 1) common_start = max((qubit_time.get(q, 0) for q in qargs), default=0) if multi_gate else None for i, qubit in enumerate(qargs): t_in = qubit_time[qubit] qname = f\"q{qubit}\" in_id = f\"{qname}_t{t_in}\" hdh.add_node(in_id, \"q\", t_in, node_real=cond_flag) #print(f\" [+] Node added: {in_id} (type q, time {t_in})\") #print(f\" [+] Node added: {in_id} (type q, time {t_in})\") in_nodes.append(in_id) #print(f\" [qubit {qubit}] modifies_flag = {modifies_flags[i]}\") #print(f\" [qubit {qubit}] modifies_flag = {modifies_flags[i]}\") # choose timeline if multi_gate: t1 = common_start + 1 t2 = common_start + 2 t3 = common_start + 3 else: t1 = t_in + 1 t2 = t1 + 1 t3 = t2 + 1 # create mid/final/post nodes for BOTH cases mid_id = f\"{qname}_t{t1}\" final_id = f\"{qname}_t{t2}\" post_id = f\"{qname}_t{t3}\" hdh.add_node(mid_id, \"q\", t1, node_real=cond_flag) hdh.add_node(final_id, \"q\", t2, node_real=cond_flag) hdh.add_node(post_id, \"q\", t3, node_real=cond_flag) intermediate_nodes.append(mid_id) final_nodes.append(final_id) post_nodes.append(post_id) last_gate_input_time[qubit] = t_in qubit_time[qubit] = t3 edges = [] if len(qargs) > 1: # Stage 1: input \u2192 intermediate (1:1) for in_node, mid_node in zip(in_nodes, intermediate_nodes): e = hdh.add_hyperedge({in_node, mid_node}, \"q\", name=f\"{name}_stage1\", node_real=cond_flag) #print(f\" [~] stage1 {in_node} \u2192 {mid_node}\") #print(f\" [~] stage1 {in_node} \u2192 {mid_node}\") edges.append(e) # Stage 2: full multiqubit edge from intermediate \u2192 final e2 = hdh.add_hyperedge(set(intermediate_nodes) | set(final_nodes), \"q\", name=f\"{name}_stage2\", node_real=cond_flag) #print(f\" [~] stage2 |intermediate|={len(intermediate_nodes)} \u2192 |final|={len(final_nodes)}\") #print(f\" [~] stage2 |intermediate|={len(intermediate_nodes)} \u2192 |final|={len(final_nodes)}\") edges.append(e2) # Stage 3: final \u2192 post (1:1) for f_node, p_node in zip(final_nodes, post_nodes): e = hdh.add_hyperedge({f_node, p_node}, \"q\", name=f\"{name}_stage3\", node_real=cond_flag) #print(f\" [~] stage3 {f_node} \u2192 {p_node}\") #print(f\" [~] stage3 {f_node} \u2192 {p_node}\") edges.append(e) if name == \"measure\": for i, qubit in enumerate(qargs): t_in = qubit_time.get(qubit, 0) q_in = f\"q{qubit}_t{t_in}\" hdh.add_node(q_in, \"q\", t_in, node_real=cond_flag) bit = cargs[i] t_out = t_in + 1 c_out = f\"c{bit}_t{t_out}\" hdh.add_node(c_out, \"c\", t_out, node_real=cond_flag) hdh.add_hyperedge({q_in, c_out}, \"c\", name=\"measure\", node_real=cond_flag) bit_time[bit] = t_out + 1 continue if name != \"measure\": for bit in cargs: t = bit_time.get(bit, 0) cname = f\"c{bit}\" out_id = f\"{cname}_t{t + 1}\" hdh.add_node(out_id, \"c\", t + 1, node_real=cond_flag) out_nodes.append(out_id) bit_time[bit] = t + 1 all_nodes = set(in_nodes) | set(out_nodes) if all(n.startswith(\"c\") for n in all_nodes): edge_type = \"c\" elif any(n.startswith(\"c\") for n in all_nodes): edge_type = \"c\" else: edge_type = \"q\" edges = [] if len(qargs) > 1: # Multi-qubit gate # Stage 1: input \u2192 intermediate (1:1) for in_node, mid_node in zip(in_nodes, intermediate_nodes): edge = hdh.add_hyperedge({in_node, mid_node}, \"q\", name=f\"{name}_stage1\", node_real=cond_flag) # DEBUG #print(f\" [~] Hyperedge added over: {in_node} \u2192 {mid_node}, label: {name}_stage1\") #print(f\" [~] Hyperedge added over: {in_node} \u2192 {mid_node}, label: {name}_stage1\") edges.append(edge) # Stage 2: full multiqubit edge from intermediate \u2192 final edge2 = hdh.add_hyperedge(set(intermediate_nodes) | set(final_nodes), \"q\", name=f\"{name}_stage2\", node_real=cond_flag) # DEBUG #print(f\" [~] Hyperedge added over: {in_node} \u2192 {mid_node}, label: {name}_stage2\") #print(f\" [~] Hyperedge added over: {in_node} \u2192 {mid_node}, label: {name}_stage2\") edges.append(edge2) # Stage 3: final \u2192 post (1:1 again) for final_node, post_node in zip(final_nodes, post_nodes): edge = hdh.add_hyperedge({final_node, post_node}, \"q\", name=f\"{name}_stage3\", node_real=cond_flag) # DEBUG #print(f\" [~] Hyperedge added over: {in_node} \u2192 {mid_node}, label: {name}_stage1\") #print(f\" [~] Hyperedge added over: {in_node} \u2192 {mid_node}, label: {name}_stage1\") edges.append(edge) else: # Single-qubit gate for i, qubit in enumerate(qargs): if modifies_flags[i] and name != \"measure\": t_in = last_gate_input_time[qubit] t_out = t_in + 1 qname = f\"q{qubit}\" in_id = f\"{qname}_t{t_in}\" out_id = f\"{qname}_t{t_out}\" # DEBUG #print(f\"[{name}] Q{qubit} t_in = {t_in}, expected from qubit_time = {qubit_time[qubit]}\") #print(f\"[{name}] Q{qubit} t_in = {t_in}, expected from qubit_time = {qubit_time[qubit]}\") hdh.add_node(out_id, \"q\", t_out, node_real=cond_flag) # DEBUG #print(f\" [+] Node added: {in_id} (type q, time {t_in})\") #print(f\" [+] Node added: {in_id} (type q, time {t_in})\") edge = hdh.add_hyperedge({in_id, out_id}, \"q\", name=name, node_real=cond_flag) # DEBUG #print(f\" [~] Hyperedge added over: {in_id} \u2192 {out_id}, label: {name}_stage1\") #print(f\" [~] Hyperedge added over: {in_id} \u2192 {out_id}, label: {name}_stage1\") edges.append(edge) # Update time qubit_time[qubit] = t_out last_gate_input_time[qubit] = t_in q_with_time = [(q, qubit_time[q]) for q in qargs] c_with_time = [(c, bit_time.get(c, 0)) for c in cargs] for edge in edges: hdh.edge_args[edge] = (q_with_time, c_with_time, modifies_flags) return hdh","title":"Make your own instruction set"},{"location":"passes/","text":"HDH Partitioning Utilities Here is an overview of the partitioning utilities available in the HDH library, designed to distribute quantum computations across multiple devices. Partitioning HDHs for Distribution The hdh/passes directory contains scripts for partitioning and manipulating HDH graphs. The primary file for partitioning is cut.py , which offers two main approaches: a greedy, HDH-aware method and a METIS-based method operating on a qubit graph representation (telegate). Partitioner The main partitioning function is compute_cut , which implements a greedy, bin-filling algorithm. Here's how it works: Node-level assignment: The partitioner assigns individual nodes of the HDH graph to bins. Qubit-based capacity: While the assignment is at the node level, the capacity of each bin is determined by the number of unique qubits it contains. Automatic sibling placement: Once a node corresponding to a particular qubit is placed in a bin, all other nodes associated with that same qubit are automatically assigned to the same bin. Ordering and selection: The algorithm first selects a representative node for each qubit, based on the weighted degree of the nodes. These representatives are then sorted in descending order of their weighted degrees to prioritize high-connectivity qubits. A beam search is used to select the best candidate nodes to place in each bin, considering the change in cut cost and a \"frontier score\" that measures how connected a node is to the nodes already in the bin. Mop-up: Any remaining unassigned nodes are distributed among the bins in a round-robin fashion, respecting the capacity constraints. METIS partitioners For a different approach to partitioning, the library provides the metis_telegate function, which leverages the METIS algorithm (with a fallback to the Kernighan-Lin algorithm if METIS is not available). Telegate graph: This method first converts the HDH into a \"telegate\" graph. In this representation: Nodes are the qubits of the quantum circuit. Edges represent quantum operations between qubits (i.e., their co-appearance in a quantum hyperedge). The weight of each edge corresponds to the number of times the two connected qubits interact. METIS partitioning: The telegate graph is then partitioned using METIS, which is a highly efficient graph partitioning tool. Overflow repair: Since METIS does not guarantee perfectly balanced partitions, a greedy rebalancing algorithm ( _repair_overflow ) is used to adjust the partitions and ensure that no bin exceeds its qubit capacity. Cut evaluations The quality of a partition is determined by the number of \"cuts\"\u2014that is, the number of hyperedges that span across multiple bins. The library provides two functions for this purpose: _total_cost_hdh : Calculates the total cost of a partition on an HDH graph. This is the sum of the weights of all hyperedges that have nodes in more than one bin. _cut_edges_unweighted : Counts the number of edges that cross between different bins in a standard graph (used for evaluating the telegate graph partitions). The partitioner leaderboard For a detailed comparison of the performance of different partitioning strategies on various quantum circuits, please refer to the partitioner leaderboard in the repository's [database]{https://github.com/grageragarces/HDH/tree/database-branch/database}. This can provide valuable insights into which partitioning method is best suited for your specific needs. See the database file in the documentation for more details. Notes on evaluating partitioners on random circuits We would like to warn users and partitioning strategy developpers that we have found partitioners to behave differently on real quantum workloads when compared to randomly generated ones. As such we recommend to not test partitioners on randomly generated workloads unless that is the goal. In the database you can specify the origin of your workloads, tagging it as random if it is so.","title":"Partitioning HDHs for distribution"},{"location":"passes/#hdh-partitioning-utilities","text":"Here is an overview of the partitioning utilities available in the HDH library, designed to distribute quantum computations across multiple devices.","title":"HDH Partitioning Utilities"},{"location":"passes/#partitioning-hdhs-for-distribution","text":"The hdh/passes directory contains scripts for partitioning and manipulating HDH graphs. The primary file for partitioning is cut.py , which offers two main approaches: a greedy, HDH-aware method and a METIS-based method operating on a qubit graph representation (telegate).","title":"Partitioning HDHs for Distribution"},{"location":"passes/#partitioner","text":"The main partitioning function is compute_cut , which implements a greedy, bin-filling algorithm. Here's how it works: Node-level assignment: The partitioner assigns individual nodes of the HDH graph to bins. Qubit-based capacity: While the assignment is at the node level, the capacity of each bin is determined by the number of unique qubits it contains. Automatic sibling placement: Once a node corresponding to a particular qubit is placed in a bin, all other nodes associated with that same qubit are automatically assigned to the same bin. Ordering and selection: The algorithm first selects a representative node for each qubit, based on the weighted degree of the nodes. These representatives are then sorted in descending order of their weighted degrees to prioritize high-connectivity qubits. A beam search is used to select the best candidate nodes to place in each bin, considering the change in cut cost and a \"frontier score\" that measures how connected a node is to the nodes already in the bin. Mop-up: Any remaining unassigned nodes are distributed among the bins in a round-robin fashion, respecting the capacity constraints.","title":"Partitioner"},{"location":"passes/#metis-partitioners","text":"For a different approach to partitioning, the library provides the metis_telegate function, which leverages the METIS algorithm (with a fallback to the Kernighan-Lin algorithm if METIS is not available). Telegate graph: This method first converts the HDH into a \"telegate\" graph. In this representation: Nodes are the qubits of the quantum circuit. Edges represent quantum operations between qubits (i.e., their co-appearance in a quantum hyperedge). The weight of each edge corresponds to the number of times the two connected qubits interact. METIS partitioning: The telegate graph is then partitioned using METIS, which is a highly efficient graph partitioning tool. Overflow repair: Since METIS does not guarantee perfectly balanced partitions, a greedy rebalancing algorithm ( _repair_overflow ) is used to adjust the partitions and ensure that no bin exceeds its qubit capacity.","title":"METIS partitioners"},{"location":"passes/#cut-evaluations","text":"The quality of a partition is determined by the number of \"cuts\"\u2014that is, the number of hyperedges that span across multiple bins. The library provides two functions for this purpose: _total_cost_hdh : Calculates the total cost of a partition on an HDH graph. This is the sum of the weights of all hyperedges that have nodes in more than one bin. _cut_edges_unweighted : Counts the number of edges that cross between different bins in a standard graph (used for evaluating the telegate graph partitions).","title":"Cut evaluations"},{"location":"passes/#the-partitioner-leaderboard","text":"For a detailed comparison of the performance of different partitioning strategies on various quantum circuits, please refer to the partitioner leaderboard in the repository's [database]{https://github.com/grageragarces/HDH/tree/database-branch/database}. This can provide valuable insights into which partitioning method is best suited for your specific needs. See the database file in the documentation for more details.","title":"The partitioner leaderboard"},{"location":"passes/#notes-on-evaluating-partitioners-on-random-circuits","text":"We would like to warn users and partitioning strategy developpers that we have found partitioners to behave differently on real quantum workloads when compared to randomly generated ones. As such we recommend to not test partitioners on randomly generated workloads unless that is the goal. In the database you can specify the origin of your workloads, tagging it as random if it is so.","title":"Notes on evaluating partitioners on random circuits"},{"location":"vis/","text":"Visualising HDHs plot_hdh renders an HDH as a time-vs-index diagram, showing how q uantum and c lassical states evolve across timesteps and how hyperedges connect them. plot_hdh(hdh: HDH, save_path: str | None = None) -> None The function returns nothing, and shows the HDH in a python window unless a save_path = hdh.png is set. In this case the image will be directly saved to the input path. It can be used on any HDH, after it is generated from model instructions: import hdh from hdh.models.qca import QCA from hdh.visualize import plot_hdh # Topology of lattice over which QCA will evolve topology = { \"q0\": [\"q1\", \"q2\"], \"q1\": [\"q0\"], \"q2\": [\"q0\"] } measurements = {\"q1\", \"q2\"} qca = QCA(topology=topology, measurements=measurements, steps=3) hdh = qca.build_hdh() # Generate HDH fig = plot_hdh(hdh) # Visualize HDH or directly from \"manually\" defining all nodes and hyperedges of a HDH: import hdh from hdh.hdh import HDH from hdh.visualize import plot_hdh hdh = HDH() # swap hdh.add_node(\"q1_t0\",\"q\",0) hdh.add_node(\"q3_t0\",\"q\",0) hdh.add_node(\"q1_t1\",\"q\",1) hdh.add_node(\"q3_t1\",\"q\",1) hdh.add_node(\"q1_t2\",\"q\",2) hdh.add_node(\"q3_t2\",\"q\",2) hdh.add_node(\"q1_t3\",\"q\",3) hdh.add_node(\"q3_t3\",\"q\",3) hdh.add_hyperedge([\"q1_t0\", \"q1_t1\"], \"q\") hdh.add_hyperedge([\"q3_t0\", \"q3_t1\"], \"q\") hdh.add_hyperedge([\"q1_t1\", \"q3_t1\", \"q1_t2\", \"q3_t2\"], \"q\") hdh.add_hyperedge([\"q1_t2\", \"q1_t3\"], \"q\") hdh.add_hyperedge([\"q3_t2\", \"q3_t3\"], \"q\") # # cnot hdh.add_node(\"q0_t4\",\"q\",4) hdh.add_node(\"q0_t3\",\"q\",3) hdh.add_node(\"q0_t2\",\"q\",2) hdh.add_node(\"q1_t4\",\"q\",4) hdh.add_node(\"q0_t5\",\"q\",5) hdh.add_node(\"q1_t5\",\"q\",5) hdh.add_hyperedge([\"q0_t2\", \"q0_t3\"], \"q\") hdh.add_hyperedge([\"q1_t3\", \"q1_t4\", \"q0_t3\", \"q0_t4\"], \"q\") hdh.add_hyperedge([\"q0_t4\", \"q0_t5\"], \"q\") hdh.add_hyperedge([\"q1_t4\", \"q1_t5\"], \"q\") # meas hdh.add_node(\"c1_t6\",\"c\",6) hdh.add_node(\"q3_t7\",\"q\",7) hdh.add_node(\"q2_t7\",\"q\",7) hdh.add_hyperedge([\"c1_t6\", \"q1_t5\"], \"c\") hdh.add_hyperedge([\"c1_t6\", \"q3_t7\"], \"c\") hdh.add_hyperedge([\"c1_t6\", \"q2_t7\"], \"c\") # target cnot hdh.add_node(\"q3_t8\",\"q\",8) hdh.add_node(\"q4_t8\",\"q\",8) hdh.add_node(\"q3_t9\",\"q\",9) hdh.add_node(\"q4_t9\",\"q\",9) hdh.add_node(\"q4_t7\",\"q\",7) hdh.add_node(\"q4_t10\",\"q\",10) hdh.add_node(\"q3_t10\",\"q\",10) hdh.add_hyperedge([\"q3_t8\", \"q4_t8\",\"q3_t9\", \"q4_t9\"], \"q\") hdh.add_hyperedge([\"q3_t8\", \"q3_t7\"], \"q\") hdh.add_hyperedge([\"q4_t8\", \"q4_t7\"], \"q\") hdh.add_hyperedge([\"q4_t9\", \"q4_t10\"], \"q\") hdh.add_hyperedge([\"q3_t9\", \"q3_t10\"], \"q\") # h gate hdh.add_node(\"q3_t11\",\"q\",11) hdh.add_hyperedge([\"q3_t10\",\"q3_t11\"], \"q\") # meas hdh.add_node(\"q0_t13\",\"q\",13) hdh.add_node(\"c3_t12\",\"c\",12) hdh.add_hyperedge([\"c3_t12\", \"q3_t11\"], \"c\") hdh.add_hyperedge([\"c3_t12\", \"q0_t13\"], \"c\") hdh.add_hyperedge([\"q0_t5\", \"q0_t13\"], \"q\") fig = plot_hdh(hdh,save_path=\"test2.png\") # Visualize HDH A few things to note about the HDH visualizations: Index ordering : y-positions are \u201cflipped\u201d so that the largest index appears at the bottom of the axis, while tick labels still increase upward. This matches typical circuit diagrams where q0 is drawn at the top. This is consistent with popular quantum packages, and allows for users to \"read\" from top to bottom. Participation filter : nodes not present in any edge are omitted from the plot to reduce clutter. Style inference : if the hyperedge type is not defined for an edge, it is inferred from its output nodes (if one of the nodes is classical it assumes classical, otherwise it defaults to quantum). Same-timestep edges : are not drawn (to avoid the misconception that cuts can occur \"instantaneously\" -> they require communication primitives and thus operations spanning different partitions must have experienced a cut previous to their execution). Lazy qubit appearance : qubits appear in the diagram only when first used in an operation. Their initialization motifs (e.g. in the Circuit model) are delayed until one timestep before the first gate on that qubit. This reduces clutter from unused wires. Example: in the HDH above, qubit 4 only begins appearing from timestep 7.","title":"Visualizing HDHs"},{"location":"vis/#visualising-hdhs","text":"plot_hdh renders an HDH as a time-vs-index diagram, showing how q uantum and c lassical states evolve across timesteps and how hyperedges connect them. plot_hdh(hdh: HDH, save_path: str | None = None) -> None The function returns nothing, and shows the HDH in a python window unless a save_path = hdh.png is set. In this case the image will be directly saved to the input path. It can be used on any HDH, after it is generated from model instructions: import hdh from hdh.models.qca import QCA from hdh.visualize import plot_hdh # Topology of lattice over which QCA will evolve topology = { \"q0\": [\"q1\", \"q2\"], \"q1\": [\"q0\"], \"q2\": [\"q0\"] } measurements = {\"q1\", \"q2\"} qca = QCA(topology=topology, measurements=measurements, steps=3) hdh = qca.build_hdh() # Generate HDH fig = plot_hdh(hdh) # Visualize HDH or directly from \"manually\" defining all nodes and hyperedges of a HDH: import hdh from hdh.hdh import HDH from hdh.visualize import plot_hdh hdh = HDH() # swap hdh.add_node(\"q1_t0\",\"q\",0) hdh.add_node(\"q3_t0\",\"q\",0) hdh.add_node(\"q1_t1\",\"q\",1) hdh.add_node(\"q3_t1\",\"q\",1) hdh.add_node(\"q1_t2\",\"q\",2) hdh.add_node(\"q3_t2\",\"q\",2) hdh.add_node(\"q1_t3\",\"q\",3) hdh.add_node(\"q3_t3\",\"q\",3) hdh.add_hyperedge([\"q1_t0\", \"q1_t1\"], \"q\") hdh.add_hyperedge([\"q3_t0\", \"q3_t1\"], \"q\") hdh.add_hyperedge([\"q1_t1\", \"q3_t1\", \"q1_t2\", \"q3_t2\"], \"q\") hdh.add_hyperedge([\"q1_t2\", \"q1_t3\"], \"q\") hdh.add_hyperedge([\"q3_t2\", \"q3_t3\"], \"q\") # # cnot hdh.add_node(\"q0_t4\",\"q\",4) hdh.add_node(\"q0_t3\",\"q\",3) hdh.add_node(\"q0_t2\",\"q\",2) hdh.add_node(\"q1_t4\",\"q\",4) hdh.add_node(\"q0_t5\",\"q\",5) hdh.add_node(\"q1_t5\",\"q\",5) hdh.add_hyperedge([\"q0_t2\", \"q0_t3\"], \"q\") hdh.add_hyperedge([\"q1_t3\", \"q1_t4\", \"q0_t3\", \"q0_t4\"], \"q\") hdh.add_hyperedge([\"q0_t4\", \"q0_t5\"], \"q\") hdh.add_hyperedge([\"q1_t4\", \"q1_t5\"], \"q\") # meas hdh.add_node(\"c1_t6\",\"c\",6) hdh.add_node(\"q3_t7\",\"q\",7) hdh.add_node(\"q2_t7\",\"q\",7) hdh.add_hyperedge([\"c1_t6\", \"q1_t5\"], \"c\") hdh.add_hyperedge([\"c1_t6\", \"q3_t7\"], \"c\") hdh.add_hyperedge([\"c1_t6\", \"q2_t7\"], \"c\") # target cnot hdh.add_node(\"q3_t8\",\"q\",8) hdh.add_node(\"q4_t8\",\"q\",8) hdh.add_node(\"q3_t9\",\"q\",9) hdh.add_node(\"q4_t9\",\"q\",9) hdh.add_node(\"q4_t7\",\"q\",7) hdh.add_node(\"q4_t10\",\"q\",10) hdh.add_node(\"q3_t10\",\"q\",10) hdh.add_hyperedge([\"q3_t8\", \"q4_t8\",\"q3_t9\", \"q4_t9\"], \"q\") hdh.add_hyperedge([\"q3_t8\", \"q3_t7\"], \"q\") hdh.add_hyperedge([\"q4_t8\", \"q4_t7\"], \"q\") hdh.add_hyperedge([\"q4_t9\", \"q4_t10\"], \"q\") hdh.add_hyperedge([\"q3_t9\", \"q3_t10\"], \"q\") # h gate hdh.add_node(\"q3_t11\",\"q\",11) hdh.add_hyperedge([\"q3_t10\",\"q3_t11\"], \"q\") # meas hdh.add_node(\"q0_t13\",\"q\",13) hdh.add_node(\"c3_t12\",\"c\",12) hdh.add_hyperedge([\"c3_t12\", \"q3_t11\"], \"c\") hdh.add_hyperedge([\"c3_t12\", \"q0_t13\"], \"c\") hdh.add_hyperedge([\"q0_t5\", \"q0_t13\"], \"q\") fig = plot_hdh(hdh,save_path=\"test2.png\") # Visualize HDH A few things to note about the HDH visualizations: Index ordering : y-positions are \u201cflipped\u201d so that the largest index appears at the bottom of the axis, while tick labels still increase upward. This matches typical circuit diagrams where q0 is drawn at the top. This is consistent with popular quantum packages, and allows for users to \"read\" from top to bottom. Participation filter : nodes not present in any edge are omitted from the plot to reduce clutter. Style inference : if the hyperedge type is not defined for an edge, it is inferred from its output nodes (if one of the nodes is classical it assumes classical, otherwise it defaults to quantum). Same-timestep edges : are not drawn (to avoid the misconception that cuts can occur \"instantaneously\" -> they require communication primitives and thus operations spanning different partitions must have experienced a cut previous to their execution). Lazy qubit appearance : qubits appear in the diagram only when first used in an operation. Their initialization motifs (e.g. in the Circuit model) are delayed until one timestep before the first gate on that qubit. This reduces clutter from unused wires. Example: in the HDH above, qubit 4 only begins appearing from timestep 7.","title":"Visualising HDHs"}]}